{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "## PageRank on Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "1. Download a dump of Wikipedia's articles, named `enwiki-{date_string}-pages-articles-multistream.xml.bz2`\n",
    "2. Download the `enwiki-{date_string}-pages-articles-multistream-index.txt.bz2` file\n",
    "3. Move those files into the same folder, removing the `enwiki-{date_string}` prefix\n",
    "4. Process the `xml.bz2` file into a Parquet file using `wikiplain.load_bz2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import glob\n",
    "import gzip\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import socket\n",
    "import struct\n",
    "import subprocess\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "from collections import ChainMap, defaultdict, deque\n",
    "from contextlib import asynccontextmanager\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from enum import Enum, auto\n",
    "from functools import lru_cache, partial\n",
    "from urllib.parse import urlencode, urlsplit, quote as urlquote, unquote as urlunquote\n",
    "from typing import Any, Awaitable, Callable, Literal, TypeVar\n",
    "\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import polars as pl\n",
    "import sqlalchemy as sa\n",
    "import scipy.sparse\n",
    "from dotenv import load_dotenv\n",
    "from ipywidgets import interact\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import select, text as sqltext\n",
    "from tqdm.auto import tqdm\n",
    "from arsenal.datastructures.unionfind import UnionFind\n",
    "\n",
    "import wikiplain\n",
    "from nbhelpers.polars import pager, searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.Config.set_fmt_str_lengths(160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageRankFiles:\n",
    "    def __init__(self, date_string):\n",
    "        self.date_string = date_string\n",
    "        self.enwiki_dir = f\"{os.environ['ENWIKI_DIR']}/{date_string}\"\n",
    "        self.parquet_dir = os.environ.get('ENWIKI_PARQUET_DIR', self.enwiki_dir)\n",
    "        try:\n",
    "            os.mkdir(f\"{self.enwiki_dir}/pagerank\")\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "    \n",
    "    @property\n",
    "    def enwiki_parquet_filename(self):\n",
    "        return f\"{self.parquet_dir}/enwiki_{self.date_string}.parquet\"\n",
    "    \n",
    "    @property\n",
    "    def pagerank_parquet_filename(self):\n",
    "        return f\"{self.parquet_dir}/enwiki_{self.date_string}_pagerank.parquet\"\n",
    "\n",
    "    @property\n",
    "    def nub_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/nub.pkl\"\n",
    "    \n",
    "    @property\n",
    "    def id_maps_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/id_maps.pkl\"\n",
    "    \n",
    "    @property\n",
    "    def dense_id_arr_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/dense_id_arr.pkl\"\n",
    "    \n",
    "    @property\n",
    "    def edge_filename_pattern(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/edges_*.pkl\"\n",
    "    \n",
    "    def edge_filenames(self, num_partitions):\n",
    "        return [\n",
    "            f\"{self.enwiki_dir}/pagerank/edges_{i}.pkl\"\n",
    "            for i in range(num_partitions)\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def in_degree_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/in_degree.pkl\"\n",
    "    \n",
    "    @property\n",
    "    def out_degree_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/out_degree.pkl\"\n",
    "    \n",
    "    def adjacency_filename(self, partition):\n",
    "        return f\"{self.enwiki_dir}/pagerank/adjacency_{partition}.npz\"\n",
    "    \n",
    "    def adjacency_filenames(self, num_partitions):\n",
    "        return [self.adjacency_filename(i) for i in range(num_partitions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = PageRankFiles(\"20230301\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Find title collisions\n",
    "\n",
    "1. There are some pages with the same title - I think this is caused by pages deleted and recreated while the snapshot is in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pqf = pq.ParquetFile(files.enwiki_parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overwritten():\n",
    "    overwritten = set()\n",
    "    timestamp_map = {}\n",
    "    article_ids = {}\n",
    "    pqf_size = 0\n",
    "    for batch in tqdm(pqf.iter_batches(batch_size=100), total=pqf.num_row_groups):\n",
    "        for aid, ns, ttl, tm in zip(batch[\"id\"].to_numpy(), batch[\"ns\"].to_numpy(), batch[\"title\"].to_pylist(), batch[\"timestamp\"].to_pylist()):\n",
    "            pqf_size += 1\n",
    "            if ns != 0:\n",
    "                continue\n",
    "            tm = np.datetime64(tm)\n",
    "            other_id = article_ids.setdefault(ttl, aid)\n",
    "            if other_id != aid:\n",
    "                if (timestamp_map[ttl], other_id) < (tm, aid):\n",
    "                    print(f\"{ttl!r}: {aid} > {other_id}\")\n",
    "                    overwritten.add(other_id)\n",
    "                    article_ids[ttl] = aid\n",
    "                    timestamp_map[ttl] = tm\n",
    "                else:\n",
    "                    print(f\"{ttl!r}: {other_id} > {aid}\")\n",
    "                    overwritten.add(aid)\n",
    "            else:\n",
    "                timestamp_map[ttl] = tm\n",
    "    return overwritten, pqf_size\n",
    "\n",
    "try:\n",
    "    with open(files.nub_filename, \"rb\") as fp:\n",
    "        overwritten, pqf_size = pickle.load(fp)\n",
    "except Exception:\n",
    "    overwritten, pqf_size = get_overwritten()\n",
    "    with open(files.nub_filename, \"wb\") as fp:\n",
    "        pickle.dump((overwritten, pqf_size), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Build representation of articles/links as a graph\n",
    "\n",
    "1. Create `id_map` from non-redirecting article titles to node number, and `id_map2` from redirecting article titles to node number\n",
    "2. Use `wikiplain` to extract link titles, and use above maps to convert to (src_id, dest_id) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vec:\n",
    "    def __init__(self, dtype):\n",
    "        self.array = np.ndarray((1024,), dtype=dtype)\n",
    "        self.length = 0\n",
    "    \n",
    "    @property\n",
    "    def capacity(self):\n",
    "        return self.array.shape[0]\n",
    "\n",
    "    def append(self, v):\n",
    "        idx = self.length\n",
    "        if idx >= self.capacity:\n",
    "            addsz = max(2, self.capacity)\n",
    "            self.array = np.hstack((self.array, np.zeros((addsz,), dtype=self.array.dtype)))\n",
    "        self.array[idx] = v\n",
    "        self.length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_maps():\n",
    "    redirect_group_map = UnionFind()\n",
    "    id_map = {}\n",
    "    redirect_lst = []\n",
    "    dense_ids = Vec(dtype=np.int64)\n",
    "    for batch in tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size // 100)):\n",
    "        for aid, ns, ttl, redir in zip(batch[\"id\"].to_numpy(), batch[\"ns\"].to_numpy(), batch[\"title\"].to_pylist(), batch[\"redirect\"].to_pylist()):\n",
    "            if ns != 0 or aid in overwritten:\n",
    "                continue\n",
    "            if redir is not None:\n",
    "                redirect_group_map.union(ttl, redir)\n",
    "                redirect_lst.append(ttl)\n",
    "            else:\n",
    "                assert ttl not in id_map, f\"Expected unique titles, got second instance of {ttl}\"\n",
    "                dense_ids.append(aid)\n",
    "                id_map[ttl] = len(id_map)\n",
    "    id_map2 = {}\n",
    "    for group in redirect_group_map.classes():\n",
    "        centers = [ttl for ttl in group if ttl in id_map]\n",
    "        if len(centers) == 0:\n",
    "            continue\n",
    "        assert len(centers) == 1, str(centers)\n",
    "        for ttl in group:\n",
    "            if ttl != centers[0]:\n",
    "                id_map2[ttl] = id_map[centers[0]]\n",
    "    return id_map, id_map2, dense_ids.array[:dense_ids.length]\n",
    "\n",
    "try:\n",
    "    with open(files.id_maps_filename, \"rb\") as fp:\n",
    "        id_map, id_map2 = pickle.load(fp)\n",
    "    with open(files.dense_id_arr_filename, \"rb\") as fp:\n",
    "        dense_id_arr = pickle.load(fp)\n",
    "except Exception:\n",
    "    id_map, id_map2, dense_id_arr = get_id_maps()\n",
    "    with open(files.id_maps_filename, \"wb\") as fp:\n",
    "        pickle.dump((id_map, id_map2), fp)\n",
    "    with open(files.dense_id_arr_filename, \"wb\") as fp:\n",
    "        pickle.dump(dense_id_arr, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6625358, 10408919)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_map), len(id_map2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anarchism',\n",
       " 'Albedo',\n",
       " 'A',\n",
       " 'Alabama',\n",
       " 'Achilles',\n",
       " 'Abraham Lincoln',\n",
       " 'Aristotle',\n",
       " 'An American in Paris',\n",
       " 'Academy Award for Best Production Design',\n",
       " 'Academy Awards']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.islice(iter(id_map.keys()), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A11y',\n",
       " 'Digital accessibility',\n",
       " 'Accessible computing',\n",
       " 'Open Accessibility Framework',\n",
       " 'Accessible Computing',\n",
       " 'AccessibleComputing',\n",
       " 'Afghan history',\n",
       " 'Afghanistan history',\n",
       " 'Afghanistan/History',\n",
       " 'Afghan History']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.islice(iter(id_map2.keys()), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73619/1456615976.py:1: DeprecationWarning: `columns` is deprecated as an argument to `__init__`; use `schema` instead.\n",
      "  localsizes = (pl.DataFrame([(id(value), name, sys.getsizeof(value)) for name, value in locals().items()],\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (50, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>size</th><th>name</th></tr><tr><td>i64</td><td>i64</td><td>list[str]</td></tr></thead><tbody><tr><td>140074719853440</td><td>335544408</td><td>[&quot;id_map2&quot;]</td></tr><tr><td>140074719854208</td><td>335544408</td><td>[&quot;id_map&quot;]</td></tr><tr><td>94811155953408</td><td>1765</td><td>[&quot;_i4&quot;]</td></tr><tr><td>94814257225104</td><td>1654</td><td>[&quot;_i9&quot;]</td></tr><tr><td>94814257182992</td><td>1309</td><td>[&quot;_i7&quot;]</td></tr><tr><td>94811117009344</td><td>1072</td><td>[&quot;ChainMap&quot;]</td></tr><tr><td>94814257220496</td><td>1072</td><td>[&quot;Vec&quot;]</td></tr><tr><td>94811116943968</td><td>1072</td><td>[&quot;auto&quot;]</td></tr><tr><td>94811143882784</td><td>1072</td><td>[&quot;tqdm&quot;]</td></tr><tr><td>94811117133360</td><td>1072</td><td>[&quot;TypeVar&quot;]</td></tr><tr><td>94811116558208</td><td>1072</td><td>[&quot;PageRankFiles&quot;]</td></tr><tr><td>94811155929184</td><td>1072</td><td>[&quot;UnionFind&quot;]</td></tr><tr><td>...</td><td>...</td><td>...</td></tr><tr><td>140074719583472</td><td>144</td><td>[&quot;get_overwritten&quot;]</td></tr><tr><td>140076769478400</td><td>144</td><td>[&quot;urlencode&quot;]</td></tr><tr><td>140076767651744</td><td>144</td><td>[&quot;dataclass&quot;]</td></tr><tr><td>140076757176496</td><td>113</td><td>[&quot;__doc__&quot;]</td></tr><tr><td>140070529225648</td><td>112</td><td>[&quot;dense_id_arr&quot;]</td></tr><tr><td>140074719224400</td><td>100</td><td>[&quot;_i6&quot;]</td></tr><tr><td>140074719226416</td><td>97</td><td>[&quot;_i&quot;, &quot;_i12&quot;]</td></tr><tr><td>140070529227664</td><td>96</td><td>[&quot;_ii&quot;, &quot;_i11&quot;]</td></tr><tr><td>140074719435664</td><td>83</td><td>[&quot;_i3&quot;]</td></tr><tr><td>140074719439408</td><td>82</td><td>[&quot;_i5&quot;]</td></tr><tr><td>140074719795264</td><td>74</td><td>[&quot;_iii&quot;, &quot;_i10&quot;]</td></tr><tr><td>140076764665296</td><td>72</td><td>[&quot;glob&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (50, 3)\n",
       "┌─────────────────┬───────────┬──────────────────┐\n",
       "│ id              ┆ size      ┆ name             │\n",
       "│ ---             ┆ ---       ┆ ---              │\n",
       "│ i64             ┆ i64       ┆ list[str]        │\n",
       "╞═════════════════╪═══════════╪══════════════════╡\n",
       "│ 140074719853440 ┆ 335544408 ┆ [\"id_map2\"]      │\n",
       "│ 140074719854208 ┆ 335544408 ┆ [\"id_map\"]       │\n",
       "│ 94811155953408  ┆ 1765      ┆ [\"_i4\"]          │\n",
       "│ 94814257225104  ┆ 1654      ┆ [\"_i9\"]          │\n",
       "│ ...             ┆ ...       ┆ ...              │\n",
       "│ 140074719435664 ┆ 83        ┆ [\"_i3\"]          │\n",
       "│ 140074719439408 ┆ 82        ┆ [\"_i5\"]          │\n",
       "│ 140074719795264 ┆ 74        ┆ [\"_iii\", \"_i10\"] │\n",
       "│ 140076764665296 ┆ 72        ┆ [\"glob\"]         │\n",
       "└─────────────────┴───────────┴──────────────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localsizes = (pl.DataFrame([(id(value), name, sys.getsizeof(value)) for name, value in locals().items()],\n",
    "                          columns=['id', 'name', 'size'])\n",
    "              .groupby('id')\n",
    "              .agg(pl.max(\"size\"), pl.col(\"name\").apply(lambda ser: ser.to_list()))\n",
    "              .sort('size', descending=True)\n",
    "              .head(50)\n",
    "             )\n",
    "localsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset -f out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairVec:\n",
    "    def __init__(self, dtype):\n",
    "        self.array = np.ndarray((1024, 2), dtype=dtype)\n",
    "        self.length = 0\n",
    "    \n",
    "    @property\n",
    "    def capacity(self):\n",
    "        return self.array.shape[0]\n",
    "\n",
    "    def append(self, v1, v2):\n",
    "        idx = self.length\n",
    "        if idx >= self.capacity:\n",
    "            addsz = max(2, self.capacity)\n",
    "            self.array = np.vstack((self.array, np.zeros((addsz, 2), dtype=self.array.dtype)))\n",
    "        self.array[idx] = [v1, v2]\n",
    "        self.length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wiki_link(line):\n",
    "    dest_ttl = line.strip()\n",
    "    if len(dest_ttl) == 0:\n",
    "        return None\n",
    "    dest_ttl = dest_ttl[0].upper() + dest_ttl[1:]\n",
    "    dest_ttl = dest_ttl.split('|', maxsplit=1)[0]\n",
    "    dest_ttl = dest_ttl.split('#', maxsplit=1)[0]\n",
    "    return dest_ttl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PARTITION_SIZE = 16\n",
    "PARTITION_SIZE = 1 << LOG_PARTITION_SIZE\n",
    "N = len(id_map)\n",
    "NUM_PARTITIONS = math.ceil(N / PARTITION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_id_map = ChainMap(id_map, id_map2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6625358"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge format\n",
    "\n",
    "- `edges_{n}.pkl` stores the outgoing links from `PARITION_SIZE*n ..< PARTITION_SIZE*(n+1)`\n",
    "- These are stored in a list where element `i` contains the links out to `PARITION_SIZE*i ..< PARTITION_SIZE*(i+1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(iterable, size):\n",
    "    \"\"\"Split an iterable into list chunks of size `n`.\n",
    "    \n",
    "    The last chunk can be fewer than `n` elements long, but it won't be empty.\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(iterator, size))\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "        else:\n",
    "            return\n",
    "\n",
    "def lazy_chunk(iterable, n):\n",
    "    \"\"\"Split an iterable into iterable chunks of size `n`.\n",
    "    \n",
    "    The last chunk can be fewer than `n` elements long, but it won't be empty.\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        try:\n",
    "            first = next(iterator)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        yield itertools.chain([first], itertools.islice(iterator, n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "in_degree = np.zeros(N, dtype=np.int32)\n",
    "out_degree = np.zeros(N, dtype=np.int32)\n",
    "# article_len_arr = np.zeros(N, dtype=np.int32)\n",
    "def get_edges():\n",
    "    with tqdm(position=1) as progress:\n",
    "        iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "        iterator = map(\n",
    "            lambda b: zip(\n",
    "                b[\"id\"].to_numpy(),\n",
    "                b[\"ns\"].to_numpy(),\n",
    "                map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "                b[\"text\"].to_pylist()\n",
    "            ),\n",
    "            iterator\n",
    "        )\n",
    "        iterator = itertools.chain.from_iterable(iterator)\n",
    "        iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "        iterator = enumerate(map(operator.itemgetter(3), iterator))\n",
    "        filenames = files.edge_filenames(NUM_PARTITIONS)\n",
    "        for part_idx, subitr in enumerate(lazy_chunk(iterator, PARTITION_SIZE)):\n",
    "            edges = [PairVec('int32') for _ in range(0, N, PARTITION_SIZE)]\n",
    "            for src_id, text in subitr:\n",
    "                for link in wikiplain.get_links(text):\n",
    "                    dest_ttl = parse_wiki_link(link)\n",
    "                    if (dest_id := id_map.get(dest_ttl) or id_map2.get(dest_ttl)) is not None:\n",
    "                        partition = dest_id >> LOG_PARTITION_SIZE\n",
    "                        edges[partition].append(src_id, dest_id)\n",
    "                        in_degree[dest_id] += 1\n",
    "                        out_degree[src_id] += 1\n",
    "                        progress.update()\n",
    "            with open(filenames[part_idx], \"wb\") as fp:\n",
    "                pickle.dump([vec.array[:vec.length] for vec in edges], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_dab_array():\n",
    "#     result = np.zeros(N, dtype=np.bool8)\n",
    "#     dab_proc = subprocess.Popen(\n",
    "#         [\"wikiplain\", \"--fraction\", \"1\", \"-c\", \"only-dab\", \"--ns\", \"0\", files.enwiki_database_filename],\n",
    "#         stdout=subprocess.PIPE,\n",
    "#         stderr=subprocess.PIPE\n",
    "#     )\n",
    "#     iterator = make_links_iter(dab_proc.stdout)\n",
    "#     iterator = tqdm(iterator, position=0, total=len(id_map))\n",
    "#     iterator = map(lambda pair: (pair[0].decode(\"utf-8\"), pair[1]), iterator)\n",
    "#     for n, subitr in enumerate(lazy_chunk(iterator, PARTITION_SIZE)):\n",
    "#         for ttl, text in subitr:\n",
    "#             src_id = id_map[ttl]\n",
    "#             if len(text) > 0:\n",
    "#                 result[src_id] = True\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDF = pl.scan_parquet(files.enwiki_parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_fnames = glob.glob(files.edge_filename_pattern)\n",
    "try:\n",
    "    assert set(edge_fnames) == set(files.edge_filenames(NUM_PARTITIONS))\n",
    "    for fname in edge_fnames:\n",
    "        with open(fname, \"rb\") as fp:\n",
    "            assert len(pickle.load(fp)) == NUM_PARTITIONS\n",
    "    with open(files.in_degree_filename, \"rb\") as fp:\n",
    "        in_degree = pickle.load(fp)\n",
    "    with open(files.out_degree_filename, \"rb\") as fp:\n",
    "        out_degree = pickle.load(fp)\n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "    get_edges()\n",
    "    edge_fnames = glob.glob(files.edge_filename_pattern)\n",
    "    with open(files.in_degree_filename, \"wb\") as fp:\n",
    "        pickle.dump(in_degree, fp)\n",
    "    with open(files.out_degree_filename, \"wb\") as fp:\n",
    "        pickle.dump(out_degree, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     with open(files.dab_array_filename, \"rb\") as fp:\n",
    "#         dab_array = pickle.load(fp)\n",
    "# except Exception as exc:\n",
    "#     print(exc)\n",
    "#     dab_array = get_dab_array()\n",
    "#     with open(files.dab_array_filename, \"wb\") as fp:\n",
    "#         pickle.dump(dab_array, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_adjacency_matrix_slice(partition, progress):\n",
    "    \"\"\"Computes the slice of the adjacency matrix A starting at row p*S and ending before row (p+1)*S\n",
    "    \n",
    "    p=partition, S=PARTITION_SIZE, and A is defined so that\n",
    "    A @ np.eye(N)[i] = v, a probability vector where\n",
    "        v[j] = out-degree(i) > 0 | count((i,j) in E) / out-degree(i)\n",
    "               otherwise         | 0\n",
    "    \"\"\"\n",
    "    origin_row = partition * PARTITION_SIZE\n",
    "    n_rows = min(PARTITION_SIZE, N - origin_row)\n",
    "    index_arrs = []\n",
    "    value_arrs = []\n",
    "    for fname in glob.glob(files.edge_filename_pattern):\n",
    "        with open(fname, \"rb\") as fp:\n",
    "            vec = pickle.load(fp)[partition]\n",
    "        # vec is\n",
    "        #  [[src_id_0, dest_id_0],\n",
    "        #   [src_id_1, dest_id_1],\n",
    "        #   ...\n",
    "        #  ]\n",
    "        # Sort by (src,dest), make unique and get counts\n",
    "        key_arr = (vec[:, 0].astype('int64') << 32) | vec[:, 1]\n",
    "        _, order, count = np.unique(key_arr, return_index=True, return_counts=True)\n",
    "        vec = vec[order]\n",
    "        # Normalize `count` based on (src,)\n",
    "        count = count.astype('float64') / out_degree[vec[:, 0]]\n",
    "        index_arrs.append(vec)\n",
    "        value_arrs.append(count)\n",
    "        progress.update()\n",
    "    index_arr = np.vstack(index_arrs)\n",
    "    matrix_slice = scipy.sparse.csr_array(\n",
    "        (np.hstack(value_arrs), (index_arr[:, 1] - origin_row, index_arr[:, 0])),\n",
    "        shape=(n_rows, N),\n",
    "        dtype=np.float64\n",
    "    )\n",
    "    return matrix_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1e71482df8458db67d51f502e32afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tqdm(total=NUM_PARTITIONS**2) as progress:\n",
    "    for partition in range(NUM_PARTITIONS):\n",
    "        adj_matrix = compute_adjacency_matrix_slice(partition, progress)\n",
    "        scipy.sparse.save_npz(files.adjacency_filename(partition), adj_matrix, compressed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     8,    28,   104,   460,  1466, 25044])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(out_degree, [0, 0.1, 0.5, 0.9, 0.99, 0.999, 1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_out_degree = np.log(out_degree + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_out_degree /= log_out_degree.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WMf5L5VEqZb"
   },
   "source": [
    "### Global PageRank\n",
    "\n",
    "The initial rank is a column vector $\\mathbf{r}$ where $\\mathbf{r}_i = \\frac{1}{N}$\n",
    "\n",
    "The transition matrix $\\mathbf{M}$ is N x N; each column represents a source, and each row represents a destination.\n",
    "$\\mathbf{M}_{ij} = P(\\text{next}=i\\,|\\,\\text{current}=j)$. Each column **must** sum to 1 for the calculation to be stable, so if page $j$ contains no links, it is treated as if it had a link to every page.\n",
    "\n",
    "The power method iteratively computes better ranks: $\\mathbf{r'} = (1 - \\alpha) \\mathbf{M}\\mathbf{r} + \\frac{\\alpha}{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalized PageRank\n",
    "\n",
    "Personalized PageRank uses a preference vector $\\mathbf{p}$ in place of the uniform $\\frac{1}{N}$ for _teleportation_. Pages with no out-links still use a uniform distribution. The initial rank can be any vector, because of the converging property of the power method (explanation at https://mathworld.wolfram.com/Eigenvector.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WMf5L5VEqZb"
   },
   "source": [
    "### Ending iteration\n",
    "\n",
    "At each iteration, we calculate the [perplexity](https://en.wikipedia.org/wiki/Perplexity) of the PageRank distribution, where perplexity is defined as 2 raised to the [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) of the PageRank distribution, i.e., $2^{H(PR)}$. The initial guess is at maximum entropy, so the first iteration causes perplexity to decrease. Later iterations may change perplexity in either direction; we stop when the change is below a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "wsL0yQKvKqAC"
   },
   "outputs": [],
   "source": [
    "def perplexity(distribution):\n",
    "    return np.power(2, np.sum(-distribution * np.log2(distribution)))\n",
    "\n",
    "def personalized_page_rank(preference, threshold=1, random_jump_prob=0.15):\n",
    "    current_rank = np.ones(N, dtype=np.float64) / N\n",
    "    next_rank = np.zeros(N, dtype=np.float64)\n",
    "    # iteratively update current_rank\n",
    "    edge_follow_prob = 1 - random_jump_prob\n",
    "    prev_perplexity = float('inf')\n",
    "    current_perplexity = perplexity(current_rank)\n",
    "    current_iter = 0\n",
    "    iter_start = time.time()\n",
    "    print(\"Itr# | ΔPerplexity     | Seconds\")\n",
    "    while abs(prev_perplexity - current_perplexity) > threshold:\n",
    "        current_iter += 1\n",
    "        next_rank[:] = random_jump_prob * preference\n",
    "        # update destinations from non-sink nodes (N x N times N x 1 -> N x 1)\n",
    "        spread_probs = np.vstack([\n",
    "            adjacency_matrix_slice.dot(current_rank[:, np.newaxis])\n",
    "            for adjacency_matrix_slice in map(scipy.sparse.load_npz, files.adjacency_filenames(NUM_PARTITIONS))\n",
    "        ])\n",
    "        next_rank += edge_follow_prob * spread_probs[:, 0]  # make column vector 1-D\n",
    "        # update destinations from sink nodes\n",
    "        next_rank[:] += edge_follow_prob * current_rank[out_degree == 0].sum() / N\n",
    "        # copy `next_rank` values into `current_rank``\n",
    "        current_rank[:] = next_rank\n",
    "        # --\n",
    "        # compute perplexity and progress\n",
    "        prev_perplexity = current_perplexity\n",
    "        current_perplexity = perplexity(current_rank)\n",
    "        next_iter_start = time.time()\n",
    "        print(\"{:<3d}    {:<15.6f}   {:.3f}\".format(current_iter,\n",
    "                                                    current_perplexity - prev_perplexity,\n",
    "                                                    next_iter_start - iter_start))\n",
    "        iter_start = next_iter_start\n",
    "    df = pl.DataFrame({\n",
    "        \"title\": id_map.keys(), \"value\": next_rank, \"in_deg\": in_degree, \"out_deg\": out_degree,\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "wsL0yQKvKqAC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr# | ΔPerplexity     | Seconds\n",
      "1      -6590087.373375   63.876\n",
      "2      -18066.651574     16.104\n",
      "3      -7991.425620      16.401\n",
      "4      -3135.253593      7.857\n",
      "5      -1614.272158      6.682\n",
      "6      -903.522792       5.989\n",
      "7      -552.961068       6.086\n",
      "8      -355.355849       6.304\n",
      "9      -238.212649       7.212\n",
      "10     -164.180308       5.578\n",
      "11     -115.689300       5.264\n",
      "12     -82.804801        4.777\n",
      "13     -59.988596        7.572\n",
      "14     -43.842793        6.921\n",
      "15     -32.258310        5.569\n",
      "16     -23.852399        6.584\n",
      "17     -17.703208        5.494\n",
      "18     -13.175938        7.325\n",
      "19     -9.827130         7.049\n",
      "20     -7.340991         6.674\n",
      "21     -5.490338         6.617\n",
      "22     -4.109888         6.281\n",
      "23     -3.078600         5.053\n",
      "24     -2.307258         4.805\n",
      "25     -1.729838         4.901\n",
      "26     -1.297300         5.170\n",
      "27     -0.973129         5.137\n"
     ]
    }
   ],
   "source": [
    "# Run until perplexity changes by less than 1\n",
    "PR = personalized_page_rank(log_out_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_sorted = PR.sort('value', descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a496ac4b4a2b4fe1beb092bc2a5c0b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='page', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.pager.<locals>.<lambda>(page)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pager(PR_sorted.slice(0, 2000), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346208af520b4554b893aea5678ce505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='q'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.searcher.<locals>.searcher_run(q)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher(\n",
    "    PR_sorted.slice(0, 200000).with_columns(pl.Series(\"rank\", range(200000))).select([\"rank\", *PR_sorted.columns]),\n",
    "    ['title'],\n",
    "    20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR.write_parquet(files.pagerank_parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNw97vMeu3UGKrk6j3TYQO8",
   "include_colab_link": true,
   "name": "PageRank.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
