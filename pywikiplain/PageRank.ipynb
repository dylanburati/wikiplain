{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "## PageRank on Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "1. Download a dump of Wikipedia's articles, named `enwiki-{date_string}-pages-articles-multistream.xml.bz2`\n",
    "2. Download the `enwiki-{date_string}-pages-articles-multistream-index.txt.bz2` file\n",
    "3. Move those files into the same folder, removing the `enwiki-{date_string}` prefix\n",
    "4. Process the `xml.bz2` file into a Parquet file using `wikiplain.load_bz2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import glob\n",
    "import gzip\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import socket\n",
    "import struct\n",
    "import subprocess\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "from collections import ChainMap, defaultdict, deque\n",
    "from contextlib import asynccontextmanager\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from enum import Enum, auto\n",
    "from functools import lru_cache, partial\n",
    "from urllib.parse import urlencode, urlsplit, quote as urlquote, unquote as urlunquote\n",
    "from typing import Any, Awaitable, Callable, Literal, TypeVar\n",
    "\n",
    "import cbor2\n",
    "import numpy as np\n",
    "import pypocketmap as pkm\n",
    "import pyarrow.parquet as pq\n",
    "import polars as pl\n",
    "import sqlalchemy as sa\n",
    "import scipy.sparse\n",
    "from dotenv import load_dotenv\n",
    "from ipywidgets import interact\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import select, text as sqltext\n",
    "from tqdm.auto import tqdm\n",
    "from arsenal.datastructures.unionfind import UnionFind\n",
    "\n",
    "import wikiplain\n",
    "from nbhelpers.polars import pager, searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.Config.set_fmt_str_lengths(160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageRankFiles:\n",
    "    def __init__(self, date_string):\n",
    "        self.date_string = date_string\n",
    "        self.enwiki_dir = f\"{os.environ['ENWIKI_DIR']}/{date_string}\"\n",
    "        self.parquet_dir = os.environ.get('ENWIKI_PARQUET_DIR', self.enwiki_dir)\n",
    "        try:\n",
    "            os.mkdir(f\"{self.enwiki_dir}/pagerank\")\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "    \n",
    "    @property\n",
    "    def enwiki_parquet_filename(self):\n",
    "        return f\"{self.parquet_dir}/enwiki_{self.date_string}.parquet\"\n",
    "    \n",
    "    @property\n",
    "    def pagerank_parquet_filename(self):\n",
    "        return f\"{self.parquet_dir}/enwiki_{self.date_string}_pagerank.parquet\"\n",
    "\n",
    "    @property\n",
    "    def nub_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/nub.bin\"\n",
    "    \n",
    "    @property\n",
    "    def id_map_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/id_map.bin\"\n",
    "    \n",
    "    @property\n",
    "    def id_map2_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/id_map2.bin\"\n",
    "    \n",
    "    @property\n",
    "    def dense_id_arr_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/dense_id_arr.npy\"\n",
    "    \n",
    "    @property\n",
    "    def edge_filename_pattern(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/edges_*.npz\"\n",
    "    \n",
    "    def edge_filenames(self, num_partitions):\n",
    "        return [\n",
    "            f\"{self.enwiki_dir}/pagerank/edges_{i}.npz\"\n",
    "            for i in range(num_partitions)\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def in_degree_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/in_degree.npy\"\n",
    "    \n",
    "    @property\n",
    "    def out_degree_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/out_degree.npy\"\n",
    "    \n",
    "    def adjacency_filename(self, partition):\n",
    "        return f\"{self.enwiki_dir}/pagerank/adjacency_{partition}.npz\"\n",
    "    \n",
    "    def adjacency_filenames(self, num_partitions):\n",
    "        return [self.adjacency_filename(i) for i in range(num_partitions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = PageRankFiles(\"20240401\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Find title collisions\n",
    "\n",
    "1. There are some pages with the same title - I think this is caused by pages deleted and recreated while the snapshot is in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pqf = pq.ParquetFile(files.enwiki_parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overwritten():\n",
    "    overwritten = set()\n",
    "    timestamp_map = {}\n",
    "    article_ids = {}\n",
    "    pqf_size = 0\n",
    "    for batch in tqdm(pqf.iter_batches(batch_size=100), total=pqf.num_row_groups):\n",
    "        for aid, ns, ttl, tm in zip(batch[\"id\"].to_numpy(), batch[\"ns\"].to_numpy(), batch[\"title\"].to_pylist(), batch[\"timestamp\"].to_pylist()):\n",
    "            pqf_size += 1\n",
    "            if ns != 0:\n",
    "                continue\n",
    "            tm = np.datetime64(tm)\n",
    "            other_id = article_ids.setdefault(ttl, aid)\n",
    "            if other_id != aid:\n",
    "                if (timestamp_map[ttl], other_id) < (tm, aid):\n",
    "                    print(f\"{ttl!r}: {aid} > {other_id}\")\n",
    "                    overwritten.add(other_id)\n",
    "                    article_ids[ttl] = aid\n",
    "                    timestamp_map[ttl] = tm\n",
    "                else:\n",
    "                    print(f\"{ttl!r}: {other_id} > {aid}\")\n",
    "                    overwritten.add(aid)\n",
    "            else:\n",
    "                timestamp_map[ttl] = tm\n",
    "    return overwritten, pqf_size\n",
    "\n",
    "try:\n",
    "    with open(files.nub_filename, \"rb\") as fp:\n",
    "        overwritten, pqf_size = cbor2.load(fp)\n",
    "except Exception:\n",
    "    overwritten, pqf_size = get_overwritten()\n",
    "    overwritten = {int(e) for e in overwritten}\n",
    "    with open(files.nub_filename, \"wb\") as fp:\n",
    "        cbor2.dump((overwritten, pqf_size), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_overwritten time to complete\n",
    "\n",
    "| date | batches/s | time |\n",
    "| :--- | :------ | :--- |\n",
    "| 2024-05-14 | 230.5 | 17min 04s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Build representation of articles/links as a graph\n",
    "\n",
    "1. Create `id_map` from non-redirecting article titles to node number, and `id_map2` from redirecting article titles to node number\n",
    "2. Use `wikiplain` to extract link titles, and use above maps to convert to (src_id, dest_id) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vec:\n",
    "    def __init__(self, dtype):\n",
    "        self.array = np.ndarray((1024,), dtype=dtype)\n",
    "        self.length = 0\n",
    "    \n",
    "    @property\n",
    "    def capacity(self):\n",
    "        return self.array.shape[0]\n",
    "\n",
    "    def append(self, v):\n",
    "        idx = self.length\n",
    "        if idx >= self.capacity:\n",
    "            addsz = max(2, self.capacity)\n",
    "            self.array = np.hstack((self.array, np.zeros((addsz,), dtype=self.array.dtype)))\n",
    "        self.array[idx] = v\n",
    "        self.length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterdecode(f):\n",
    "    decoder = cbor2.CBORDecoder(f)\n",
    "    while True:\n",
    "        try:\n",
    "            yield decoder.decode()\n",
    "        except EOFError:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_maps():\n",
    "    redirect_group_map = UnionFind()\n",
    "    id_map = pkm.create(str, int)\n",
    "    redirect_lst = []\n",
    "    dense_ids = Vec(dtype=np.int64)\n",
    "    for batch in tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100)):\n",
    "        for aid, ns, ttl, redir in zip(batch[\"id\"].to_numpy(), batch[\"ns\"].to_numpy(), batch[\"title\"].to_pylist(), batch[\"redirect\"].to_pylist()):\n",
    "            if ns != 0 or aid in overwritten:\n",
    "                continue\n",
    "            if redir is not None:\n",
    "                redirect_group_map.union(ttl, redir)\n",
    "                redirect_lst.append(ttl)\n",
    "            else:\n",
    "                assert ttl not in id_map, f\"Expected unique titles, got second instance of {ttl}\"\n",
    "                dense_ids.append(aid)\n",
    "                id_map[ttl] = len(id_map)\n",
    "    id_map2 = pkm.create(str, int)\n",
    "    for group in redirect_group_map.classes():\n",
    "        centers = [ttl for ttl in group if ttl in id_map]\n",
    "        if len(centers) == 0:\n",
    "            continue\n",
    "        assert len(centers) == 1, str(centers)\n",
    "        for ttl in group:\n",
    "            if ttl != centers[0]:\n",
    "                id_map2[ttl] = id_map[centers[0]]\n",
    "    return id_map, id_map2, dense_ids.array[:dense_ids.length]\n",
    "\n",
    "try:\n",
    "    with open(files.id_map_filename, \"rb\") as fp:\n",
    "        id_map = pkm.create(str, int)\n",
    "        for k, v in iterdecode(fp):\n",
    "            id_map[k] = v\n",
    "    with open(files.id_map2_filename, \"rb\") as fp:\n",
    "        id_map2 = pkm.create(str, int)\n",
    "        for k, v in iterdecode(fp):\n",
    "            id_map2[k] = v\n",
    "    with open(files.dense_id_arr_filename, \"rb\") as fp:\n",
    "        dense_id_arr = np.load(fp)\n",
    "except Exception:\n",
    "    id_map, id_map2, dense_id_arr = get_id_maps()\n",
    "    with open(files.id_map_filename, \"wb\") as fp:\n",
    "        for k, v in id_map.items():\n",
    "            cbor2.dump((k, v), fp)\n",
    "    with open(files.id_map2_filename, \"wb\") as fp:\n",
    "        for k, v in id_map2.items():\n",
    "            cbor2.dump((k, v), fp)\n",
    "    with open(files.dense_id_arr_filename, \"wb\") as fp:\n",
    "        np.save(fp, dense_id_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6806227, 10873732)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_map), len(id_map2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Martín Vázquez',\n",
       " 'Rouverol',\n",
       " 'Ron Hansell',\n",
       " 'Jim Wolf (musician)',\n",
       " 'McAllen Miller International Airport',\n",
       " '1937 Albanian National Championship',\n",
       " 'Manilius (crater)',\n",
       " 'Nerka Lake',\n",
       " 'Metallacarboxylic acid',\n",
       " 'Nicolas Appert']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.islice(iter(id_map.keys()), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Office for fair access',\n",
       " 'The Catalyst (newspaper)',\n",
       " 'Scarecrow Press historical dictionary series',\n",
       " 'Boissieri',\n",
       " 'Edward Northey (disambiguation)',\n",
       " 'Herzogschloss Zweibrücken',\n",
       " 'Trinity Square (disambiguation)',\n",
       " 'Urban cowboy',\n",
       " 'K 424a',\n",
       " 'Nawaz Shareef']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.islice(iter(id_map2.keys()), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairVec:\n",
    "    def __init__(self, dtype):\n",
    "        self.array = np.ndarray((1024, 2), dtype=dtype)\n",
    "        self.length = 0\n",
    "    \n",
    "    @property\n",
    "    def capacity(self):\n",
    "        return self.array.shape[0]\n",
    "\n",
    "    def append(self, v1, v2):\n",
    "        idx = self.length\n",
    "        if idx >= self.capacity:\n",
    "            addsz = max(2, self.capacity)\n",
    "            self.array = np.vstack((self.array, np.zeros((addsz, 2), dtype=self.array.dtype)))\n",
    "        self.array[idx] = [v1, v2]\n",
    "        self.length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wiki_link(line):\n",
    "    dest_ttl = line.strip()\n",
    "    if len(dest_ttl) == 0:\n",
    "        return None\n",
    "    dest_ttl = dest_ttl[0].upper() + dest_ttl[1:]\n",
    "    dest_ttl = dest_ttl.split('|', maxsplit=1)[0]\n",
    "    dest_ttl = dest_ttl.split('#', maxsplit=1)[0]\n",
    "    return dest_ttl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PARTITION_SIZE = 16\n",
    "PARTITION_SIZE = 1 << LOG_PARTITION_SIZE\n",
    "N = len(id_map)\n",
    "NUM_PARTITIONS = math.ceil(N / PARTITION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_id_map = ChainMap(id_map, id_map2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6806227"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge format\n",
    "\n",
    "- `edges_{n}.npz` stores the outgoing links from `PARITION_SIZE*n ..< PARTITION_SIZE*(n+1)`\n",
    "- These are stored in a list where element `i` contains the links out to `PARITION_SIZE*i ..< PARTITION_SIZE*(i+1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(iterable, size):\n",
    "    \"\"\"Split an iterable into list chunks of size `n`.\n",
    "    \n",
    "    The last chunk can be fewer than `n` elements long, but it won't be empty.\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(iterator, size))\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "        else:\n",
    "            return\n",
    "\n",
    "def lazy_chunk(iterable, n):\n",
    "    \"\"\"Split an iterable into iterable chunks of size `n`.\n",
    "    \n",
    "    The last chunk can be fewer than `n` elements long, but it won't be empty.\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        try:\n",
    "            first = next(iterator)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        yield itertools.chain([first], itertools.islice(iterator, n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ORDER_TAG_BITS = 3\n",
    "NUM_ORDER_TAGS = 1 << ORDER_TAG_BITS\n",
    "DEST_ID_BITS = 31 - ORDER_TAG_BITS\n",
    "DEST_ID_MASK = (1 << DEST_ID_BITS) - 1\n",
    "def get_edges():\n",
    "    in_degree = np.zeros(N, dtype=np.int32)\n",
    "    out_degree = np.zeros(N, dtype=np.int32)\n",
    "    with tqdm(position=1, miniters=1000) as progress:\n",
    "        iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "        iterator = map(\n",
    "            lambda b: zip(\n",
    "                b[\"id\"].to_numpy(),\n",
    "                b[\"ns\"].to_numpy(),\n",
    "                map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "                b[\"text\"].to_pylist()\n",
    "            ),\n",
    "            iterator\n",
    "        )\n",
    "        iterator = itertools.chain.from_iterable(iterator)\n",
    "        iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "        iterator = enumerate(map(operator.itemgetter(3), iterator))\n",
    "        filenames = files.edge_filenames(NUM_PARTITIONS)\n",
    "        order_tags = np.arange(NUM_ORDER_TAGS, dtype=np.int32)[::-1] << DEST_ID_BITS\n",
    "        for part_idx, subitr in enumerate(lazy_chunk(iterator, PARTITION_SIZE)):\n",
    "            edges = [PairVec('int32') for _ in range(0, N, PARTITION_SIZE)]\n",
    "            for src_id, text in subitr:\n",
    "                link_idx = 0\n",
    "                for link in wikiplain.get_links(text):\n",
    "                    dest_ttl = parse_wiki_link(link)\n",
    "                    if dest_ttl:\n",
    "                        dest_id = id_map.get(dest_ttl)\n",
    "                        dest_id = dest_id if (dest_id is not None) else id_map2.get(dest_ttl)\n",
    "                        if dest_id is not None:\n",
    "                            partition = dest_id >> LOG_PARTITION_SIZE\n",
    "                            edges[partition].append(src_id, order_tags[link_idx] | dest_id)\n",
    "                            in_degree[dest_id] += 1\n",
    "                            out_degree[src_id] += 1\n",
    "                            progress.update()\n",
    "                            link_idx = min(NUM_ORDER_TAGS - 1, link_idx + 1)\n",
    "            with open(filenames[part_idx], \"wb\") as fp:\n",
    "                np.savez(fp, *([vec.array[:vec.length] for vec in edges]))\n",
    "    return in_degree, out_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033b287053074c78ba5fc2df76b0e5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872f966a595f4d00821f13836d083d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236033 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 3\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mset\u001b[39m(edge_fnames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(files\u001b[38;5;241m.\u001b[39medge_filenames(NUM_PARTITIONS))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(files\u001b[38;5;241m.\u001b[39min_degree_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n",
      "\u001b[0;31mAssertionError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(exc)\n\u001b[0;32m---> 13\u001b[0m     in_degree, out_degree \u001b[38;5;241m=\u001b[39m \u001b[43mget_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     edge_fnames \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(files\u001b[38;5;241m.\u001b[39medge_filename_pattern)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(files\u001b[38;5;241m.\u001b[39min_degree_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n",
      "Cell \u001b[0;32mIn[57], line 35\u001b[0m, in \u001b[0;36mget_edges\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dest_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     partition \u001b[38;5;241m=\u001b[39m dest_id \u001b[38;5;241m>>\u001b[39m LOG_PARTITION_SIZE\n\u001b[0;32m---> 35\u001b[0m     \u001b[43medges\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_tags\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlink_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdest_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     in_degree[dest_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     37\u001b[0m     out_degree[src_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m, in \u001b[0;36mPairVec.append\u001b[0;34m(self, v1, v2)\u001b[0m\n\u001b[1;32m     13\u001b[0m     addsz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapacity)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39mzeros((addsz, \u001b[38;5;241m2\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mdtype)))\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m [v1, v2]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "edge_fnames = glob.glob(files.edge_filename_pattern)\n",
    "try:\n",
    "    assert set(edge_fnames) == set(files.edge_filenames(NUM_PARTITIONS))\n",
    "    with open(files.in_degree_filename, \"rb\") as fp:\n",
    "        in_degree = np.load(fp)\n",
    "    with open(files.out_degree_filename, \"rb\") as fp:\n",
    "        out_degree = np.load(fp)\n",
    "    # for fname in edge_fnames:\n",
    "    #    with open(fname, \"rb\") as fp:\n",
    "    #         assert len(pickle.load(fp)) == NUM_PARTITIONS\n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "    in_degree, out_degree = get_edges()\n",
    "    edge_fnames = glob.glob(files.edge_filename_pattern)\n",
    "    with open(files.in_degree_filename, \"wb\") as fp:\n",
    "        np.save(fp, in_degree)\n",
    "    with open(files.out_degree_filename, \"wb\") as fp:\n",
    "        np.save(fp, out_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_edges time to complete\n",
    "\n",
    "| date | links/s | time |\n",
    "| :--- | :------ | :--- |\n",
    "| 2024-02-06 | 101000 | 46min 48s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_dab_array():\n",
    "#     result = np.zeros(N, dtype=np.bool8)\n",
    "#     dab_proc = subprocess.Popen(\n",
    "#         [\"wikiplain\", \"--fraction\", \"1\", \"-c\", \"only-dab\", \"--ns\", \"0\", files.enwiki_database_filename],\n",
    "#         stdout=subprocess.PIPE,\n",
    "#         stderr=subprocess.PIPE\n",
    "#     )\n",
    "#     iterator = make_links_iter(dab_proc.stdout)\n",
    "#     iterator = tqdm(iterator, position=0, total=len(id_map))\n",
    "#     iterator = map(lambda pair: (pair[0].decode(\"utf-8\"), pair[1]), iterator)\n",
    "#     for n, subitr in enumerate(lazy_chunk(iterator, PARTITION_SIZE)):\n",
    "#         for ttl, text in subitr:\n",
    "#             src_id = id_map[ttl]\n",
    "#             if len(text) > 0:\n",
    "#                 result[src_id] = True\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDF = pl.scan_parquet(files.enwiki_parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     with open(files.dab_array_filename, \"rb\") as fp:\n",
    "#         dab_array = pickle.load(fp)\n",
    "# except Exception as exc:\n",
    "#     print(exc)\n",
    "#     dab_array = get_dab_array()\n",
    "#     with open(files.dab_array_filename, \"wb\") as fp:\n",
    "#         pickle.dump(dab_array, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_adjacency_matrix_slice(partition, progress):\n",
    "    \"\"\"Computes the slice of the adjacency matrix A starting at row p*S and ending before row (p+1)*S\n",
    "    \n",
    "    p=partition, S=PARTITION_SIZE, and A is defined so that\n",
    "    A @ np.eye(N)[i] = v, a probability vector where\n",
    "        v[j] = out-degree(i) > 0 | count((i,j) in E) / out-degree(i)\n",
    "               otherwise         | 0\n",
    "    \"\"\"\n",
    "    origin_row = partition * PARTITION_SIZE\n",
    "    n_rows = min(PARTITION_SIZE, N - origin_row)\n",
    "    index_arrs = []\n",
    "    value_arrs = []\n",
    "    pkey = f'arr_{partition}'\n",
    "    for fname in glob.glob(files.edge_filename_pattern):\n",
    "        with np.load(fname) as npz:\n",
    "            vec = npz[pkey]\n",
    "        vec[:, 1] &= DEST_ID_MASK  # remove order_tag\n",
    "        # vec is\n",
    "        #  [[src_id_0, dest_id_0],\n",
    "        #   [src_id_1, dest_id_1],\n",
    "        #   ...\n",
    "        #  ]\n",
    "        # Sort by (src,dest), make unique and get counts\n",
    "        key_arr = (vec[:, 0].astype('int64') << 32) | vec[:, 1]\n",
    "        _, order, count = np.unique(key_arr, return_index=True, return_counts=True)\n",
    "        vec = vec[order]\n",
    "        # Normalize `count` based on (src,)\n",
    "        count = count.astype('float64') / out_degree[vec[:, 0]]\n",
    "        index_arrs.append(vec)\n",
    "        value_arrs.append(count)\n",
    "        progress.update()\n",
    "    index_arr = np.vstack(index_arrs)\n",
    "    matrix_slice = scipy.sparse.csr_array(\n",
    "        (np.hstack(value_arrs), (index_arr[:, 1] - origin_row, index_arr[:, 0])),\n",
    "        shape=(n_rows, N),\n",
    "        dtype=np.float64\n",
    "    )\n",
    "    return matrix_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f692df5316c241069125b4f0c2841979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tqdm(total=NUM_PARTITIONS**2) as progress:\n",
    "    for partition in range(NUM_PARTITIONS):\n",
    "        adj_matrix = compute_adjacency_matrix_slice(partition, progress)\n",
    "        scipy.sparse.save_npz(files.adjacency_filename(partition), adj_matrix, compressed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     5,    19,    84,   406,  1189, 16248])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(out_degree, [0, 0.1, 0.5, 0.9, 0.99, 0.999, 1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_out_degree = np.log(out_degree + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_out_degree /= log_out_degree.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WMf5L5VEqZb"
   },
   "source": [
    "### Global PageRank\n",
    "\n",
    "The initial rank is a column vector $\\mathbf{r}$ where $\\mathbf{r}_i = \\frac{1}{N}$\n",
    "\n",
    "The transition matrix $\\mathbf{M}$ is N x N; each column represents a source, and each row represents a destination.\n",
    "$\\mathbf{M}_{ij} = P(\\text{next}=i\\,|\\,\\text{current}=j)$. Each column **must** sum to 1 for the calculation to be stable, so if page $j$ contains no links, it is treated as if it had a link to every page.\n",
    "\n",
    "The power method iteratively computes better ranks: $\\mathbf{r'} = (1 - \\alpha) \\mathbf{M}\\mathbf{r} + \\frac{\\alpha}{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalized PageRank\n",
    "\n",
    "Personalized PageRank uses a preference vector $\\mathbf{p}$ in place of the uniform $\\frac{1}{N}$ for _teleportation_. Pages with no out-links still use a uniform distribution. The initial rank can be any vector, because of the converging property of the power method (explanation at https://mathworld.wolfram.com/Eigenvector.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WMf5L5VEqZb"
   },
   "source": [
    "### Ending iteration\n",
    "\n",
    "At each iteration, we calculate the [perplexity](https://en.wikipedia.org/wiki/Perplexity) of the PageRank distribution, where perplexity is defined as 2 raised to the [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) of the PageRank distribution, i.e., $2^{H(PR)}$. The initial guess is at maximum entropy, so the first iteration causes perplexity to decrease. Later iterations may change perplexity in either direction; we stop when the change is below a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "wsL0yQKvKqAC"
   },
   "outputs": [],
   "source": [
    "def perplexity(distribution):\n",
    "    return np.power(2, np.sum(-distribution * np.log2(distribution)))\n",
    "\n",
    "def personalized_page_rank(preference, threshold=1, random_jump_prob=0.15):\n",
    "    current_rank = np.ones(N, dtype=np.float64) / N\n",
    "    next_rank = np.zeros(N, dtype=np.float64)\n",
    "    # iteratively update current_rank\n",
    "    edge_follow_prob = 1 - random_jump_prob\n",
    "    prev_perplexity = float('inf')\n",
    "    current_perplexity = perplexity(current_rank)\n",
    "    current_iter = 0\n",
    "    iter_start = time.time()\n",
    "    print(\"Itr# | ΔPerplexity     | Seconds\")\n",
    "    while abs(prev_perplexity - current_perplexity) > threshold:\n",
    "        current_iter += 1\n",
    "        next_rank[:] = random_jump_prob * preference\n",
    "        # update destinations from non-sink nodes (N x N times N x 1 -> N x 1)\n",
    "        spread_probs = np.vstack([\n",
    "            adjacency_matrix_slice.dot(current_rank[:, np.newaxis])\n",
    "            for adjacency_matrix_slice in map(scipy.sparse.load_npz, files.adjacency_filenames(NUM_PARTITIONS))\n",
    "        ])\n",
    "        next_rank += edge_follow_prob * spread_probs[:, 0]  # make column vector 1-D\n",
    "        # update destinations from sink nodes\n",
    "        next_rank[:] += edge_follow_prob * current_rank[out_degree == 0].sum() / N\n",
    "        # copy `next_rank` values into `current_rank``\n",
    "        current_rank[:] = next_rank\n",
    "        # --\n",
    "        # compute perplexity and progress\n",
    "        prev_perplexity = current_perplexity\n",
    "        current_perplexity = perplexity(current_rank)\n",
    "        next_iter_start = time.time()\n",
    "        print(\"{:<3d}    {:<15.6f}   {:.3f}\".format(current_iter,\n",
    "                                                    current_perplexity - prev_perplexity,\n",
    "                                                    next_iter_start - iter_start))\n",
    "        iter_start = next_iter_start\n",
    "    df = pl.DataFrame({\n",
    "        \"title\": id_map.keys(), \"value\": next_rank, \"in_deg\": in_degree, \"out_deg\": out_degree,\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "wsL0yQKvKqAC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr# | ΔPerplexity     | Seconds\n",
      "1      -6013796.145013   6.664\n",
      "2      145977.970099     4.352\n",
      "3      -38511.688381     3.560\n",
      "4      -2270.893838      3.486\n",
      "5      -8357.308689      3.434\n",
      "6      -2717.844135      3.383\n",
      "7      -3166.973678      3.376\n",
      "8      -1640.290433      3.345\n",
      "9      -1428.332263      3.334\n",
      "10     -892.535524       3.370\n",
      "11     -699.347582       3.386\n",
      "12     -469.568557       3.320\n",
      "13     -357.639085       3.311\n",
      "14     -246.562236       3.303\n",
      "15     -186.548257       3.304\n",
      "16     -130.940713       3.338\n",
      "17     -98.650553        3.296\n",
      "18     -70.132001        3.306\n",
      "19     -52.927872        3.314\n",
      "20     -37.852054        3.279\n",
      "21     -28.705336        3.289\n",
      "22     -20.618140        3.291\n",
      "23     -15.707255        3.288\n",
      "24     -11.320650        3.305\n",
      "25     -8.670001         3.291\n",
      "26     -6.260557         3.283\n",
      "27     -4.822257         3.265\n",
      "28     -3.486686         3.308\n",
      "29     -2.700787         3.254\n",
      "30     -1.954268         3.287\n",
      "31     -1.522622         3.266\n",
      "32     -1.101887         3.280\n",
      "33     -0.863597         3.281\n"
     ]
    }
   ],
   "source": [
    "# Run until perplexity changes by less than 1\n",
    "PR = personalized_page_rank(log_out_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_sorted = PR.sort('value', descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349e120810db4e5dafd329324bfa66de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='page', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function nbhelpers.polars.pager.<locals>.<lambda>(page)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pager(PR_sorted.slice(0, 2000), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51179153b8e43919b986ac9b281ae35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='q'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function nbhelpers.polars.searcher.<locals>.searcher_run(q)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher(\n",
    "    PR_sorted.slice(0, 200000).with_columns(pl.Series(\"rank\", range(200000))).select([\"rank\", *PR_sorted.columns]),\n",
    "    ['title'],\n",
    "    20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR.write_parquet(files.pagerank_parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNw97vMeu3UGKrk6j3TYQO8",
   "include_colab_link": true,
   "name": "PageRank.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
