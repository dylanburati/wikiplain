{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "## PageRank on Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "1. Download a dump of Wikipedia's articles, named `enwiki-{date_string}-pages-articles-multistream.xml.bz2`\n",
    "2. Download the `enwiki-{date_string}-pages-articles-multistream-index.txt.bz2` file\n",
    "3. Move those files into the same folder, removing the `enwiki-{date_string}` prefix\n",
    "4. Process the `xml.bz2` file into a Parquet file using `wikiplain.load_bz2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import glob\n",
    "import gzip\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import socket\n",
    "import struct\n",
    "import subprocess\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "from collections import ChainMap, defaultdict, deque\n",
    "from contextlib import asynccontextmanager\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from enum import Enum, auto\n",
    "from functools import lru_cache, partial\n",
    "from urllib.parse import urlencode, urlsplit, quote as urlquote, unquote as urlunquote\n",
    "from typing import Any, Awaitable, Callable, Literal, TypeVar\n",
    "\n",
    "import cbor2\n",
    "import numpy as np\n",
    "import pypocketmap as pkm\n",
    "import pyarrow.parquet as pq\n",
    "import polars as pl\n",
    "import sqlalchemy as sa\n",
    "import scipy.sparse\n",
    "from dotenv import load_dotenv\n",
    "from ipywidgets import interact\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import select, text as sqltext\n",
    "from tqdm.auto import tqdm\n",
    "from arsenal.datastructures.unionfind import UnionFind\n",
    "from arsenal.datastructures.heap import MinMaxHeap\n",
    "\n",
    "import wikiplain\n",
    "from wikiplain import Token, TokenKind as TK\n",
    "from nbhelpers.polars import pager, searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.Config.set_fmt_str_lengths(160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageRankFiles:\n",
    "    def __init__(self, date_string):\n",
    "        self.date_string = date_string\n",
    "        self.enwiki_dir = f\"{os.environ['ENWIKI_DIR']}/{date_string}\"\n",
    "        self.parquet_dir = os.environ.get('ENWIKI_PARQUET_DIR', self.enwiki_dir)\n",
    "        try:\n",
    "            os.mkdir(f\"{self.enwiki_dir}/pagerank\")\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "    \n",
    "    @property\n",
    "    def enwiki_parquet_filename(self):\n",
    "        return f\"{self.parquet_dir}/enwiki_{self.date_string}.parquet\"\n",
    "    \n",
    "    @property\n",
    "    def pagerank_parquet_filename(self):\n",
    "        return f\"{self.parquet_dir}/enwiki_{self.date_string}_pagerank.parquet\"\n",
    "\n",
    "    @property\n",
    "    def nub_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/nub.bin\"\n",
    "    \n",
    "    @property\n",
    "    def id_map_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/id_map.bin\"\n",
    "    \n",
    "    @property\n",
    "    def id_map2_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/id_map2.bin\"\n",
    "    \n",
    "    @property\n",
    "    def dense_id_arr_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/dense_id_arr.npy\"\n",
    "    \n",
    "    @property\n",
    "    def edge_filename_pattern(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/edges_*.npz\"\n",
    "    \n",
    "    def edge_filenames(self, num_partitions):\n",
    "        return [\n",
    "            f\"{self.enwiki_dir}/pagerank/edges_{i}.npz\"\n",
    "            for i in range(num_partitions)\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def in_degree_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/in_degree.npy\"\n",
    "    \n",
    "    @property\n",
    "    def out_degree_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/out_degree.npy\"\n",
    "    \n",
    "    def adjacency_filename(self, partition):\n",
    "        return f\"{self.enwiki_dir}/pagerank/adjacency_{partition}.npz\"\n",
    "    \n",
    "    def adjacency_filenames(self, num_partitions):\n",
    "        return [self.adjacency_filename(i) for i in range(num_partitions)]\n",
    "\n",
    "    @property\n",
    "    def disambig_arr_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/disambig_arr.npy\"\n",
    "    \n",
    "    @property\n",
    "    def moved_article_set_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/moved_article_set.bin\"\n",
    "    \n",
    "    @property\n",
    "    def top_cite_domains_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/top_cite_domains.bin\"\n",
    "    \n",
    "    @property\n",
    "    def hatedge_filename_pattern(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/hatedges_*.npy\"\n",
    "    \n",
    "    def hatedge_filenames(self, num_partitions):\n",
    "        return [\n",
    "            f\"{self.enwiki_dir}/pagerank/hatedges_{i}.npy\"\n",
    "            for i in range(num_partitions)\n",
    "        ]\n",
    "    \n",
    "    @property\n",
    "    def hatcheck_arr_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/hatcheck_arr.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = PageRankFiles(\"20240401\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Find title collisions\n",
    "\n",
    "1. There are some pages with the same title - I think this is caused by pages deleted and recreated while the snapshot is in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pqf = pq.ParquetFile(files.enwiki_parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overwritten():\n",
    "    overwritten = set()\n",
    "    timestamp_map = {}\n",
    "    article_ids = {}\n",
    "    pqf_size = 0\n",
    "    for batch in tqdm(pqf.iter_batches(batch_size=100), total=pqf.num_row_groups):\n",
    "        for aid, ns, ttl, tm in zip(batch[\"id\"].to_numpy(), batch[\"ns\"].to_numpy(), batch[\"title\"].to_pylist(), batch[\"timestamp\"].to_pylist()):\n",
    "            pqf_size += 1\n",
    "            if ns != 0:\n",
    "                continue\n",
    "            tm = np.datetime64(tm)\n",
    "            other_id = article_ids.setdefault(ttl, aid)\n",
    "            if other_id != aid:\n",
    "                if (timestamp_map[ttl], other_id) < (tm, aid):\n",
    "                    print(f\"{ttl!r}: {aid} > {other_id}\")\n",
    "                    overwritten.add(other_id)\n",
    "                    article_ids[ttl] = aid\n",
    "                    timestamp_map[ttl] = tm\n",
    "                else:\n",
    "                    print(f\"{ttl!r}: {other_id} > {aid}\")\n",
    "                    overwritten.add(aid)\n",
    "            else:\n",
    "                timestamp_map[ttl] = tm\n",
    "    return overwritten, pqf_size\n",
    "\n",
    "try:\n",
    "    with open(files.nub_filename, \"rb\") as fp:\n",
    "        overwritten, pqf_size = cbor2.load(fp)\n",
    "except Exception:\n",
    "    overwritten, pqf_size = get_overwritten()\n",
    "    overwritten = {int(e) for e in overwritten}\n",
    "    with open(files.nub_filename, \"wb\") as fp:\n",
    "        cbor2.dump((overwritten, pqf_size), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Build representation of articles/links as a graph\n",
    "\n",
    "1. Create `id_map` from non-redirecting article titles to node number, and `id_map2` from redirecting article titles to node number\n",
    "2. Use `wikiplain` to extract link titles, and use above maps to convert to (src_id, dest_id) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vec:\n",
    "    def __init__(self, dtype):\n",
    "        self.array = np.ndarray((1024,), dtype=dtype)\n",
    "        self.length = 0\n",
    "    \n",
    "    @property\n",
    "    def capacity(self):\n",
    "        return self.array.shape[0]\n",
    "\n",
    "    def append(self, v):\n",
    "        idx = self.length\n",
    "        if idx >= self.capacity:\n",
    "            addsz = max(2, self.capacity)\n",
    "            self.array = np.hstack((self.array, np.zeros((addsz,), dtype=self.array.dtype)))\n",
    "        self.array[idx] = v\n",
    "        self.length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairVec:\n",
    "    def __init__(self, dtype):\n",
    "        self.array = np.ndarray((1024, 2), dtype=dtype)\n",
    "        self.length = 0\n",
    "    \n",
    "    @property\n",
    "    def capacity(self):\n",
    "        return self.array.shape[0]\n",
    "\n",
    "    def append(self, v1, v2):\n",
    "        idx = self.length\n",
    "        if idx >= self.capacity:\n",
    "            addsz = max(2, self.capacity)\n",
    "            self.array = np.vstack((self.array, np.zeros((addsz, 2), dtype=self.array.dtype)))\n",
    "        self.array[idx] = [v1, v2]\n",
    "        self.length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(iterable, size):\n",
    "    \"\"\"Split an iterable into list chunks of size `n`.\n",
    "    \n",
    "    The last chunk can be fewer than `n` elements long, but it won't be empty.\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(iterator, size))\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "        else:\n",
    "            return\n",
    "\n",
    "def lazy_chunk(iterable, n):\n",
    "    \"\"\"Split an iterable into iterable chunks of size `n`.\n",
    "    \n",
    "    The last chunk can be fewer than `n` elements long, but it won't be empty.\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        try:\n",
    "            first = next(iterator)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        yield itertools.chain([first], itertools.islice(iterator, n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterdecode(f):\n",
    "    decoder = cbor2.CBORDecoder(f)\n",
    "    while True:\n",
    "        try:\n",
    "            yield decoder.decode()\n",
    "        except EOFError:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_maps():\n",
    "    redirect_group_map = UnionFind()\n",
    "    id_map = pkm.create(str, int)\n",
    "    redirect_lst = []\n",
    "    dense_ids = Vec(dtype=np.int64)\n",
    "    for batch in tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100)):\n",
    "        for aid, ns, ttl, redir in zip(batch[\"id\"].to_numpy(), batch[\"ns\"].to_numpy(), batch[\"title\"].to_pylist(), batch[\"redirect\"].to_pylist()):\n",
    "            if ns != 0 or aid in overwritten:\n",
    "                continue\n",
    "            if redir is not None:\n",
    "                redirect_group_map.union(ttl, redir)\n",
    "                redirect_lst.append(ttl)\n",
    "            else:\n",
    "                assert ttl not in id_map, f\"Expected unique titles, got second instance of {ttl}\"\n",
    "                dense_ids.append(aid)\n",
    "                id_map[ttl] = len(id_map)\n",
    "    id_map2 = pkm.create(str, int)\n",
    "    for group in redirect_group_map.classes():\n",
    "        centers = [ttl for ttl in group if ttl in id_map]\n",
    "        if len(centers) == 0:\n",
    "            continue\n",
    "        assert len(centers) == 1, str(centers)\n",
    "        for ttl in group:\n",
    "            if ttl != centers[0]:\n",
    "                id_map2[ttl] = id_map[centers[0]]\n",
    "    return id_map, id_map2, dense_ids.array[:dense_ids.length]\n",
    "\n",
    "try:\n",
    "    with open(files.id_map_filename, \"rb\") as fp:\n",
    "        id_map = pkm.create(str, int)\n",
    "        for k, v in iterdecode(fp):\n",
    "            id_map[k] = v\n",
    "    with open(files.id_map2_filename, \"rb\") as fp:\n",
    "        id_map2 = pkm.create(str, int)\n",
    "        for k, v in iterdecode(fp):\n",
    "            id_map2[k] = v\n",
    "    with open(files.dense_id_arr_filename, \"rb\") as fp:\n",
    "        dense_id_arr = np.load(fp)\n",
    "except Exception:\n",
    "    id_map, id_map2, dense_id_arr = get_id_maps()\n",
    "    with open(files.id_map_filename, \"wb\") as fp:\n",
    "        for k, v in id_map.items():\n",
    "            cbor2.dump((k, v), fp)\n",
    "    with open(files.id_map2_filename, \"wb\") as fp:\n",
    "        for k, v in id_map2.items():\n",
    "            cbor2.dump((k, v), fp)\n",
    "    with open(files.dense_id_arr_filename, \"wb\") as fp:\n",
    "        np.save(fp, dense_id_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6806227, 10873732)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_map), len(id_map2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Martín Vázquez',\n",
       " 'Rouverol',\n",
       " 'Ron Hansell',\n",
       " 'Jim Wolf (musician)',\n",
       " 'McAllen Miller International Airport',\n",
       " '1937 Albanian National Championship',\n",
       " 'Manilius (crater)',\n",
       " 'Nerka Lake',\n",
       " 'Metallacarboxylic acid',\n",
       " 'Nicolas Appert']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.islice(iter(id_map.keys()), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Office for fair access',\n",
       " 'The Catalyst (newspaper)',\n",
       " 'Scarecrow Press historical dictionary series',\n",
       " 'Boissieri',\n",
       " 'Edward Northey (disambiguation)',\n",
       " 'Herzogschloss Zweibrücken',\n",
       " 'Trinity Square (disambiguation)',\n",
       " 'Urban cowboy',\n",
       " 'K 424a',\n",
       " 'Nawaz Shareef']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.islice(iter(id_map2.keys()), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_template_id_maps():\n",
    "#     redirect_group_map = UnionFind()\n",
    "#     template_id_map = {}\n",
    "#     for batch in tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100)):\n",
    "#         for aid, ns, ttl, redir in zip(batch[\"id\"].to_numpy(), batch[\"ns\"].to_numpy(), batch[\"title\"].to_pylist(), batch[\"redirect\"].to_pylist()):\n",
    "#             if ns != 10 or aid in overwritten:\n",
    "#                 continue\n",
    "#             if redir is not None:\n",
    "#                 redirect_group_map.union(ttl, redir)\n",
    "#             else:\n",
    "#                 assert ttl not in template_id_map, f\"Expected unique titles, got second instance of {ttl}\"\n",
    "#                 template_id_map[ttl] = aid\n",
    "#     template_id_map2 = {}\n",
    "#     for group in redirect_group_map.classes():\n",
    "#         centers = [ttl for ttl in group if ttl in template_id_map]\n",
    "#         if len(centers) == 0:\n",
    "#             continue\n",
    "#         assert len(centers) == 1, str(centers)\n",
    "#         for ttl in group:\n",
    "#             if ttl != centers[0]:\n",
    "#                 template_id_map2[ttl] = template_id_map[centers[0]]\n",
    "#     return template_id_map, template_id_map2\n",
    "\n",
    "# try:\n",
    "#     with open(files.template_id_maps_filename, \"rb\") as fp:\n",
    "#         template_id_map, template_id_map2 = pickle.load(fp)\n",
    "# except FileNotFoundError:\n",
    "#     template_id_map, template_id_map2 = get_template_id_maps()\n",
    "#     with open(files.template_id_maps_filename, \"wb\") as fp:\n",
    "#         pickle.dump((template_id_map, template_id_map2), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia article stats\n",
    "\n",
    "1. Create `disambig_arr`, a simple boolean array recording whether each article is a disambiguation or set-index page.\n",
    "2. Create `moved_article_set`, a set of article titles which redirect because their article content was moved.\n",
    "3. Create `top_cite_domains`, the 1024 most commonly cited websites across all articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disambig_arr():\n",
    "    iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "    iterator = map(\n",
    "        lambda b: zip(\n",
    "            b[\"id\"].to_numpy(),\n",
    "            b[\"ns\"].to_numpy(),\n",
    "            map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "            b[\"text\"].to_pylist()\n",
    "        ),\n",
    "        iterator\n",
    "    )\n",
    "    iterator = itertools.chain.from_iterable(iterator)\n",
    "    iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "    iterator = enumerate(map(operator.itemgetter(3), iterator))\n",
    "    disambig_arr = np.zeros(N, dtype=np.bool_)\n",
    "    for node_id, text in iterator:\n",
    "        disambig_arr[node_id] = wikiplain.is_disambiguation_page(text)\n",
    "    return disambig_arr\n",
    "\n",
    "try:\n",
    "    with open(files.disambig_arr_filename, \"rb\") as fp:\n",
    "        disambig_arr = np.load(fp)\n",
    "except Exception:\n",
    "    disambig_arr = get_disambig_arr()\n",
    "    with open(files.disambig_arr_filename, \"wb\") as fp:\n",
    "        np.save(fp, disambig_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moved_article_set():\n",
    "    iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "    iterator = map(\n",
    "        lambda b: zip(\n",
    "            b[\"id\"].to_numpy(),\n",
    "            b[\"ns\"].to_numpy(),\n",
    "            map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "            b[\"title\"].to_pylist(),\n",
    "            map(b[\"text\"].__getitem__, itertools.count()),\n",
    "        ),\n",
    "        iterator\n",
    "    )\n",
    "    iterator = itertools.chain.from_iterable(iterator)\n",
    "    iterator = filter(lambda e: e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "    iterator = map(lambda e: (e[3], e[4]), iterator)\n",
    "    moved_article_set = set()\n",
    "    for title, text in iterator:\n",
    "        if '{{R from move' in text.as_py():\n",
    "            moved_article_set.add(title)\n",
    "    return moved_article_set\n",
    "\n",
    "try:\n",
    "    with open(files.moved_article_set_filename, 'rb') as fp:\n",
    "        moved_article_set = cbor2.load(fp)\n",
    "except FileNotFoundError:\n",
    "    moved_article_set = get_moved_article_set()\n",
    "    with open(files.moved_article_set_filename, 'wb') as fp:\n",
    "        cbor2.dump(moved_article_set, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_cite_domains():\n",
    "    iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "    iterator = map(\n",
    "        lambda b: zip(\n",
    "            b[\"id\"].to_numpy(),\n",
    "            b[\"ns\"].to_numpy(),\n",
    "            map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "            b[\"text\"].to_pylist()\n",
    "        ),\n",
    "        iterator\n",
    "    )\n",
    "    iterator = itertools.chain.from_iterable(iterator)\n",
    "    iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "    iterator = enumerate(map(operator.itemgetter(3), iterator))\n",
    "    heap = MinMaxHeap()\n",
    "    heap_phases = 9\n",
    "    heap_limits = [\n",
    "        (750000*i, 1024 << (heap_phases-i-1)) for i in range(heap_phases)\n",
    "    ]\n",
    "    heap_phase = 0\n",
    "    heap_limit = heap_limits[heap_phase][1]\n",
    "    for node_id, text in iterator:\n",
    "        if heap_phase+1 < heap_phases and node_id < heap_limits[heap_phase+1][0]:\n",
    "            heap_phase += 1\n",
    "            heap_limit = heap_limits[heap_phase][1]\n",
    "            while len(heap) > heap_limit:\n",
    "                heap.popmin()\n",
    "        page = defaultdict(int)\n",
    "        for url in wikiplain.get_cite_urls(text):\n",
    "            full_domain = re.sub(r\"[:/].*\", \"\", url)\n",
    "            parts = full_domain.split('.')\n",
    "            if len(parts) >= 2:\n",
    "                site_domain = parts[-2] + '.' + parts[-1]\n",
    "                if site_domain in SECOND_LEVEL_DOMAINS:\n",
    "                    if len(parts) >= 3:\n",
    "                        site_domain = parts[-3] + '.' + site_domain\n",
    "                    else:\n",
    "                        continue\n",
    "                page[site_domain] += 1\n",
    "        for k, v in page.items():\n",
    "            if k in heap:\n",
    "                heap[k] = heap.max[k] + v\n",
    "            elif len(heap) < heap_limit:\n",
    "                heap[k] = v\n",
    "            elif v > heap.peekmin()[1]:\n",
    "                heap.popmin()\n",
    "                heap[k] = v\n",
    "    top_cite_domains = []\n",
    "    while len(heap) > 0:\n",
    "        top_cite_domains.append(heap.popmax())\n",
    "    return top_cite_domains\n",
    "\n",
    "try:\n",
    "    with open(files.top_cite_domains_filename, \"rb\") as fp:\n",
    "        top_cite_domains = cbor2.load(fp)\n",
    "except Exception:\n",
    "    top_cite_domains = get_top_cite_domains()\n",
    "    with open(files.top_cite_domains_filename, \"wb\") as fp:\n",
    "        cbor2.dump(top_cite_domains, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hatedge_arr\n",
    "\n",
    "Find links to highly related pages, displayed at the top using one of several Wikitext templates. This is used by RedditRank.ipynb, however it's not feasible to compute it there because opening the enwiki parquet takes too much memory (needed more by other computations in that notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_token(tc, c):\n",
    "    return [Token(kind=TK.Content, data=piece) for piece in tc.split(c)]\n",
    "\n",
    "def split_template(line):\n",
    "    finished = []\n",
    "    tdepth = 0\n",
    "    ldepth = 0\n",
    "    tokens = wikiplain.tokenize(line)\n",
    "    acc = []\n",
    "    for tok in tokens:\n",
    "        match tok:\n",
    "            case Token(kind=TK.Content, data=tc):\n",
    "                if tdepth > 0 or ldepth > 0:\n",
    "                    acc.append(tok)\n",
    "                elif tc != '':\n",
    "                    parts = split_token(tc, '|')\n",
    "                    if parts[0].data != '':\n",
    "                        acc.append(parts[0])\n",
    "                    if len(parts) > 1:\n",
    "                        finished.append(acc)\n",
    "                        if parts[-1].data != '':\n",
    "                            acc = [parts.pop()]\n",
    "                        for p in parts[1:]:\n",
    "                            finished.append([p])\n",
    "            case Token(kind=TK.LinkStart):\n",
    "                ldepth += 1\n",
    "                acc.append(tok)\n",
    "            case Token(kind=TK.LinkEnd):\n",
    "                ldepth -= 1\n",
    "                acc.append(tok)\n",
    "            case Token(kind=TK.TemplateStart):\n",
    "                tdepth += 1\n",
    "                acc.append(tok)\n",
    "            case Token(kind=TK.TemplateEnd):\n",
    "                tdepth -= 1\n",
    "                acc.append(tok)\n",
    "    if acc:\n",
    "        finished.append(acc)\n",
    "    return finished\n",
    "\n",
    "def tokenlist_string(tl):\n",
    "    if len(tl) != 1 or tl[0].kind != TK.Content:\n",
    "        return None\n",
    "    return tl[0].data\n",
    "\n",
    "def token_fmt(tok):\n",
    "    match tok:\n",
    "        case Token(kind=TK.Content, data=tc):\n",
    "            return tc\n",
    "        case Token(kind=TK.LinkStart):\n",
    "            return '[['\n",
    "        case Token(kind=TK.LinkEnd):\n",
    "            return ']]'\n",
    "        case Token(kind=TK.TemplateStart):\n",
    "            return '{{'\n",
    "        case Token(kind=TK.TemplateEnd):\n",
    "            return '}}'\n",
    "\n",
    "def tokenlist_gettext(tl):\n",
    "    return \"\".join(map(token_fmt, tl))\n",
    "    \n",
    "def tokenlist_startswith(tl, s):\n",
    "    if len(tl) == 0 or tl[0].kind != TK.Content:\n",
    "        return False\n",
    "    return tl[0].data.startswith(s)\n",
    "\n",
    "def tokenlist_links(tl):\n",
    "    accs = []\n",
    "    for tok in tl:\n",
    "        match tok:\n",
    "            case Token(kind=TK.Content, data=tc):\n",
    "                if accs:\n",
    "                    accs[-1].append(tc)\n",
    "            case Token(kind=TK.LinkStart):\n",
    "                accs.append([])\n",
    "            case Token(kind=TK.LinkEnd):\n",
    "                if accs:\n",
    "                    yield \"\".join(accs.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linking the reader to other articles with similar titles or concepts\n",
    "# that they may have been seeking instead\n",
    "HATNOTE_ABOUT = 1\n",
    "# placed at the top of the article or section that is the primary topic\n",
    "# of a redirect, and links to other topics that are ambiguous with the\n",
    "# name of that redirect\n",
    "HATNOTE_REDIRECT = 2\n",
    "# concise about / other uses\n",
    "HATNOTE_FOR = 3\n",
    "# \"not to be confused with\"\n",
    "HATNOTE_DISTINGUISH = 4\n",
    "def parse_hatnote(line):\n",
    "    lst = split_template(line)\n",
    "    if len(lst) == 0:\n",
    "        return\n",
    "    tmpl_toks, *args = lst\n",
    "    tmpl = tokenlist_string(tmpl_toks)\n",
    "    if not tmpl:\n",
    "        return\n",
    "    tmpl = tmpl[0].upper() + tmpl[1:]\n",
    "    if tmpl == 'About':\n",
    "        # the Latin letter|the similar Greek letter|Alpha|the similar Cyrillic letter|A (Cyrillic)|other uses\n",
    "        for dest_ttl in map(tokenlist_gettext, args[2::2]):\n",
    "            yield HATNOTE_ABOUT, dest_ttl\n",
    "    elif tmpl == 'Redirect':\n",
    "        # Achilleus|the Roman usurper with this name|Aurelius Achilleus|other uses|Achilles (disambiguation)\n",
    "        for dest_ttl in map(tokenlist_gettext, args[2::2]):\n",
    "            yield HATNOTE_REDIRECT, dest_ttl\n",
    "    elif tmpl == 'For':\n",
    "        # the racehorse|Ambiorix (horse)\n",
    "        for dest_ttl in map(tokenlist_gettext, args[2::2]):\n",
    "            yield HATNOTE_FOR, dest_ttl\n",
    "    elif tmpl == 'Redirect-synonym':\n",
    "        # Wild cranberry|[[Arctostaphylos uva-ursi]]\n",
    "        for dest_ttl in map(tokenlist_gettext, args[2::2]):\n",
    "            yield HATNOTE_REDIRECT, dest_ttl\n",
    "    elif tmpl == 'About-distinguish':\n",
    "        if len(args) > 1:\n",
    "            yield HATNOTE_DISTINGUISH, tokenlist_gettext(args[1])\n",
    "    elif tmpl == 'Redirect2':\n",
    "        # Anarchist|Anarchists|other uses|Anarchist (disambiguation)\n",
    "        for dest_ttl in map(tokenlist_gettext, args[2::2]):\n",
    "            yield HATNOTE_REDIRECT, dest_ttl\n",
    "    elif tmpl == 'Redirect-multi':\n",
    "        # Redirect-multi|2|Oscars|The Oscar|other uses|Oscar (disambiguation)\n",
    "        try:\n",
    "            skip = int(tokenlist_string(args[1]))\n",
    "        except (TypeError, ValueError):\n",
    "            return\n",
    "        for dest_ttl in map(tokenlist_gettext, args[2::2]):\n",
    "            yield HATNOTE_REDIRECT, dest_ttl\n",
    "    elif tmpl == 'Distinguish' or tmpl == 'Redirect-distinguish-text':\n",
    "        # distinguish|text=[[Lucius Appuleius Saturninus]], a Roman demagogue, or others with the name Apuleius or [[Appuleia (gens)|Appuleius]]\n",
    "        for tl in args:\n",
    "            if (\n",
    "                tmpl == 'Redirect-distinguish-text'\n",
    "                or tokenlist_startswith(tl, 'text=')\n",
    "                or tokenlist_startswith(tl, 'Text=')\n",
    "            ):\n",
    "                for link_inner in tokenlist_links(tl):\n",
    "                    dest = link_inner.split('|', maxsplit=1)[0]\n",
    "                    yield HATNOTE_DISTINGUISH, dest\n",
    "            else:\n",
    "                yield HATNOTE_DISTINGUISH, tokenlist_gettext(tl)\n",
    "    elif tmpl == 'Redirect-distinguish':\n",
    "        # ethyne|ethane|ethene\n",
    "        for dest_ttl in map(tokenlist_gettext, args[1:]):\n",
    "            yield HATNOTE_DISTINGUISH, dest_ttl\n",
    "    elif tmpl == 'Redirect-distinguish-for':\n",
    "        # Phoebus|Phobos (mythology)|other uses|Phoebus (disambiguation)\n",
    "        if len(args) > 1:\n",
    "            yield HATNOTE_DISTINGUISH, tokenlist_gettext(args[1])\n",
    "            for dest_ttl in map(tokenlist_gettext, args[3::2]):\n",
    "                yield HATNOTE_FOR, dest_ttl\n",
    "    elif tmpl == 'About-distinguish':\n",
    "        if len(args) > 1:\n",
    "            yield HATNOTE_DISTINGUISH, tokenlist_gettext(args[1])\n",
    "    elif tmpl == 'About-distinguish-text':\n",
    "        # the sub-group of the Semitic languages native to Mesopotamia and the Levant|[[Amharic]], the Semitic language spoken in [[Ethiopia]]\n",
    "        for tl in args[1:]:\n",
    "            for link_inner in tokenlist_links(tl):\n",
    "                dest = link_inner.split('|', maxsplit=1)[0]\n",
    "                yield HATNOTE_DISTINGUISH, dest\n",
    "    elif tmpl == 'For-text':\n",
    "        for tl in args[1:]:\n",
    "            for link_inner in tokenlist_links(tl):\n",
    "                dest = link_inner.split('|', maxsplit=1)[0]\n",
    "                yield HATNOTE_DISTINGUISH, dest\n",
    "    elif tmpl in ('Other uses', 'Other people', 'About other people', 'Hatnote', 'Technical reasons'):\n",
    "        return\n",
    "    elif tmpl in ('Redirect-several',):\n",
    "        return  # too complicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hatedges():\n",
    "    iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "    iterator = map(\n",
    "        lambda b: zip(\n",
    "            b[\"id\"].to_numpy(),\n",
    "            b[\"ns\"].to_numpy(),\n",
    "            map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "            b[\"text\"].to_pylist()\n",
    "        ),\n",
    "        iterator\n",
    "    )\n",
    "    iterator = itertools.chain.from_iterable(iterator)\n",
    "    iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "    iterator = enumerate(map(operator.itemgetter(3), iterator))\n",
    "    filenames = files.hatedge_filenames(NUM_PARTITIONS)\n",
    "    hatcheck_arr = np.zeros(N, dtype=np.bool_)\n",
    "    for part_idx, subitr in enumerate(lazy_chunk(iterator, PARTITION_SIZE)):\n",
    "        edges = PairVec('int32')\n",
    "        for src_id, text in subitr:\n",
    "            for line in wikiplain.get_distinguish_hatnotes(text):\n",
    "                for tag, dest_ttl in parse_hatnote(line):\n",
    "                    dest_ttl = dest_ttl.split('{{!', maxsplit=1)[0]  # for {{!}}\n",
    "                    if dest_ttl == \"\" or '#' in dest_ttl:\n",
    "                        continue\n",
    "                    dest_ttl = dest_ttl[0].upper() + dest_ttl[1:]\n",
    "                    if (dest_id := id_map.get(dest_ttl) or id_map2.get(dest_ttl)) is not None:\n",
    "                        edges.append(src_id, (tag << 28) | dest_id)\n",
    "                        hatcheck_arr[src_id] = True\n",
    "        with open(filenames[part_idx], \"wb\") as fp:\n",
    "            np.save(fp, edges.array[:edges.length])\n",
    "    return hatcheck_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatedge_fnames = glob.glob(files.hatedge_filename_pattern)\n",
    "try:\n",
    "    assert set(hatedge_fnames) == set(files.hatedge_filenames(NUM_PARTITIONS))\n",
    "    with open(files.hatcheck_arr_filename, \"rb\") as fp:\n",
    "        hatcheck_arr = np.load(fp)\n",
    "except Exception as exc:\n",
    "    hatcheck_arr = get_hatedges()\n",
    "    with open(files.hatcheck_arr_filename, \"wb\") as fp:\n",
    "        np.save(fp, hatcheck_arr)\n",
    "    hatedge_fnames = glob.glob(files.hatedge_filename_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wiki_link(line):\n",
    "    dest_ttl = line.strip()\n",
    "    if len(dest_ttl) == 0:\n",
    "        return None\n",
    "    dest_ttl = dest_ttl[0].upper() + dest_ttl[1:]\n",
    "    dest_ttl = dest_ttl.split('|', maxsplit=1)[0]\n",
    "    dest_ttl = dest_ttl.split('#', maxsplit=1)[0]\n",
    "    return dest_ttl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PARTITION_SIZE = 16\n",
    "PARTITION_SIZE = 1 << LOG_PARTITION_SIZE\n",
    "NUM_PARTITIONS = math.ceil(N / PARTITION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_id_map = ChainMap(id_map, id_map2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6806227"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge format\n",
    "\n",
    "- `edges_{n}.npz` stores the outgoing links from `PARITION_SIZE*n ..< PARTITION_SIZE*(n+1)`\n",
    "- These are stored in a list where element `i` contains the links out to `PARITION_SIZE*i ..< PARTITION_SIZE*(i+1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(iterable, size):\n",
    "    \"\"\"Split an iterable into list chunks of size `n`.\n",
    "    \n",
    "    The last chunk can be fewer than `n` elements long, but it won't be empty.\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(iterator, size))\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "        else:\n",
    "            return\n",
    "\n",
    "def lazy_chunk(iterable, n):\n",
    "    \"\"\"Split an iterable into iterable chunks of size `n`.\n",
    "    \n",
    "    The last chunk can be fewer than `n` elements long, but it won't be empty.\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        try:\n",
    "            first = next(iterator)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        yield itertools.chain([first], itertools.islice(iterator, n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ORDER_TAG_BITS = 3\n",
    "NUM_ORDER_TAGS = 1 << ORDER_TAG_BITS\n",
    "DEST_ID_BITS = 31 - ORDER_TAG_BITS\n",
    "DEST_ID_MASK = (1 << DEST_ID_BITS) - 1\n",
    "def get_edges():\n",
    "    in_degree = np.zeros(N, dtype=np.int32)\n",
    "    out_degree = np.zeros(N, dtype=np.int32)\n",
    "    with tqdm(position=1) as progress:\n",
    "        iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "        iterator = map(\n",
    "            lambda b: zip(\n",
    "                b[\"id\"].to_numpy(),\n",
    "                b[\"ns\"].to_numpy(),\n",
    "                map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "                b[\"text\"].to_pylist()\n",
    "            ),\n",
    "            iterator\n",
    "        )\n",
    "        iterator = itertools.chain.from_iterable(iterator)\n",
    "        iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "        iterator = enumerate(map(operator.itemgetter(3), iterator))\n",
    "        filenames = files.edge_filenames(NUM_PARTITIONS)\n",
    "        order_tags = np.arange(NUM_ORDER_TAGS, dtype=np.int32)[::-1] << DEST_ID_BITS\n",
    "        for part_idx, subitr in enumerate(lazy_chunk(iterator, PARTITION_SIZE)):\n",
    "            edges = [PairVec('int32') for _ in range(0, N, PARTITION_SIZE)]\n",
    "            for src_id, text in subitr:\n",
    "                link_idx = 0\n",
    "                for link in wikiplain.get_links(text):\n",
    "                    dest_ttl = parse_wiki_link(link)\n",
    "                    if dest_ttl:\n",
    "                        dest_id = id_map.get(dest_ttl)\n",
    "                        dest_id = dest_id if (dest_id is not None) else id_map2.get(dest_ttl)\n",
    "                        if dest_id is not None:\n",
    "                            partition = dest_id >> LOG_PARTITION_SIZE\n",
    "                            edges[partition].append(src_id, order_tags[link_idx] | dest_id)\n",
    "                            in_degree[dest_id] += 1\n",
    "                            out_degree[src_id] += 1\n",
    "                            progress.update()\n",
    "                            link_idx = min(NUM_ORDER_TAGS - 1, link_idx + 1)\n",
    "            with open(filenames[part_idx], \"wb\") as fp:\n",
    "                np.savez(fp, *([vec.array[:vec.length] for vec in edges]))\n",
    "    return in_degree, out_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_fnames = glob.glob(files.edge_filename_pattern)\n",
    "try:\n",
    "    assert set(edge_fnames) == set(files.edge_filenames(NUM_PARTITIONS))\n",
    "    with open(files.in_degree_filename, \"rb\") as fp:\n",
    "        in_degree = np.load(fp)\n",
    "    with open(files.out_degree_filename, \"rb\") as fp:\n",
    "        out_degree = np.load(fp)\n",
    "    # for fname in edge_fnames:\n",
    "    #    with open(fname, \"rb\") as fp:\n",
    "    #         assert len(pickle.load(fp)) == NUM_PARTITIONS\n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "    in_degree, out_degree = get_edges()\n",
    "    edge_fnames = glob.glob(files.edge_filename_pattern)\n",
    "    with open(files.in_degree_filename, \"wb\") as fp:\n",
    "        np.save(fp, in_degree)\n",
    "    with open(files.out_degree_filename, \"wb\") as fp:\n",
    "        np.save(fp, out_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_edges time to complete\n",
    "\n",
    "| date | links/s | time |\n",
    "| :--- | :------ | :--- |\n",
    "| 2024-02-06 | 101000 | 46min 48s\n",
    "| 2024-05-14 | 62500 | 76min 39s\n",
    "\n",
    "2024-05-14: `ORDER_BITS` and `link_idx` logic was added, accounting for most of the slowdown \\\n",
    "numbers are for `strider`. `kicker` was stuck around 17500 links/s and I couldn't figure out why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_dab_array():\n",
    "#     result = np.zeros(N, dtype=np.bool8)\n",
    "#     dab_proc = subprocess.Popen(\n",
    "#         [\"wikiplain\", \"--fraction\", \"1\", \"-c\", \"only-dab\", \"--ns\", \"0\", files.enwiki_database_filename],\n",
    "#         stdout=subprocess.PIPE,\n",
    "#         stderr=subprocess.PIPE\n",
    "#     )\n",
    "#     iterator = make_links_iter(dab_proc.stdout)\n",
    "#     iterator = tqdm(iterator, position=0, total=len(id_map))\n",
    "#     iterator = map(lambda pair: (pair[0].decode(\"utf-8\"), pair[1]), iterator)\n",
    "#     for n, subitr in enumerate(lazy_chunk(iterator, PARTITION_SIZE)):\n",
    "#         for ttl, text in subitr:\n",
    "#             src_id = id_map[ttl]\n",
    "#             if len(text) > 0:\n",
    "#                 result[src_id] = True\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDF = pl.scan_parquet(files.enwiki_parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     with open(files.dab_array_filename, \"rb\") as fp:\n",
    "#         dab_array = pickle.load(fp)\n",
    "# except Exception as exc:\n",
    "#     print(exc)\n",
    "#     dab_array = get_dab_array()\n",
    "#     with open(files.dab_array_filename, \"wb\") as fp:\n",
    "#         pickle.dump(dab_array, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_adjacency_matrix_slice(partition, progress):\n",
    "    \"\"\"Computes the slice of the adjacency matrix A starting at row p*S and ending before row (p+1)*S\n",
    "    \n",
    "    p=partition, S=PARTITION_SIZE, and A is defined so that\n",
    "    A @ np.eye(N)[i] = v, a probability vector where\n",
    "        v[j] = out-degree(i) > 0 | count((i,j) in E) / out-degree(i)\n",
    "               otherwise         | 0\n",
    "    \"\"\"\n",
    "    origin_row = partition * PARTITION_SIZE\n",
    "    n_rows = min(PARTITION_SIZE, N - origin_row)\n",
    "    index_arrs = []\n",
    "    value_arrs = []\n",
    "    pkey = f'arr_{partition}'\n",
    "    for fname in glob.glob(files.edge_filename_pattern):\n",
    "        with np.load(fname) as npz:\n",
    "            vec = npz[pkey]\n",
    "        vec[:, 1] &= DEST_ID_MASK  # remove order_tag\n",
    "        # vec is\n",
    "        #  [[src_id_0, dest_id_0],\n",
    "        #   [src_id_1, dest_id_1],\n",
    "        #   ...\n",
    "        #  ]\n",
    "        # Sort by (src,dest), make unique and get counts\n",
    "        key_arr = (vec[:, 0].astype('int64') << 32) | vec[:, 1]\n",
    "        _, order, count = np.unique(key_arr, return_index=True, return_counts=True)\n",
    "        vec = vec[order]\n",
    "        # Normalize `count` based on (src,)\n",
    "        count = count.astype('float64') / out_degree[vec[:, 0]]\n",
    "        index_arrs.append(vec)\n",
    "        value_arrs.append(count)\n",
    "        progress.update()\n",
    "    index_arr = np.vstack(index_arrs)\n",
    "    matrix_slice = scipy.sparse.csr_array(\n",
    "        (np.hstack(value_arrs), (index_arr[:, 1] - origin_row, index_arr[:, 0])),\n",
    "        shape=(n_rows, N),\n",
    "        dtype=np.float64\n",
    "    )\n",
    "    return matrix_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a7b8e667964490877915a942cda4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tqdm(total=NUM_PARTITIONS**2) as progress:\n",
    "    for partition in range(NUM_PARTITIONS):\n",
    "        adj_matrix = compute_adjacency_matrix_slice(partition, progress)\n",
    "        scipy.sparse.save_npz(files.adjacency_filename(partition), adj_matrix, compressed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     5,    19,    85,   409,  1193, 14381])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(out_degree, [0, 0.1, 0.5, 0.9, 0.99, 0.999, 1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_out_degree = np.log(out_degree + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_out_degree /= log_out_degree.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WMf5L5VEqZb"
   },
   "source": [
    "### Global PageRank\n",
    "\n",
    "The initial rank is a column vector $\\mathbf{r}$ = $\\frac{1}{N} \\left( \\mathbf{\\vec{1}} \\right)$\n",
    "\n",
    "The transition matrix $\\mathbf{M}$ is N x N; each column represents a source, and each row represents a destination.\n",
    "$\\mathbf{M}_{ij} = P(\\text{next}=i\\,|\\,\\text{current}=j)$. Each column **must** sum to 1 for the calculation to be stable, so if page $j$ contains no links, it is treated as if it had a link to every page.\n",
    "\n",
    "The power method iteratively computes better ranks: $\\mathbf{r'} = (1 - \\alpha) \\mathbf{M}\\mathbf{r} + \\frac{\\alpha}{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalized PageRank\n",
    "\n",
    "Personalized PageRank uses a preference vector $\\mathbf{p}$ in place of the uniform $\\frac{1}{N}$ for _teleportation_. Pages with no out-links still use a uniform distribution. The initial rank can be any vector, because of the converging property of the power method (explanation at https://mathworld.wolfram.com/Eigenvector.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WMf5L5VEqZb"
   },
   "source": [
    "### Ending iteration\n",
    "\n",
    "At each iteration, we calculate the [perplexity](https://en.wikipedia.org/wiki/Perplexity) of the PageRank distribution, where perplexity is defined as 2 raised to the [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) of the PageRank distribution, i.e., $2^{H(PR)}$. The initial guess is at maximum entropy, so the first iteration causes perplexity to decrease. Later iterations may change perplexity in either direction; we stop when the change is below a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "wsL0yQKvKqAC"
   },
   "outputs": [],
   "source": [
    "def perplexity(distribution):\n",
    "    return np.power(2, np.sum(-distribution * np.log2(distribution)))\n",
    "\n",
    "def personalized_page_rank(preference, threshold=1, random_jump_prob=0.15):\n",
    "    current_rank = np.ones(N, dtype=np.float64) / N\n",
    "    next_rank = np.zeros(N, dtype=np.float64)\n",
    "    # iteratively update current_rank\n",
    "    edge_follow_prob = 1 - random_jump_prob\n",
    "    prev_perplexity = float('inf')\n",
    "    current_perplexity = perplexity(current_rank)\n",
    "    current_iter = 0\n",
    "    iter_start = time.time()\n",
    "    print(\"Itr# | ΔPerplexity     | Seconds\")\n",
    "    while abs(prev_perplexity - current_perplexity) > threshold:\n",
    "        current_iter += 1\n",
    "        next_rank[:] = random_jump_prob * preference\n",
    "        # update destinations from non-sink nodes (N x N times N x 1 -> N x 1)\n",
    "        spread_probs = np.vstack([\n",
    "            adjacency_matrix_slice.dot(current_rank[:, np.newaxis])\n",
    "            for adjacency_matrix_slice in map(scipy.sparse.load_npz, files.adjacency_filenames(NUM_PARTITIONS))\n",
    "        ])\n",
    "        next_rank += edge_follow_prob * spread_probs[:, 0]  # make column vector 1-D\n",
    "        # update destinations from sink nodes\n",
    "        next_rank[:] += edge_follow_prob * current_rank[out_degree == 0].sum() / N\n",
    "        # copy `next_rank` values into `current_rank``\n",
    "        current_rank[:] = next_rank\n",
    "        # --\n",
    "        # compute perplexity and progress\n",
    "        prev_perplexity = current_perplexity\n",
    "        current_perplexity = perplexity(current_rank)\n",
    "        next_iter_start = time.time()\n",
    "        print(\"{:<3d}    {:<15.6f}   {:.3f}\".format(current_iter,\n",
    "                                                    current_perplexity - prev_perplexity,\n",
    "                                                    next_iter_start - iter_start))\n",
    "        iter_start = next_iter_start\n",
    "\n",
    "    title_df = pl.DataFrame({\n",
    "        \"title\": id_map.keys(),\n",
    "        \"node_id\": id_map.values(),\n",
    "    })\n",
    "    df = pl.DataFrame({\n",
    "        \"value\": next_rank,\n",
    "        \"in_deg\": in_degree,\n",
    "        \"out_deg\": out_degree,\n",
    "    })\n",
    "    df = (\n",
    "        df.with_row_count(name=\"node_id\")\n",
    "        .with_columns(pl.col(\"node_id\").cast(pl.Int64))\n",
    "        .join(title_df, on=\"node_id\", how=\"left\")\n",
    "    )\n",
    "    df = df.select(\"title\", \"value\", \"in_deg\", \"out_deg\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "wsL0yQKvKqAC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr# | ΔPerplexity     | Seconds\n",
      "1      -6046820.239511   4.111\n",
      "2      145020.070720     4.205\n",
      "3      -38459.999095     4.222\n",
      "4      -2475.166518      3.944\n",
      "5      -8409.933054      3.903\n",
      "6      -2782.290156      3.913\n",
      "7      -3207.065144      3.833\n",
      "8      -1677.649700      3.768\n",
      "9      -1454.159588      3.814\n",
      "10     -913.633434       3.831\n",
      "11     -714.444587       3.850\n",
      "12     -481.289193       3.849\n",
      "13     -366.054468       3.870\n",
      "14     -253.044415       3.803\n",
      "15     -191.168698       3.872\n",
      "16     -134.530969       3.976\n",
      "17     -101.180405       4.069\n",
      "18     -72.127842        3.999\n",
      "19     -54.318969        4.022\n",
      "20     -38.967238        3.929\n",
      "21     -29.473556        4.019\n",
      "22     -21.245904        4.089\n",
      "23     -16.133095        4.000\n",
      "24     -11.676279        4.016\n",
      "25     -8.907162         4.022\n",
      "26     -6.463019         4.020\n",
      "27     -4.954798         4.007\n",
      "28     -3.602528         4.040\n",
      "29     -2.774976         4.068\n",
      "30     -2.020830         4.056\n",
      "31     -1.564180         4.065\n",
      "32     -1.140244         4.027\n",
      "33     -0.886867         4.076\n"
     ]
    }
   ],
   "source": [
    "# Run until perplexity changes by less than 1\n",
    "PR = personalized_page_rank(log_out_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR.write_parquet(files.pagerank_parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_sorted = PR.sort('value', descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffdf21159f1424e8f0c686fd1d940cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='page', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function nbhelpers.polars.pager.<locals>.<lambda>(page)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pager(PR_sorted.slice(0, 2000), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c0eb1b8d32441aa660784eae4e1eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='q'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function nbhelpers.polars.searcher.<locals>.searcher_run(q)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher(\n",
    "    PR_sorted.slice(0, 200000).with_columns(pl.Series(\"rank\", range(200000))).select([\"rank\", *PR_sorted.columns]),\n",
    "    ['title'],\n",
    "    20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNw97vMeu3UGKrk6j3TYQO8",
   "include_colab_link": true,
   "name": "PageRank.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
