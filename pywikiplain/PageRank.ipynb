{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "## PageRank on Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "1. Download a dump of Wikipedia's articles, named `enwiki-{date_string}-pages-articles-multistream.xml.bz2`\n",
    "2. Download the `enwiki-{date_string}-pages-articles-multistream-index.txt.bz2` file\n",
    "3. Move those files into the same folder, removing the `enwiki-{date_string}` prefix\n",
    "4. Process the `xml.bz2` file into a Parquet file using `wikiplain.load_bz2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import glob\n",
    "import gzip\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import socket\n",
    "import struct\n",
    "import subprocess\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "from collections import ChainMap, defaultdict, deque\n",
    "from contextlib import asynccontextmanager\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from enum import Enum, auto\n",
    "from functools import lru_cache, partial\n",
    "from urllib.parse import urlencode, urlsplit, quote as urlquote, unquote as urlunquote\n",
    "from typing import Any, Awaitable, Callable, Literal, TypeVar\n",
    "\n",
    "import cbor2\n",
    "import cytoolz\n",
    "import numpy as np\n",
    "import pypocketmap as pkm\n",
    "import pyarrow.parquet as pq\n",
    "import polars as pl\n",
    "import sqlalchemy as sa\n",
    "import scipy.sparse\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from ipywidgets import interact\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import select, text as sqltext\n",
    "from tqdm.auto import tqdm\n",
    "from arsenal.datastructures.unionfind import UnionFind\n",
    "from arsenal.datastructures.heap import MinMaxHeap\n",
    "\n",
    "import wikiplain\n",
    "from wikiplain import Node, NodeKind, Token, TokenKind as TK\n",
    "from nbhelpers.polars import pager, searcher\n",
    "from special_cases import SECOND_LEVEL_DOMAINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.Config.set_fmt_str_lengths(160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageRankFiles:\n",
    "    def __init__(self, date_string):\n",
    "        self.date_string = date_string\n",
    "        self.enwiki_dir = f\"{os.environ['ENWIKI_DIR']}/{date_string}\"\n",
    "        self.parquet_dir = os.environ.get('ENWIKI_PARQUET_DIR', self.enwiki_dir)\n",
    "        try:\n",
    "            os.mkdir(f\"{self.enwiki_dir}/pagerank\")\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "    \n",
    "    @property\n",
    "    def enwiki_parquet_filename(self):\n",
    "        return f\"{self.parquet_dir}/enwiki_{self.date_string}.parquet\"\n",
    "    \n",
    "    @property\n",
    "    def pagerank_parquet_filename(self):\n",
    "        return f\"{self.parquet_dir}/enwiki_{self.date_string}_pagerank.parquet\"\n",
    "\n",
    "    @property\n",
    "    def nub_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/nub.bin\"\n",
    "    \n",
    "    @property\n",
    "    def id_map_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/id_map.bin\"\n",
    "    \n",
    "    @property\n",
    "    def id_map2_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/id_map2.bin\"\n",
    "    \n",
    "    @property\n",
    "    def dense_id_arr_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/dense_id_arr.npy\"\n",
    "    \n",
    "    @property\n",
    "    def edge_filename_pattern(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/edges_*.npz\"\n",
    "    \n",
    "    def edge_filenames(self, num_partitions):\n",
    "        return [\n",
    "            f\"{self.enwiki_dir}/pagerank/edges_{i}.npz\"\n",
    "            for i in range(num_partitions)\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def in_degree_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/in_degree.npy\"\n",
    "    \n",
    "    @property\n",
    "    def out_degree_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/out_degree.npy\"\n",
    "    \n",
    "    def adjacency_filename(self, partition):\n",
    "        return f\"{self.enwiki_dir}/pagerank/adjacency_{partition}.npz\"\n",
    "    \n",
    "    def adjacency_filenames(self, num_partitions):\n",
    "        return [self.adjacency_filename(i) for i in range(num_partitions)]\n",
    "\n",
    "    @property\n",
    "    def disambig_arr_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/disambig_arr.npy\"\n",
    "    \n",
    "    @property\n",
    "    def moved_article_set_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/moved_article_set.bin\"\n",
    "    \n",
    "    @property\n",
    "    def top_cite_domains_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/top_cite_domains.bin\"\n",
    "\n",
    "    @property\n",
    "    def infobox_title_arr_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/infobox_titles.bin\"\n",
    "\n",
    "    @property\n",
    "    def short_description_arr_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/short_descriptions.bin\"\n",
    "\n",
    "    @property\n",
    "    def h2_heading_arr_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/h2_headings.bin\"\n",
    "\n",
    "    @property\n",
    "    def hatedge_filename_pattern(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/hatedges_*.npy\"\n",
    "    \n",
    "    def hatedge_filenames(self, num_partitions):\n",
    "        return [\n",
    "            f\"{self.enwiki_dir}/pagerank/hatedges_{i}.npy\"\n",
    "            for i in range(num_partitions)\n",
    "        ]\n",
    "    \n",
    "    @property\n",
    "    def hatcheck_arr_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/hatcheck_arr.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = PageRankFiles(\"20240620\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Find title collisions\n",
    "\n",
    "1. There are some pages with the same title - I think this is caused by pages deleted and recreated while the snapshot is in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pqf = pq.ParquetFile(files.enwiki_parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overwritten():\n",
    "    overwritten = set()\n",
    "    timestamp_map = {}\n",
    "    article_ids = {}\n",
    "    pqf_size = 0\n",
    "    for batch in tqdm(pqf.iter_batches(batch_size=100), total=pqf.num_row_groups):\n",
    "        for aid, ns, ttl, tm in zip(batch[\"id\"].to_numpy(), batch[\"ns\"].to_numpy(), batch[\"title\"].to_pylist(), batch[\"timestamp\"].to_pylist()):\n",
    "            pqf_size += 1\n",
    "            if ns != 0:\n",
    "                continue\n",
    "            tm = np.datetime64(tm)\n",
    "            other_id = article_ids.setdefault(ttl, aid)\n",
    "            if other_id != aid:\n",
    "                if (timestamp_map[ttl], other_id) < (tm, aid):\n",
    "                    print(f\"{ttl!r}: {aid} > {other_id}\")\n",
    "                    overwritten.add(other_id)\n",
    "                    article_ids[ttl] = aid\n",
    "                    timestamp_map[ttl] = tm\n",
    "                else:\n",
    "                    print(f\"{ttl!r}: {other_id} > {aid}\")\n",
    "                    overwritten.add(aid)\n",
    "            else:\n",
    "                timestamp_map[ttl] = tm\n",
    "    return overwritten, pqf_size\n",
    "\n",
    "try:\n",
    "    with open(files.nub_filename, \"rb\") as fp:\n",
    "        overwritten, pqf_size = cbor2.load(fp)\n",
    "except Exception:\n",
    "    overwritten, pqf_size = get_overwritten()\n",
    "    overwritten = {int(e) for e in overwritten}\n",
    "    with open(files.nub_filename, \"wb\") as fp:\n",
    "        cbor2.dump((overwritten, pqf_size), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Build representation of articles/links as a graph\n",
    "\n",
    "1. Create `id_map` from non-redirecting article titles to node number, and `id_map2` from redirecting article titles to node number\n",
    "2. Use `wikiplain` to extract link titles, and use above maps to convert to (src_id, dest_id) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vec:\n",
    "    def __init__(self, dtype):\n",
    "        self.array = np.ndarray((1024,), dtype=dtype)\n",
    "        self.length = 0\n",
    "    \n",
    "    @property\n",
    "    def capacity(self):\n",
    "        return self.array.shape[0]\n",
    "\n",
    "    def append(self, v):\n",
    "        idx = self.length\n",
    "        if idx >= self.capacity:\n",
    "            addsz = max(2, self.capacity)\n",
    "            self.array = np.hstack((self.array, np.zeros((addsz,), dtype=self.array.dtype)))\n",
    "        self.array[idx] = v\n",
    "        self.length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairVec:\n",
    "    def __init__(self, dtype):\n",
    "        self.array = np.ndarray((1024, 2), dtype=dtype)\n",
    "        self.length = 0\n",
    "    \n",
    "    @property\n",
    "    def capacity(self):\n",
    "        return self.array.shape[0]\n",
    "\n",
    "    def append(self, v1, v2):\n",
    "        idx = self.length\n",
    "        if idx >= self.capacity:\n",
    "            addsz = max(2, self.capacity)\n",
    "            self.array = np.vstack((self.array, np.zeros((addsz, 2), dtype=self.array.dtype)))\n",
    "        self.array[idx] = [v1, v2]\n",
    "        self.length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(iterable, size):\n",
    "    \"\"\"Split an iterable into list chunks of size `n`.\n",
    "    \n",
    "    The last chunk can be fewer than `n` elements long, but it won't be empty.\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(iterator, size))\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "        else:\n",
    "            return\n",
    "\n",
    "def lazy_chunk(iterable, n):\n",
    "    \"\"\"Split an iterable into iterable chunks of size `n`.\n",
    "    \n",
    "    The last chunk can be fewer than `n` elements long, but it won't be empty.\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        try:\n",
    "            first = next(iterator)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        yield itertools.chain([first], itertools.islice(iterator, n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterdecode(f):\n",
    "    decoder = cbor2.CBORDecoder(f)\n",
    "    while True:\n",
    "        try:\n",
    "            yield decoder.decode()\n",
    "        except EOFError:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_maps():\n",
    "    redirect_group_map = UnionFind()\n",
    "    id_map = pkm.create(str, int)\n",
    "    redirect_lst = []\n",
    "    dense_ids = Vec(dtype=np.int64)\n",
    "    for batch in tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100)):\n",
    "        for aid, ns, ttl, redir in zip(batch[\"id\"].to_numpy(), batch[\"ns\"].to_numpy(), batch[\"title\"].to_pylist(), batch[\"redirect\"].to_pylist()):\n",
    "            if ns != 0 or aid in overwritten:\n",
    "                continue\n",
    "            if redir is not None:\n",
    "                redirect_group_map.union(ttl, redir)\n",
    "                redirect_lst.append(ttl)\n",
    "            else:\n",
    "                assert ttl not in id_map, f\"Expected unique titles, got second instance of {ttl}\"\n",
    "                dense_ids.append(aid)\n",
    "                id_map[ttl] = len(id_map)\n",
    "    id_map2 = pkm.create(str, int)\n",
    "    for group in redirect_group_map.classes():\n",
    "        centers = [ttl for ttl in group if ttl in id_map]\n",
    "        if len(centers) == 0:\n",
    "            continue\n",
    "        assert len(centers) == 1, str(centers)\n",
    "        for ttl in group:\n",
    "            if ttl != centers[0]:\n",
    "                id_map2[ttl] = id_map[centers[0]]\n",
    "    return id_map, id_map2, dense_ids.array[:dense_ids.length]\n",
    "\n",
    "try:\n",
    "    with open(files.id_map_filename, \"rb\") as fp:\n",
    "        id_map = pkm.create(str, int)\n",
    "        for k, v in iterdecode(fp):\n",
    "            id_map[k] = v\n",
    "    with open(files.id_map2_filename, \"rb\") as fp:\n",
    "        id_map2 = pkm.create(str, int)\n",
    "        for k, v in iterdecode(fp):\n",
    "            id_map2[k] = v\n",
    "    with open(files.dense_id_arr_filename, \"rb\") as fp:\n",
    "        dense_id_arr = np.load(fp)\n",
    "except Exception:\n",
    "    id_map, id_map2, dense_id_arr = get_id_maps()\n",
    "    with open(files.id_map_filename, \"wb\") as fp:\n",
    "        for k, v in id_map.items():\n",
    "            cbor2.dump((k, v), fp)\n",
    "    with open(files.id_map2_filename, \"wb\") as fp:\n",
    "        for k, v in id_map2.items():\n",
    "            cbor2.dump((k, v), fp)\n",
    "    with open(files.dense_id_arr_filename, \"wb\") as fp:\n",
    "        np.save(fp, dense_id_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6839104, 10987529)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_map), len(id_map2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Martín Vázquez',\n",
       " 'Rouverol',\n",
       " 'Ron Hansell',\n",
       " 'Jim Wolf (musician)',\n",
       " 'McAllen Miller International Airport',\n",
       " '1937 Albanian National Championship',\n",
       " 'Nerka Lake',\n",
       " 'Manilius (crater)',\n",
       " 'Metallacarboxylic acid',\n",
       " 'Nicolas Appert']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.islice(iter(id_map.keys()), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Office for fair access',\n",
       " 'The Catalyst (newspaper)',\n",
       " 'Scarecrow Press historical dictionary series',\n",
       " 'Boissieri',\n",
       " 'Edward Northey (disambiguation)',\n",
       " 'Herzogschloss Zweibrücken',\n",
       " 'Trinity Square (disambiguation)',\n",
       " 'Urban cowboy',\n",
       " 'K 424a',\n",
       " 'Nawaz Shareef']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.islice(iter(id_map2.keys()), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_template_id_maps():\n",
    "#     redirect_group_map = UnionFind()\n",
    "#     template_id_map = {}\n",
    "#     for batch in tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100)):\n",
    "#         for aid, ns, ttl, redir in zip(batch[\"id\"].to_numpy(), batch[\"ns\"].to_numpy(), batch[\"title\"].to_pylist(), batch[\"redirect\"].to_pylist()):\n",
    "#             if ns != 10 or aid in overwritten:\n",
    "#                 continue\n",
    "#             if redir is not None:\n",
    "#                 redirect_group_map.union(ttl, redir)\n",
    "#             else:\n",
    "#                 assert ttl not in template_id_map, f\"Expected unique titles, got second instance of {ttl}\"\n",
    "#                 template_id_map[ttl] = aid\n",
    "#     template_id_map2 = {}\n",
    "#     for group in redirect_group_map.classes():\n",
    "#         centers = [ttl for ttl in group if ttl in template_id_map]\n",
    "#         if len(centers) == 0:\n",
    "#             continue\n",
    "#         assert len(centers) == 1, str(centers)\n",
    "#         for ttl in group:\n",
    "#             if ttl != centers[0]:\n",
    "#                 template_id_map2[ttl] = template_id_map[centers[0]]\n",
    "#     return template_id_map, template_id_map2\n",
    "\n",
    "# try:\n",
    "#     with open(files.template_id_maps_filename, \"rb\") as fp:\n",
    "#         template_id_map, template_id_map2 = pickle.load(fp)\n",
    "# except FileNotFoundError:\n",
    "#     template_id_map, template_id_map2 = get_template_id_maps()\n",
    "#     with open(files.template_id_maps_filename, \"wb\") as fp:\n",
    "#         pickle.dump((template_id_map, template_id_map2), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia article stats\n",
    "\n",
    "1. Create `disambig_arr`, a simple boolean array recording whether each article is a disambiguation or set-index page.\n",
    "2. Create `moved_article_set`, a set of article titles which redirect because their article content was moved.\n",
    "3. Create `top_cite_domains`, the 1024 most commonly cited websites across all articles.\n",
    "4. Create `infobox_title_arr`, the title of the first infobox on each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disambig_arr():\n",
    "    iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "    iterator = map(\n",
    "        lambda b: zip(\n",
    "            b[\"id\"].to_numpy(),\n",
    "            b[\"ns\"].to_numpy(),\n",
    "            map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "            b[\"text\"].to_pylist()\n",
    "        ),\n",
    "        iterator\n",
    "    )\n",
    "    iterator = itertools.chain.from_iterable(iterator)\n",
    "    iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "    iterator = enumerate(map(operator.itemgetter(3), iterator))\n",
    "    disambig_arr = np.zeros(N, dtype=np.bool_)\n",
    "    for node_id, text in iterator:\n",
    "        disambig_arr[node_id] = wikiplain.is_disambiguation_page(text)\n",
    "    return disambig_arr\n",
    "\n",
    "try:\n",
    "    with open(files.disambig_arr_filename, \"rb\") as fp:\n",
    "        disambig_arr = np.load(fp)\n",
    "except Exception:\n",
    "    disambig_arr = get_disambig_arr()\n",
    "    with open(files.disambig_arr_filename, \"wb\") as fp:\n",
    "        np.save(fp, disambig_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moved_article_set():\n",
    "    iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "    iterator = map(\n",
    "        lambda b: zip(\n",
    "            b[\"id\"].to_numpy(),\n",
    "            b[\"ns\"].to_numpy(),\n",
    "            map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "            b[\"title\"].to_pylist(),\n",
    "            map(b[\"text\"].__getitem__, itertools.count()),\n",
    "        ),\n",
    "        iterator\n",
    "    )\n",
    "    iterator = itertools.chain.from_iterable(iterator)\n",
    "    iterator = filter(lambda e: e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "    iterator = map(lambda e: (e[3], e[4]), iterator)\n",
    "    moved_article_set = set()\n",
    "    for title, text in iterator:\n",
    "        if '{{R from move' in text.as_py():\n",
    "            moved_article_set.add(title)\n",
    "    return moved_article_set\n",
    "\n",
    "try:\n",
    "    with open(files.moved_article_set_filename, 'rb') as fp:\n",
    "        moved_article_set = cbor2.load(fp)\n",
    "except FileNotFoundError:\n",
    "    moved_article_set = get_moved_article_set()\n",
    "    with open(files.moved_article_set_filename, 'wb') as fp:\n",
    "        cbor2.dump(moved_article_set, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_cite_domains():\n",
    "    iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "    iterator = map(\n",
    "        lambda b: zip(\n",
    "            b[\"id\"].to_numpy(),\n",
    "            b[\"ns\"].to_numpy(),\n",
    "            map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "            b[\"text\"].to_pylist()\n",
    "        ),\n",
    "        iterator\n",
    "    )\n",
    "    iterator = itertools.chain.from_iterable(iterator)\n",
    "    iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "    iterator = enumerate(map(operator.itemgetter(3), iterator))\n",
    "    heap = MinMaxHeap()\n",
    "    heap_phases = 9\n",
    "    heap_limits = [\n",
    "        (N * i // heap_phases, 1024 << (heap_phases-i-1))\n",
    "        for i in range(heap_phases)\n",
    "    ]\n",
    "    heap_phase = 0\n",
    "    heap_limit = heap_limits[heap_phase][1]\n",
    "    for node_id, text in iterator:\n",
    "        if heap_phase+1 < heap_phases:\n",
    "            transition_node_id, lim = heap_limits[heap_phase+1]\n",
    "            if node_id >= transition_node_id:\n",
    "                heap_phase += 1\n",
    "                heap_limit = lim\n",
    "                while len(heap) > lim:\n",
    "                    heap.popmin()\n",
    "        page = defaultdict(int)\n",
    "        for url in wikiplain.get_cite_urls(text):\n",
    "            full_domain = re.sub(r\"[:/].*\", \"\", url)\n",
    "            parts = full_domain.split('.')\n",
    "            if len(parts) >= 2:\n",
    "                site_domain = parts[-2] + '.' + parts[-1]\n",
    "                if site_domain in SECOND_LEVEL_DOMAINS:\n",
    "                    if len(parts) >= 3:\n",
    "                        site_domain = parts[-3] + '.' + site_domain\n",
    "                    else:\n",
    "                        continue\n",
    "                page[site_domain] += 1\n",
    "        for k, v in page.items():\n",
    "            if k in heap:\n",
    "                heap[k] = heap.max[k] + v\n",
    "            elif len(heap) < heap_limit:\n",
    "                heap[k] = v\n",
    "            elif v > heap.peekmin()[1]:\n",
    "                heap.popmin()\n",
    "                heap[k] = v\n",
    "    top_cite_domains = []\n",
    "    while len(heap) > 0:\n",
    "        top_cite_domains.append(heap.popmax())\n",
    "    return top_cite_domains\n",
    "\n",
    "try:\n",
    "    with open(files.top_cite_domains_filename, \"rb\") as fp:\n",
    "        top_cite_domains = cbor2.load(fp)\n",
    "except Exception:\n",
    "    top_cite_domains = get_top_cite_domains()\n",
    "    with open(files.top_cite_domains_filename, \"wb\") as fp:\n",
    "        cbor2.dump(top_cite_domains, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{False: 2556920, True: 4282184}\n"
     ]
    }
   ],
   "source": [
    "def get_infobox_title_arr():\n",
    "    iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "    iterator = map(\n",
    "        lambda b: zip(\n",
    "            b[\"id\"].to_numpy(),\n",
    "            b[\"ns\"].to_numpy(),\n",
    "            map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "            b[\"text\"].to_pylist(),\n",
    "        ),\n",
    "        iterator\n",
    "    )\n",
    "    iterator = itertools.chain.from_iterable(iterator)\n",
    "    iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "    iterator = map(operator.itemgetter(3), iterator)\n",
    "    for text in iterator:\n",
    "        yield wikiplain.get_first_infobox_title(text)\n",
    "\n",
    "try:\n",
    "    with open(files.infobox_title_arr_filename, 'rb') as fp:\n",
    "        summary = cytoolz.countby(bool, iterdecode(fp))\n",
    "        print(summary)\n",
    "        total = summary.get(False, 0) + summary.get(True, 0)\n",
    "        assert total == N\n",
    "except (FileNotFoundError, AssertionError):\n",
    "    with open(files.infobox_title_arr_filename, 'wb') as fp:\n",
    "        for e in get_infobox_title_arr():\n",
    "            cbor2.dump(e, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_token(tc, c):\n",
    "    return [Token(kind=TK.Content, data=piece) for piece in tc.split(c)]\n",
    "\n",
    "def split_template(line):\n",
    "    finished = []\n",
    "    tdepth = 0\n",
    "    ldepth = 0\n",
    "    tokens = wikiplain.tokenize(line)\n",
    "    acc = []\n",
    "    for tok in tokens:\n",
    "        match tok:\n",
    "            case Token(kind=TK.Content, data=tc):\n",
    "                if tdepth > 0 or ldepth > 0:\n",
    "                    acc.append(tok)\n",
    "                elif tc != '':\n",
    "                    parts = split_token(tc, '|')\n",
    "                    if parts[0].data != '':\n",
    "                        acc.append(parts[0])\n",
    "                    if len(parts) > 1:\n",
    "                        finished.append(acc)\n",
    "                        if parts[-1].data != '':\n",
    "                            acc = [parts.pop()]\n",
    "                        for p in parts[1:]:\n",
    "                            finished.append([p])\n",
    "            case Token(kind=TK.LinkStart):\n",
    "                ldepth += 1\n",
    "                acc.append(tok)\n",
    "            case Token(kind=TK.LinkEnd):\n",
    "                ldepth -= 1\n",
    "                acc.append(tok)\n",
    "            case Token(kind=TK.TemplateStart):\n",
    "                tdepth += 1\n",
    "                acc.append(tok)\n",
    "            case Token(kind=TK.TemplateEnd):\n",
    "                tdepth -= 1\n",
    "                acc.append(tok)\n",
    "    if acc:\n",
    "        finished.append(acc)\n",
    "    return finished\n",
    "\n",
    "def tokenlist_string(tl):\n",
    "    if len(tl) != 1 or tl[0].kind != TK.Content:\n",
    "        return None\n",
    "    return tl[0].data\n",
    "\n",
    "def token_fmt(tok):\n",
    "    match tok:\n",
    "        case Token(kind=TK.Content, data=tc):\n",
    "            return tc\n",
    "        case Token(kind=TK.LinkStart):\n",
    "            return '[['\n",
    "        case Token(kind=TK.LinkEnd):\n",
    "            return ']]'\n",
    "        case Token(kind=TK.TemplateStart):\n",
    "            return '{{'\n",
    "        case Token(kind=TK.TemplateEnd):\n",
    "            return '}}'\n",
    "\n",
    "def tokenlist_gettext(tl):\n",
    "    return \"\".join(map(token_fmt, tl))\n",
    "    \n",
    "def tokenlist_startswith(tl, s):\n",
    "    if len(tl) == 0 or tl[0].kind != TK.Content:\n",
    "        return False\n",
    "    return tl[0].data.startswith(s)\n",
    "\n",
    "def tokenlist_links(tl):\n",
    "    accs = []\n",
    "    for tok in tl:\n",
    "        match tok:\n",
    "            case Token(kind=TK.Content, data=tc):\n",
    "                if accs:\n",
    "                    accs[-1].append(tc)\n",
    "            case Token(kind=TK.LinkStart):\n",
    "                accs.append([])\n",
    "            case Token(kind=TK.LinkEnd):\n",
    "                if accs:\n",
    "                    yield \"\".join(accs.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{True: 3877101, False: 2962003}\n"
     ]
    }
   ],
   "source": [
    "def parse_short_desc(line):\n",
    "    lst = split_template(line)\n",
    "    if len(lst) < 2:\n",
    "        return None\n",
    "    desc_toks = lst[1]\n",
    "    desc = tokenlist_string(desc_toks)\n",
    "    if desc is None or (len(desc) == 4 and desc.lower() == \"none\"):\n",
    "        return None\n",
    "    # https://en.wikipedia.org/wiki/Template:Escape_template_list\n",
    "    desc = desc.replace(\"{{!}}\", \"|\")\n",
    "    return desc\n",
    "\n",
    "def get_short_description_arr():\n",
    "    iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "    iterator = map(\n",
    "        lambda b: zip(\n",
    "            b[\"id\"].to_numpy(),\n",
    "            b[\"ns\"].to_numpy(),\n",
    "            map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "            b[\"text\"].to_pylist(),\n",
    "        ),\n",
    "        iterator\n",
    "    )\n",
    "    iterator = itertools.chain.from_iterable(iterator)\n",
    "    iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "    iterator = map(operator.itemgetter(3), iterator)\n",
    "    for text in iterator:\n",
    "        yield next(map(parse_short_desc, wikiplain.get_templates_by_name(text, [\"Short description\"])), None)\n",
    "\n",
    "try:\n",
    "    with open(files.short_description_arr_filename, 'rb') as fp:\n",
    "        summary = cytoolz.countby(bool, iterdecode(fp))\n",
    "        print(summary)\n",
    "        total = summary.get(False, 0) + summary.get(True, 0)\n",
    "        assert total == N\n",
    "except (FileNotFoundError, AssertionError):\n",
    "    with open(files.short_description_arr_filename, 'wb') as fp:\n",
    "        for e in get_short_description_arr():\n",
    "            cbor2.dump(e, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{True: 6550703, False: 288401}\n"
     ]
    }
   ],
   "source": [
    "def get_h2_heading_arr():\n",
    "    iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "    iterator = map(\n",
    "        lambda b: zip(\n",
    "            b[\"id\"].to_numpy(),\n",
    "            b[\"ns\"].to_numpy(),\n",
    "            map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "            b[\"text\"].to_pylist(),\n",
    "        ),\n",
    "        iterator\n",
    "    )\n",
    "    iterator = itertools.chain.from_iterable(iterator)\n",
    "    iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "    iterator = map(operator.itemgetter(3), iterator)\n",
    "    for text in iterator:\n",
    "        result = []\n",
    "        tokens = wikiplain.tokenize(text)\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if tokens[i].kind == TK.ElementStart and tokens[i].data == \"h2\":\n",
    "                j = i + 1\n",
    "                while j < len(tokens) and tokens[j].kind != TK.ElementEnd:\n",
    "                    j += 1\n",
    "                if j < len(tokens) and tokens[j].data == \"h2\":\n",
    "                    if (htxt := tokenlist_string(tokens[i+1:j])):\n",
    "                        result.append(htxt)\n",
    "                i = j + 1\n",
    "            else:\n",
    "                i += 1\n",
    "        yield (result if result else None)\n",
    "\n",
    "try:\n",
    "    with open(files.h2_heading_arr_filename, 'rb') as fp:\n",
    "        summary = cytoolz.countby(bool, iterdecode(fp))\n",
    "        print(summary)\n",
    "        total = summary.get(False, 0) + summary.get(True, 0)\n",
    "        assert total == N\n",
    "except (FileNotFoundError, AssertionError):\n",
    "    with open(files.h2_heading_arr_filename, 'wb') as fp:\n",
    "        for e in get_h2_heading_arr():\n",
    "            cbor2.dump(e, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### parse_tokens_dbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd02905af5b4e00a9cd675e709e9c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "iterator = map(\n",
    "    lambda b: zip(\n",
    "        b[\"id\"].to_numpy(),\n",
    "        b[\"ns\"].to_numpy(),\n",
    "        map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "        b[\"text\"].to_pylist(),\n",
    "    ),\n",
    "    iterator\n",
    ")\n",
    "iterator = itertools.chain.from_iterable(iterator)\n",
    "iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "iterator = enumerate(map(operator.itemgetter(3), iterator))\n",
    "lst = []\n",
    "for node_id, text in iterator:\n",
    "    lst.append(text)\n",
    "    if node_id == 146:\n",
    "        break\n",
    "with open(\"/tmp/wikitext.json\", \"w\") as fp:\n",
    "    json.dump(lst, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/tmp/wikitext.json\", \"r\") as fp:\n",
    "    lst = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtokens = wikiplain.tokenize(lst[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdoc = wikiplain.parse_tokens(wtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead:\n",
    "    def __init__(self, inner, n):\n",
    "        self.inner = iter(inner)\n",
    "        self.bufsize = n\n",
    "        self.buffer = [None for _ in range(n)]\n",
    "        self.closed = False\n",
    "        self.bufhead = 0\n",
    "        self.head = 0\n",
    "        \n",
    "    def peek(self, n):\n",
    "        if n > self.bufsize:\n",
    "            raise ValueError()\n",
    "        return self.buffer[(self.head + n - 1) % self.bufsize]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.closed:\n",
    "            if self.head >= self.bufhead:\n",
    "                raise StopIteration()\n",
    "            val = self.buffer[self.head % self.bufsize]\n",
    "            self.buffer[self.head % self.bufsize] = None\n",
    "            self.head += 1\n",
    "            return val\n",
    "        while self.bufhead - self.head < self.bufsize:\n",
    "            try:\n",
    "                self.buffer[self.bufhead % self.bufsize] = next(self.inner)\n",
    "                self.bufhead += 1\n",
    "            except StopIteration:\n",
    "                self.buffer[self.bufhead % self.bufsize] = None\n",
    "                self.closed = True\n",
    "        val = self.buffer[self.head % self.bufsize]\n",
    "        self.head += 1\n",
    "        try:\n",
    "            self.buffer[self.bufhead % self.bufsize] = next(self.inner)\n",
    "            self.bufhead += 1\n",
    "        except StopIteration:\n",
    "            self.buffer[self.bufhead % self.bufsize] = None\n",
    "            self.closed = True\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event(Enum):\n",
    "    BEGIN = auto()\n",
    "    END = auto()\n",
    "\n",
    "def tree_events(root):\n",
    "    stack = deque([(Event.END, root), (Event.BEGIN, root)])\n",
    "    while len(stack) > 0:\n",
    "        ek, curr = stack.pop()\n",
    "        yield ek, curr\n",
    "        if ek == Event.BEGIN:\n",
    "            for child in reversed(curr.children):\n",
    "                stack.append((Event.END, child))\n",
    "                stack.append((Event.BEGIN, child))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_p = lambda name: lambda elem: hasattr(elem, \"name\") and name == elem.name\n",
    "def node_to_document(parsed, mapper):\n",
    "    stack = deque()\n",
    "    soup = BeautifulSoup(features=\"xml\")\n",
    "    curr = soup\n",
    "    iterator = Lookahead(tree_events(parsed), 4)\n",
    "    for ek, n in iterator:\n",
    "        if ek == Event.BEGIN:\n",
    "            if n.kind == NodeKind.Document:\n",
    "                nxt = soup.new_tag(\"document\")\n",
    "                stack.append(curr)\n",
    "                curr = nxt\n",
    "            elif n.kind == NodeKind.Element:\n",
    "                nxt = soup.new_tag(n.data)\n",
    "                stack.append(curr)\n",
    "                curr = nxt\n",
    "            elif n.kind == NodeKind.Template:\n",
    "                nxt = soup.new_tag(\"wmt\")\n",
    "                stack.append(curr)\n",
    "                curr = nxt\n",
    "                match (iterator.peek(1), iterator.peek(2), iterator.peek(3), iterator.peek(4)):\n",
    "                    case ((Event.BEGIN, Node(kind=NodeKind.Argument)),\n",
    "                          (Event.BEGIN, Node(kind=NodeKind.Content, data=title)),\n",
    "                          (Event.END, Node(kind=NodeKind.Content)),\n",
    "                          (Event.END, Node(kind=NodeKind.Argument)),\n",
    "                    ):\n",
    "                        title = title.strip()\n",
    "                        curr[\"title\"] = title[:1].lower() + title[1:]\n",
    "                        for _ in range(4):\n",
    "                            next(iterator)\n",
    "            elif n.kind == NodeKind.Link:\n",
    "                nxt = soup.new_tag(\"wml\")\n",
    "                stack.append(curr)\n",
    "                curr = nxt\n",
    "            elif n.kind == NodeKind.Argument:\n",
    "                nxt = soup.new_tag(\"arg\")\n",
    "                stack.append(curr)\n",
    "                curr = nxt\n",
    "                match (iterator.peek(1), iterator.peek(2)):\n",
    "                    case ((Event.BEGIN, Node(kind=NodeKind.Content, data=data)),\n",
    "                          (Event.END, Node(kind=NodeKind.Content)),\n",
    "                    ) if data.startswith('|'):\n",
    "                        m = re.match(r\"\\|\\s*([a-zA-Z][a-zA-Z0-9_]*)\\s*=\\s*\", data)\n",
    "                        if m is not None:\n",
    "                            curr[\"name\"] = m.group(1)\n",
    "                            txt = re.sub(r\"[']{2,3}\", \"\", data[m.end():])\n",
    "                            curr.append(txt)\n",
    "                        else:\n",
    "                            txt = re.sub(r\"[']{2,3}\", \"\", data[1:])\n",
    "                            curr.append(txt)\n",
    "                        for _ in range(2):\n",
    "                            next(iterator)\n",
    "            elif n.kind == NodeKind.Content:\n",
    "                txt = re.sub(r\"[']{2,3}\", \"\", n.data)\n",
    "                curr.append(txt)\n",
    "        else:\n",
    "            if n.kind != NodeKind.Content:\n",
    "                prev = stack.pop()\n",
    "                for el in mapper(curr):\n",
    "                    prev.append(el)\n",
    "                curr = prev\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document([Template([Argument([Content(\"Short description\")]), Argument([Content(\"|American rapper\")])]), Content(\"\\n\"), Template([Argument([Content(\"Infobox musical artist\\n\")]), Argument([Content(\"| name              = Arabian Prince\\n\")]), Argument([Content(\"| image             = The_Arabian_Prince_aka_Professor_X_(N.W.A.).jpg\\n\")]), Argument([Content(\"| caption           = Arabian Prince in 2018\\n\")]), Argument([Content(\"| image_size        = \\n\")]), Argument([Content(\"| birth_name        = Kim Renard Nazel\\n\")]), Argument([Content(\"| alias             = Professor X\\n\")]), Argument([Content(\"| birth_date        = \"), Template([Argument([Content(\"birth date and age\")]), Argument([Content(\"|1965\")]), Argument([Content(\"|6\")]), Argument([Content(\"|17\")])]), Content(\"\\n\")]), Argument([Content(\"| birth_place       = \"), Link([Argument([Content(\"Compton, California\")])]), Content(\", U.S.\\n\")]), Argument([Content(\"| instruments       = \"), Template([Argument([Content(\"hlist\")]), Argument([Content(\"|Vocals\")]), Argument([Content(\"|synthesizer\")]), Argument([Content(\"|keyboards\")]), Argument([Content(\"|turntables\")]), Argument([Content(\"|drum machine\")]), Argument([Content(\"|sampler\")])]), Content(\"\\n\")]), Argument([Content(\"| genre             = \"), Template([Argument([Content(\"hlist\")]), Argument([Content(\"|\"), Link([Argument([Content(\"Hip hop music\")]), Argument([Content(\"|Hip hop\")])]), Content(\"\")]), Argument([Content(\"|\"), Link([Argument([Content(\"Electro music\")]), Argument([Content(\"|electro\")])]), Content(\"\")]), Argument([Content(\"|\"), Link([Argument([Content(\"gangsta rap\")])])])]), Content(\"\\n\")]), Argument([Content(\"| occupations       = \"), Template([Argument([Content(\"hlist\")]), Argument([Content(\"|Rapper\")]), Argument([Content(\"|singer\")]), Argument([Content(\"|songwriter\")]), Argument([Content(\"|\"), Link([Argument([Content(\"record producer\")])]), Content(\"\")]), Argument([Content(\"|\"), Link([Argument([Content(\"DJ\")])])])]), Content(\"\\n\")]), Argument([Content(\"| years_active      = 1984–present\\n\")]), Argument([Content(\"| label             = \"), Template([Argument([Content(\"hlist\")]), Argument([Content(\"|Orpheus\")]), Argument([Content(\"|Da Bozak\")]), Argument([Content(\"|Macola\")]), Argument([Content(\"|\"), Link([Argument([Content(\"Stones Throw Records\")]), Argument([Content(\"|Stones Throw\")])])])]), Content(\"\\n\")]), Argument([Content(\"| past_member_of    = \"), Link([Argument([Content(\"N.W.A\")])]), Content(\"\\n\")]), Argument([Content(\"| website           = \\n\")])]), Content(\"\\n\\n'''Kim Renard Nazel''' (born June 17, 1965),\"), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=http://www.californiabirthindex.org/birth/kim_r_nazel_born_1965_9284309\")]), Argument([Content(\"|title=Kim R Nazel, Born 06/17/1965 in California - CaliforniaBirthIndex.org\")]), Argument([Content(\"|website=www.californiabirthindex.org\")])])])]), Content(\" better known by his \"), Link([Argument([Content(\"stage name\")])]), Content(\"s '''Arabian Prince''' or '''Professor X''', is an American rapper, record producer, and DJ.\"), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=http://hiphopdx.com/interviews/id.1197/title.arabian-prince-new-funky-nation\")]), Argument([Content(\"|title=Arabian Prince: New Funky Nation\")]), Argument([Content(\"|author=HipHopDX\")]), Argument([Content(\"|date=23 August 2008\")]), Argument([Content(\"|work=HipHopDX\")]), Argument([Content(\"|access-date=23 August 2015\")])])])]), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=http://www.phoenixnewtimes.com/music/arabian-prince-what-happened-after-nwa-and-the-posse-6588582\")]), Argument([Content(\"|title=Arabian Prince: What Happened After N.W.A. and the Posse?\")]), Argument([Content(\"|author=Martin Cizmar\")]), Argument([Content(\"|work=Phoenix New Times\")]), Argument([Content(\"|access-date=23 August 2015\")])])])]), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=http://www.scpr.org/blogs/offramp/2012/07/16/7044/lost-nwa-member-arabian-prince-plays-macarthur-par/\")]), Argument([Content(\"|title=Lost N.W.A member Arabian Prince plays MacArthur Park on July 28\")]), Argument([Content(\"|author=Southern California Public Radio\")]), Argument([Content(\"|work=Southern California Public Radio\")]), Argument([Content(\"|date=16 July 2012 \")]), Argument([Content(\"|access-date=23 August 2015\")])])])]), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=http://uk.askmen.com/daily/austin_150/182d_fashion_style.html\")]), Argument([Content(\"|title=N.W.A. - AskMen\")]), Argument([Content(\"|author=Kyle Grace\")]), Argument([Content(\"|work=AskMen\")]), Argument([Content(\"|access-date=23 August 2015\")])])])]), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=http://westcoastpioneers.com/artists/arabian-prince.html \")]), Argument([Content(\"|title=Arabian Prince &#124; West Coast Rap Artists &#124; West Coast Rap Pioneers &#124; Tribute to the Early West Coast Rap Scene: Website Title \")]), Argument([Content(\"|publisher=Westcoastpioneers.com \")]), Argument([Content(\"|date=1965-06-17 \")]), Argument([Content(\"|access-date=2015-08-15 \")]), Argument([Content(\"|url-status=dead \")]), Argument([Content(\"|archive-url=https://web.archive.org/web/20150808034808/http://westcoastpioneers.com/artists/arabian-prince.html \")]), Argument([Content(\"|archive-date=2015-08-08 \")])])])]), Element(\"ref\", [Argument([Template([Argument([Content(\"cite news\")]), Argument([Content(\"|url=http://www.huffingtonpost.com/wendy-brandes/kept-outta-compton-nwas-a_b_8101462.html\")]), Argument([Content(\"|title=Kept Outta \\\"Compton\\\": N.W.A's Arabian Prince Has No Regrets\")]), Argument([Content(\"|author=Brandes, Wendy\")]), Argument([Content(\"|work=Huffington Post\")]), Argument([Content(\"|date=September 8, 2015\")]), Argument([Content(\"|access-date=November 15, 2015\")])])])]), Content(\" He was a founding member of \"), Link([Argument([Content(\"N.W.A\")])]), Content(\". and performed on one track from the group's major debut ''\"), Link([Argument([Content(\"Straight Outta Compton\")])]), Content(\"'' before leaving.\"), Element(\"ref\", [Argument([Template([Argument([Content(\"Cite web \")]), Argument([Content(\"|title=N.W.A.'s Forgotten Member Explains Why He Was The First To Leave The GroupAmbrosia For Heads \")]), Argument([Content(\"|url=https://ambrosiaforheads.com/2019/01/why-arabian-prince-left-nwa-video/ \")]), Argument([Content(\"|access-date=2024-05-23 \")]), Argument([Content(\"|website=ambrosiaforheads.com\")])])])]), Content(\"\\n\\n\"), Element(\"h2\", [Argument([Content(\"Early life\")])]), Content(\"Nazel was born in Compton, California to the son of Joseph \\\"Skippy\\\" Nazel Jr., an African-American author and radio talk show host.\"), Element(\"ref\", [Argument([Template([Argument([Content(\"Cite AV media notes \")]), Argument([Content(\"|title=Innovative Life: The Anthology, 1984-1989 \")]), Argument([Content(\"|title-link=Innovative Life: The Anthology, 1984-1989 \")]), Argument([Content(\"|others=Arabian Prince \")]), Argument([Content(\"|year=2008 \")]), Argument([Content(\"|editor-last = Egon \")]), Argument([Content(\"|first=Amin \")]), Argument([Content(\"|last=Eshaiker \")]), Argument([Content(\"|pages=6-7 \")]), Argument([Content(\"|type=Liner notes \")]), Argument([Content(\"|publisher=\"), Link([Argument([Content(\"Stones Throw Records\")])]), Content(\" \")]), Argument([Content(\"|location=Los Angeles \")])])])]), Content(\"  His musical background came from his mother, a piano teacher and classical musician.\"), Element(\"ref\", []), Content(\" His family tried its best to shelter him, sending him to a Catholic school and keeping him busy with football to keep him away from the gangs. The younger Nazel got his first experience with making music at the radio station his father hosted his talk show on; Nazel used the radio station's equipment to put together \"), Link([Argument([Content(\"mixtape\")])]), Content(\"s that he would sell at school.\"), Element(\"ref\", []), Content(\"  Nazel went on to graduate from \"), Link([Argument([Content(\"Junípero Serra High School (Gardena, California)\")]), Argument([Content(\"|Junípero Serra High School\")])]), Content(\" in nearby Gardena.\"), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=https://melmagazine.com/arabian-prince-left-n-w-a-and-he-s-doing-just-fine-b776410bec32\")]), Argument([Content(\"|title=Arabian Prince Left N.W.A and He's Doing Just Fine\")]), Argument([Content(\"|author=Mike Sager\")]), Argument([Content(\"|date=16 January 2016\")]), Argument([Content(\"|work=MEL Magazine\")]), Argument([Content(\"|access-date=1 November 2017\")])])])]), Content(\"\\n\\n\"), Element(\"h2\", [Argument([Content(\"Music career\")])]), Content(\"Nazel took the stage name of DJ Prince and started selling mixtapes at school. While working at a luggage store at the Del Amo Mall, its owner, Sam Nassif, asked him to DJ a party at a community center. He kept performing there for several weekends and the success persuaded Nassif to invest even more in the place, renaming it \\\"The Cave\\\", where Nazel would continue to host for over three years and even after his N.W.A days. Nassif also funded DJ Prince's first record, \\\"Strange Life\\\".\"), Element(\"ref\", [Argument([Template([Argument([Content(\"Cite web \")]), Argument([Content(\"|title=10+ Singers & Rappers Born on June 17 \")]), Argument([Content(\"|url=https://www.gemtracks.com/guides/view.php?title=top-singers-rappers-born-on-june-17&id=5843 \")]), Argument([Content(\"|access-date=2024-06-06 \")]), Argument([Content(\"|website=Gemtracks Beats \")]), Argument([Content(\"|language=en\")])])])]), Content(\"\\n\\n\\n\\nHe changed his stage name when he was 15 years old at the Skateland USA, the same skating venue credited for launching N.W.A a few years later, due to a fan's suggestion. He said about his name:\"), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=https://www.thekindland.com/culture/how-arabian-prince-was-written-straight-outta-compton-1368\")]), Argument([Content(\"|title=How Arabian Prince was written \\\"Straight Outta Compton\\\"\")]), Argument([Content(\"|author=Jasmin St.Claire\")]), Argument([Content(\"|date=3 May 2016\")]), Argument([Content(\"|work=Kindland\")]), Argument([Content(\"|access-date=7 March 2019\")]), Argument([Content(\"|archive-date=8 March 2019\")]), Argument([Content(\"|archive-url=https://web.archive.org/web/20190308080816/https://www.thekindland.com/culture/how-arabian-prince-was-written-straight-outta-compton-1368\")]), Argument([Content(\"|url-status=dead\")])])])]), Template([Argument([Content(\"Blockquote\\n\")]), Argument([Content(\"|text=I called myself DJ Prince at first; back in the day, I always used to dress like \"), Link([Argument([Content(\"Prince (musician)\")]), Argument([Content(\"|Prince\")])]), Content(\". That was the thing in the early '80s\\u{200a}—\\u{200a}either you dressed like Prince or you dressed like \"), Link([Argument([Content(\"Michael Jackson\")])]), Content(\". I used to wear the tight parachute pants, and I had the trim moustache, the whole thing. One day I was DJing at a skating rink. I was with \"), Link([Argument([Content(\"Egyptian Lover\")])]), Content(\", that was my boy, still is. This girl comes up to us and asks us our names. And he's like, \\\"I'm Egyptian Lover.\\\" And I'm like, \\\"I'm DJ Prince.\\\" She looks at me and goes, \\\"I always see you two together. You should call yourself Arabian Prince.\\\" And I guess that just stuck.\"), Element(\"ref\", []), Content(\"\\n\")])]), Content(\"\\n\\nArabian Prince started working with \"), Link([Argument([Content(\"Russ Parr\")]), Argument([Content(\"|Bobby Jimmy & the Critters\")])]), Content(\" in 1984. He also produced the hit single and album for \"), Link([Argument([Content(\"J.J. Fad\")])]), Content(\", \\\"\"), Link([Argument([Content(\"Supersonic (J.J. Fad song)\")]), Argument([Content(\"|Supersonic\")])]), Content(\"\\\".\\n\\nIn 1986, he was a founding member of \"), Link([Argument([Content(\"N.W.A\")])]), Content(\", helping with production on some tracks and appearing as a vocalist on the last track of N.W.A.'s hit studio album ''\"), Link([Argument([Content(\"Straight Outta Compton\")])]), Content(\"'' (1988), \\\"Something 2 Dance 2\\\",\"), Element(\"ref\", [Argument([Template([Argument([Content(\"Cite web \")]), Argument([Content(\"|date=2023-08-05 \")]), Argument([Content(\"|title=Rediscover N.W.A’s ‘Straight Outta Compton’ Turns 35 \"), Template([Argument([Content(\"!\")])]), Content(\" Tribute \")]), Argument([Content(\"|url=https://albumism.com/features/nwa-straight-outta-compton-album-anniversary \")]), Argument([Content(\"|access-date=2024-05-23 \")]), Argument([Content(\"|website=Albumism \")]), Argument([Content(\"|language=en-US\")])])])]), Content(\" a relatively radio-friendly song which was also removed from later pressings of the album due to a dispute.\"), Element(\"ref\", [Argument([Template([Argument([Content(\"Cite web \")]), Argument([Content(\"|title=N.W.A.'s Forgotten Member Explains Why He Was The First To Leave The GroupAmbrosia For Heads \")]), Argument([Content(\"|url=https://ambrosiaforheads.com/2019/01/why-arabian-prince-left-nwa-video/ \")]), Argument([Content(\"|access-date=2024-05-23 \")]), Argument([Content(\"|website=ambrosiaforheads.com\")])])])]), Content(\" Arabian Prince left over royalty and contract disagreements. \\\"I started off as a solo artist\\\", he said, \\\"so I was aware of what a royalty statement was. I knew that when these many records were sold, there is a quarterly statement. When you look at it, you can see how much money was paid and then share it. This was not the case. We were also never paid for touring.\\\" \"), Link([Argument([Content(\"Eazy-E\")])]), Content(\", Ice Cube and \"), Link([Argument([Content(\"MC Ren\")])]), Content(\" remained as the main performers, \"), Link([Argument([Content(\"DJ Yella\")])]), Content(\" was the \"), Link([Argument([Content(\"turntablist\")])]), Content(\" and \"), Link([Argument([Content(\"Dr. Dre\")])]), Content(\" was the main producer.\"), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=http://www.laweekly.com/music/whatever-happened-to-nwas-posse-2164896\")]), Argument([Content(\"|title=Whatever Happened to N.W.A's Posse?\")]), Argument([Content(\"|author=Martin Cizmar\")]), Argument([Content(\"|work=L.A. Weekly\")]), Argument([Content(\"|access-date=23 August 2015\")])])])]), Content(\"\\n\\nAfter leaving N.W.A, Arabian Prince began a solo career. His first album, ''\"), Link([Argument([Content(\"Brother Arab\")])]), Content(\"'', was released in 1989 with the single \\\"She's Got A Big Posse\\\"; ''\"), Link([Argument([Content(\"Where's My Bytches\")])]), Content(\"'' followed in 1993.\\n\\nIn the mid-2000s, he started releasing music again, with his Professor X project on the Dutch label Clone Records. \\\"I could not release the record under Arabian Prince\\\", he said, \\\"because I already had a single out, so I called myself Professor X on that record.\\\"\"), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=http://www.sfweekly.com/music/allshookdown/hey-dj/hey-dj-og-arabian-prince/\")]), Argument([Content(\"|title=Hey, DJ: OG Arabian Prince\")]), Argument([Content(\"|author=Christina Li\")]), Argument([Content(\"|date=16 February 2017 \")])])])]), Content(\" In 2007, he performed as a DJ on the 2K Sports Holiday Bounce Tour with artists from the \"), Link([Argument([Content(\"Stones Throw\")])]), Content(\" label. In 2008, Stones Throw released a compilation of his electro-rap material from the 1980s.\"), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web \")]), Argument([Content(\"|url=http://stonesthrow.com/arabianprince/ \")]), Argument([Content(\"|title=Arabian Prince &#124; Stones Throw Records \")]), Argument([Content(\"|publisher=Stonesthrow.com \")]), Argument([Content(\"|access-date=2015-08-15 \")]), Argument([Content(\"|archive-date=2019-05-16 \")]), Argument([Content(\"|archive-url=https://web.archive.org/web/20190516034523/https://www.stonesthrow.com/arabianprince \")]), Argument([Content(\"|url-status=dead \")])])])]), Content(\" One of his songs was included on the 2007 video game, ''\"), Link([Argument([Content(\"College Hoops 2K8\")])]), Content(\"''.\\n\\nIn 2015, a biopic about N.W.A. titled ''\"), Link([Argument([Content(\"Straight Outta Compton (film)\")]), Argument([Content(\"|Straight Outta Compton\")])]), Content(\"'' was released; however, Arabian Prince was not portrayed in the film.\"), Element(\"ref\", [Argument([Template([Argument([Content(\"Cite web \")]), Argument([Content(\"|date=2015-09-08 \")]), Argument([Content(\"|title=Kept Outta \\\"Compton\\\": N.W.A's Arabian Prince Has No Regrets \")]), Argument([Content(\"|url=https://www.huffpost.com/entry/kept-outta-compton-nwas-a_b_8101462 \")]), Argument([Content(\"|access-date=2024-03-29 \")]), Argument([Content(\"|website=HuffPost \")]), Argument([Content(\"|language=en\")])])])]), Content(\" After the release film, Prince said to VladTV: \\\"A lot of the scenes in real life, I was there—I'm just not there in the film, which I'm like, if you're gonna write me out of a movie, shoot some other scenes. Don't write scenes where I was there.\\\"\"), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=https://www.youtube.com/watch?v=s1mIDSgEoVg\")]), Argument([Content(\"| archive-url=https://ghostarchive.org/varchive/youtube/20211122/s1mIDSgEoVg\")]), Argument([Content(\"| archive-date=2021-11-22 \")]), Argument([Content(\"| url-status=live\")]), Argument([Content(\"|title=Arabian Prince reveals discrepancies in \\\"Straight Outta Compton\\\"\")]), Argument([Content(\"| via=\"), Link([Argument([Content(\"YouTube\")])])])]), Template([Argument([Content(\"cbignore\")])])])]), Content(\" Some of the pivotal scenes would be choosing the name for the band, the tour and the infamous Detroit concert. He also remembers himself as the main opposer to \"), Link([Argument([Content(\"Jerry Heller\")])]), Content(\" about the royalties and the money, a role that in the film was instead given to Ice Cube.\\n\\nThe following year, N.W.A. was inducted into the \"), Link([Argument([Content(\"Rock and Roll Hall of Fame\")])]), Content(\", but again, Arabian Prince was not included nor mentioned.\"), Element(\"ref\", []), Content(\"\\n\\nIn 2018, Arabian Prince appeared on the ''\"), Link([Argument([Content(\"AmeriKKKant\")])]), Content(\"'' album of industrial-metal band \"), Link([Argument([Content(\"Ministry (band)\")]), Argument([Content(\"|Ministry\")])]), Content(\". He made a second appearance on Ministry's 2021 album ''\"), Link([Argument([Content(\"Moral Hygiene\")])]), Content(\"''.\"), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=https://www.loudersound.com/news/ministry-bring-in-ex-nwa-man-arabian-prince\")]), Argument([Content(\"|title=Ministry Bring In Ex-NWA Man Arabian Prince\")]), Argument([Content(\"|author=Scott Munro\")]), Argument([Content(\"|work=Metal Hammer\")]), Argument([Content(\"|date=2 March 2017 \")]), Argument([Content(\"|access-date=19 April 2018\")])])])]), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=https://www.blabbermouth.net/news/ministry-shares-unity-mix-of-good-trouble-single-featuring-n-w-a-s-arabian-prince/\")]), Argument([Content(\"|title=MINISTRY Shares 'Unity Mix' Of 'Good Trouble' Single Featuring N.W.A.'s ARABIAN PRINCE\")]), Argument([Content(\"|author=\"), Link([Argument([Content(\"Blabbermouth.net\")])]), Content(\"\")]), Argument([Content(\"|work=Blabbermouth\")]), Argument([Content(\"|date=15 July 2021 \")]), Argument([Content(\"|access-date=15 July 2021\")])])])]), Content(\"\\n\\n\"), Element(\"h2\", [Argument([Content(\"Other ventures\")])]), Content(\"Aside from his music career, he worked in \"), Link([Argument([Content(\"special effects\")])]), Content(\", 3D animation and \"), Link([Argument([Content(\"video game\")])]), Content(\"s.\"), Element(\"ref\", []), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=http://www.laweekly.com/music/arabian-prince-a-jheri-blossoms-2155689\")]), Argument([Content(\"|title=Arabian Prince: A Jheri Blossoms\")]), Argument([Content(\"|first=Chris\")]), Argument([Content(\"|last=Martins\")]), Argument([Content(\"|date=10 September 2008\")])])])]), Element(\"ref\", [Argument([Template([Argument([Content(\"cite web\")]), Argument([Content(\"|url=http://articles.latimes.com/2008/aug/22/entertainment/et-prince22\")]), Argument([Content(\"|title=His 'Innovative Life'\")]), Argument([Content(\"|first=Jeff\")]), Argument([Content(\"|last=Weiss\")]), Argument([Content(\"|date=22 August 2008\")]), Argument([Content(\"|via=LA Times\")])])])]), Content(\"\\n\\n\"), Element(\"h2\", [Argument([Content(\"Discography\")])]), Content(\"\\n\"), Element(\"h3\", [Argument([Content(\"Solo\")])]), Content(\"*''Strange Life'' (Rapsur, 1984)\\n* It Ain’t Tough (Rapsur, 1985)\\n* Take You Home Girl / Innovator (Rapsur, 1985)\\n* Situation Hot (Street Kut, 1986)\\n* Freak City (Macola, 1986)\\n* Professor X (Saga) (Techno Kut, 1989)\\n*''\"), Link([Argument([Content(\"Brother Arab\")])]), Content(\"'' (Orpheus, 1989)\\n*''\"), Link([Argument([Content(\"Where's My Bytches\")])]), Content(\"'' (Da Bozak, 1993)\\n* Simple Planet / Beatdabeat (Stones Throw, 2008)\\n\\n\"), Element(\"h3\", [Argument([Content(\"Compilations\")])]), Content(\"*''\"), Link([Argument([Content(\"Situation Hot\")])]), Content(\"'' (Macola, 1990)\\n*''\"), Link([Argument([Content(\"Innovative Life: The Anthology, 1984–1989\")])]), Content(\"'' (\"), Link([Argument([Content(\"Stones Throw Records\")]), Argument([Content(\"|Stones Throw\")])]), Content(\", 2008)\"), Element(\"ref\", [Argument([Template([Argument([Content(\"cite magazine\")]), Argument([Content(\"|last=Paine \")]), Argument([Content(\"|first=Jake \")]), Argument([Content(\"|url=http://www.hiphopdx.com/index/news/id.7241/title.stones-throw-records-releases-n-w-a-affiliate-album \")]), Argument([Content(\"|title=Stones Throw Records Releases N.W.A. Affiliate Album \")]), Argument([Content(\"|magazine=HipHopDX \")]), Argument([Content(\"|date=2008-07-03 \")]), Argument([Content(\"|access-date=2015-08-15\")])])])]), Content(\"\\n*''Professor X'' (Clone, 2007/2008)\\n\\n\"), Element(\"h3\", [Argument([Content(\"With Bobby Jimmy and the Critters\")])]), Content(\"* Ugly Knuckle Butt (1985)\\n* Roaches: The Beginning (1986)\\n* Back and Proud (1987)\\n\\n\"), Element(\"h3\", [Argument([Content(\"With N.W.A\")])]), Content(\"*\\\"\"), Link([Argument([Content(\"Panic Zone\")])]), Content(\"\\\" (single) (1987)\\n*''\"), Link([Argument([Content(\"N.W.A. and the Posse\")])]), Content(\"'' (1987)\\n*''\"), Link([Argument([Content(\"Straight Outta Compton\")])]), Content(\"''  (1988)\\n\\n\"), Element(\"h2\", [Argument([Content(\"References\")])]), Template([Argument([Content(\"Reflist\")])]), Content(\"\\n\\n\"), Element(\"h2\", [Argument([Content(\"External links\")])]), Content(\"*[https://web.archive.org/web/20150808034808/http://westcoastpioneers.com/artists/arabian-prince.html Interview with Arabian Prince & Biography on westcoastpioneers]\\n*[http://larecord.com/issues/2008/08/19/arabian-prince-women-and-partying-and-freaks/ August 2008 Interview] with \"), Link([Argument([Content(\"L.A. Record\")])]), Content(\"\\n*[https://web.archive.org/web/20120610182324/http://www.redbullmusicacademy.com/lectures/arabian-prince--brother-arab Arabian Prince RBMA lecture]\\n*[http://blogs.phoenixnewtimes.com/uponsun/2010/03/_arabian_prince_also_known.php Arabian Prince: What Happened After ''N.W.A. and the Posse''?] \"), Template([Argument([Content(\"Webarchive\")]), Argument([Content(\"|url=https://web.archive.org/web/20150405032751/http://blogs.phoenixnewtimes.com/uponsun/2010/03/_arabian_prince_also_known.php \")]), Argument([Content(\"|date=2015-04-05 \")])]), Content(\" at ''\"), Link([Argument([Content(\"Phoenix New Times\")])]), Content(\"''\\n*[https://www.namm.org/library/oral-history/dj-arabian-prince DJ Arabian Prince Interview] at \"), Link([Argument([Content(\"NAMM Oral History Program\")]), Argument([Content(\"|NAMM Oral History Library\")])]), Content(\" (2020)\\n\\n\"), Template([Argument([Content(\"Arabian Prince\")]), Argument([Content(\"|state=expanded\")])]), Content(\"\\n\"), Template([Argument([Content(\"N.W.A\")])]), Content(\"\\n\\n\"), Template([Argument([Content(\"Authority control\")])]), Content(\"\\n\\n\"), Template([Argument([Content(\"DEFAULTSORT:Prince, Arabian\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:1964 births\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:Living people\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:African-American male rappers\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:American male rappers\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:Musicians from Compton, California\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:N.W.A members\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:Ruthless Records artists\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:20th-century American rappers\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:21st-century American rappers\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:American hip hop singers\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:African-American male singer-songwriters\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:American male singer-songwriters\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:20th-century African-American male singers\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:20th-century American male singers\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:20th-century American singers\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:21st-century African-American male singers\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:21st-century American male singers\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:Singer-songwriters from California\")])]), Content(\"\\n\"), Link([Argument([Content(\"Category:Special effects people\")])])])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<document>\n",
      " <wmt title=\"short description\">\n",
      "  <arg>\n",
      "   American rapper\n",
      "  </arg>\n",
      " </wmt>\n",
      " <wmt title=\"infobox musical artist\">\n",
      "  <arg name=\"name\">\n",
      "   Arabian Prince\n",
      "  </arg>\n",
      "  <arg name=\"image\">\n",
      "   The_Arabian_Prince_aka_Professor_X_(N.W.A.).jpg\n",
      "  </arg>\n",
      "  <arg name=\"caption\">\n",
      "   Arabian Prince in 2018\n",
      "  </arg>\n",
      "  <arg name=\"image_size\">\n",
      "  </arg>\n",
      "  <arg name=\"birth_name\">\n",
      "   Kim Renard Nazel\n",
      "  </arg>\n",
      "  <arg name=\"alias\">\n",
      "   Professor X\n",
      "  </arg>\n",
      "  <arg name=\"birth_date\">\n",
      "   <wmt title=\"birth date and age\">\n",
      "    <arg>\n",
      "     1965\n",
      "    </arg>\n",
      "    <arg>\n",
      "     6\n",
      "    </arg>\n",
      "    <arg>\n",
      "     17\n",
      "    </arg>\n",
      "   </wmt>\n",
      "  </arg>\n",
      "  <arg name=\"birth_place\">\n",
      "   <wml>\n",
      "    <arg>\n",
      "     Compton, California\n",
      "    </arg>\n",
      "   </wml>\n",
      "   , U.S.\n",
      "  </arg>\n",
      "  <arg name=\"instruments\">\n",
      "   <wmt title=\"hlist\">\n",
      "    <arg>\n",
      "     Vocals\n",
      "    </arg>\n",
      "    <arg>\n",
      "     synthesizer\n",
      "    </arg>\n",
      "    <arg>\n",
      "     keyboards\n",
      "    </arg>\n",
      "    <arg>\n",
      "     turntables\n",
      "    </arg>\n",
      "    <arg>\n",
      "     drum machine\n",
      "    </arg>\n",
      "    <arg>\n",
      "     sampler\n",
      "    </arg>\n",
      "   </wmt>\n",
      "  </arg>\n",
      "  <arg name=\"genre\">\n",
      "   <wmt title=\"hlist\">\n",
      "    <arg>\n",
      "     <wml>\n",
      "      <arg>\n",
      "       Hip hop music\n",
      "      </arg>\n",
      "      <arg>\n",
      "       Hip hop\n",
      "      </arg>\n",
      "     </wml>\n",
      "    </arg>\n",
      "    <arg>\n",
      "     <wml>\n",
      "      <arg>\n",
      "       Electro music\n",
      "      </arg>\n",
      "      <arg>\n",
      "       electro\n",
      "      </arg>\n",
      "     </wml>\n",
      "    </arg>\n",
      "    <arg>\n",
      "     <wml>\n",
      "      <arg>\n",
      "       gangsta rap\n",
      "      </arg>\n",
      "     </wml>\n",
      "    </arg>\n",
      "   </wmt>\n",
      "  </arg>\n",
      "  <arg name=\"occupations\">\n",
      "   <wmt title=\"hlist\">\n",
      "    <arg>\n",
      "     Rapper\n",
      "    </arg>\n",
      "    <arg>\n",
      "     singer\n",
      "    </arg>\n",
      "    <arg>\n",
      "     songwriter\n",
      "    </arg>\n",
      "    <arg>\n",
      "     <wml>\n",
      "      <arg>\n",
      "       record producer\n",
      "      </arg>\n",
      "     </wml>\n",
      "    </arg>\n",
      "    <arg>\n",
      "     <wml>\n",
      "      <arg>\n",
      "       DJ\n",
      "      </arg>\n",
      "     </wml>\n",
      "    </arg>\n",
      "   </wmt>\n",
      "  </arg>\n",
      "  <arg name=\"years_active\">\n",
      "   1984–present\n",
      "  </arg>\n",
      "  <arg name=\"label\">\n",
      "   <wmt title=\"hlist\">\n",
      "    <arg>\n",
      "     Orpheus\n",
      "    </arg>\n",
      "    <arg>\n",
      "     Da Bozak\n",
      "    </arg>\n",
      "    <arg>\n",
      "     Macola\n",
      "    </arg>\n",
      "    <arg>\n",
      "     <wml>\n",
      "      <arg>\n",
      "       Stones Throw Records\n",
      "      </arg>\n",
      "      <arg>\n",
      "       Stones Throw\n",
      "      </arg>\n",
      "     </wml>\n",
      "    </arg>\n",
      "   </wmt>\n",
      "  </arg>\n",
      "  <arg name=\"past_member_of\">\n",
      "   <wml>\n",
      "    <arg>\n",
      "     N.W.A\n",
      "    </arg>\n",
      "   </wml>\n",
      "  </arg>\n",
      "  <arg name=\"website\">\n",
      "  </arg>\n",
      " </wmt>\n",
      " Kim Renard Nazel (born June 17, 1965),\n",
      " better known by his\n",
      " <wml>\n",
      "  <arg>\n",
      "   stage name\n",
      "  </arg>\n",
      " </wml>\n",
      " s Arabian Prince or Professor X, is an American rapper, record producer, and DJ.\n",
      " He was a founding member of\n",
      " <wml>\n",
      "  <arg>\n",
      "   N.W.A\n",
      "  </arg>\n",
      " </wml>\n",
      " . and performed on one track from the group's major debut\n",
      " <wml>\n",
      "  <arg>\n",
      "   Straight Outta Compton\n",
      "  </arg>\n",
      " </wml>\n",
      " before leaving.\n",
      " <h2>\n",
      "  <arg>\n",
      "   Early life\n",
      "  </arg>\n",
      " </h2>\n",
      " Nazel was born in Compton, California to the son of Joseph \"Skippy\" Nazel Jr., an African-American author and radio talk show host.\n",
      " His musical background came from his mother, a piano teacher and classical musician.\n",
      " His family tried its best to shelter him, sending him to a Catholic school and keeping him busy with football to keep him away from the gangs. The younger Nazel got his first experience with making music at the radio station his father hosted his talk show on; Nazel used the radio station's equipment to put together\n",
      " <wml>\n",
      "  <arg>\n",
      "   mixtape\n",
      "  </arg>\n",
      " </wml>\n",
      " s that he would sell at school.\n",
      " Nazel went on to graduate from\n",
      " <wml>\n",
      "  <arg>\n",
      "   Junípero Serra High School (Gardena, California)\n",
      "  </arg>\n",
      "  <arg>\n",
      "   Junípero Serra High School\n",
      "  </arg>\n",
      " </wml>\n",
      " in nearby Gardena.\n",
      " <h2>\n",
      "  <arg>\n",
      "   Music career\n",
      "  </arg>\n",
      " </h2>\n",
      " Nazel took the stage name of DJ Prince and started selling mixtapes at school. While working at a luggage store at the Del Amo Mall, its owner, Sam Nassif, asked him to DJ a party at a community center. He kept performing there for several weekends and the success persuaded Nassif to invest even more in the place, renaming it \"The Cave\", where Nazel would continue to host for over three years and even after his N.W.A days. Nassif also funded DJ Prince's first record, \"Strange Life\".\n",
      " He changed his stage name when he was 15 years old at the Skateland USA, the same skating venue credited for launching N.W.A a few years later, due to a fan's suggestion. He said about his name:\n",
      " <wmt title=\"blockquote\">\n",
      "  <arg name=\"text\">\n",
      "   I called myself DJ Prince at first; back in the day, I always used to dress like\n",
      "   <wml>\n",
      "    <arg>\n",
      "     Prince (musician)\n",
      "    </arg>\n",
      "    <arg>\n",
      "     Prince\n",
      "    </arg>\n",
      "   </wml>\n",
      "   . That was the thing in the early '80s — either you dressed like Prince or you dressed like\n",
      "   <wml>\n",
      "    <arg>\n",
      "     Michael Jackson\n",
      "    </arg>\n",
      "   </wml>\n",
      "   . I used to wear the tight parachute pants, and I had the trim moustache, the whole thing. One day I was DJing at a skating rink. I was with\n",
      "   <wml>\n",
      "    <arg>\n",
      "     Egyptian Lover\n",
      "    </arg>\n",
      "   </wml>\n",
      "   , that was my boy, still is. This girl comes up to us and asks us our names. And he's like, \"I'm Egyptian Lover.\" And I'm like, \"I'm DJ Prince.\" She looks at me and goes, \"I always see you two together. You should call yourself Arabian Prince.\" And I guess that just stuck.\n",
      "  </arg>\n",
      " </wmt>\n",
      " Arabian Prince started working with\n",
      " <wml>\n",
      "  <arg>\n",
      "   Russ Parr\n",
      "  </arg>\n",
      "  <arg>\n",
      "   Bobby Jimmy &amp; the Critters\n",
      "  </arg>\n",
      " </wml>\n",
      " in 1984. He also produced the hit single and album for\n",
      " <wml>\n",
      "  <arg>\n",
      "   J.J. Fad\n",
      "  </arg>\n",
      " </wml>\n",
      " , \"\n",
      " <wml>\n",
      "  <arg>\n",
      "   Supersonic (J.J. Fad song)\n",
      "  </arg>\n",
      "  <arg>\n",
      "   Supersonic\n",
      "  </arg>\n",
      " </wml>\n",
      " \".\n",
      "\n",
      "In 1986, he was a founding member of\n",
      " <wml>\n",
      "  <arg>\n",
      "   N.W.A\n",
      "  </arg>\n",
      " </wml>\n",
      " , helping with production on some tracks and appearing as a vocalist on the last track of N.W.A.'s hit studio album\n",
      " <wml>\n",
      "  <arg>\n",
      "   Straight Outta Compton\n",
      "  </arg>\n",
      " </wml>\n",
      " (1988), \"Something 2 Dance 2\",\n",
      " a relatively radio-friendly song which was also removed from later pressings of the album due to a dispute.\n",
      " Arabian Prince left over royalty and contract disagreements. \"I started off as a solo artist\", he said, \"so I was aware of what a royalty statement was. I knew that when these many records were sold, there is a quarterly statement. When you look at it, you can see how much money was paid and then share it. This was not the case. We were also never paid for touring.\"\n",
      " <wml>\n",
      "  <arg>\n",
      "   Eazy-E\n",
      "  </arg>\n",
      " </wml>\n",
      " , Ice Cube and\n",
      " <wml>\n",
      "  <arg>\n",
      "   MC Ren\n",
      "  </arg>\n",
      " </wml>\n",
      " remained as the main performers,\n",
      " <wml>\n",
      "  <arg>\n",
      "   DJ Yella\n",
      "  </arg>\n",
      " </wml>\n",
      " was the\n",
      " <wml>\n",
      "  <arg>\n",
      "   turntablist\n",
      "  </arg>\n",
      " </wml>\n",
      " and\n",
      " <wml>\n",
      "  <arg>\n",
      "   Dr. Dre\n",
      "  </arg>\n",
      " </wml>\n",
      " was the main producer.\n",
      " After leaving N.W.A, Arabian Prince began a solo career. His first album,\n",
      " <wml>\n",
      "  <arg>\n",
      "   Brother Arab\n",
      "  </arg>\n",
      " </wml>\n",
      " , was released in 1989 with the single \"She's Got A Big Posse\";\n",
      " <wml>\n",
      "  <arg>\n",
      "   Where's My Bytches\n",
      "  </arg>\n",
      " </wml>\n",
      " followed in 1993.\n",
      "\n",
      "In the mid-2000s, he started releasing music again, with his Professor X project on the Dutch label Clone Records. \"I could not release the record under Arabian Prince\", he said, \"because I already had a single out, so I called myself Professor X on that record.\"\n",
      " In 2007, he performed as a DJ on the 2K Sports Holiday Bounce Tour with artists from the\n",
      " <wml>\n",
      "  <arg>\n",
      "   Stones Throw\n",
      "  </arg>\n",
      " </wml>\n",
      " label. In 2008, Stones Throw released a compilation of his electro-rap material from the 1980s.\n",
      " One of his songs was included on the 2007 video game,\n",
      " <wml>\n",
      "  <arg>\n",
      "   College Hoops 2K8\n",
      "  </arg>\n",
      " </wml>\n",
      " .\n",
      "\n",
      "In 2015, a biopic about N.W.A. titled\n",
      " <wml>\n",
      "  <arg>\n",
      "   Straight Outta Compton (film)\n",
      "  </arg>\n",
      "  <arg>\n",
      "   Straight Outta Compton\n",
      "  </arg>\n",
      " </wml>\n",
      " was released; however, Arabian Prince was not portrayed in the film.\n",
      " After the release film, Prince said to VladTV: \"A lot of the scenes in real life, I was there—I'm just not there in the film, which I'm like, if you're gonna write me out of a movie, shoot some other scenes. Don't write scenes where I was there.\"\n",
      " Some of the pivotal scenes would be choosing the name for the band, the tour and the infamous Detroit concert. He also remembers himself as the main opposer to\n",
      " <wml>\n",
      "  <arg>\n",
      "   Jerry Heller\n",
      "  </arg>\n",
      " </wml>\n",
      " about the royalties and the money, a role that in the film was instead given to Ice Cube.\n",
      "\n",
      "The following year, N.W.A. was inducted into the\n",
      " <wml>\n",
      "  <arg>\n",
      "   Rock and Roll Hall of Fame\n",
      "  </arg>\n",
      " </wml>\n",
      " , but again, Arabian Prince was not included nor mentioned.\n",
      " In 2018, Arabian Prince appeared on the\n",
      " <wml>\n",
      "  <arg>\n",
      "   AmeriKKKant\n",
      "  </arg>\n",
      " </wml>\n",
      " album of industrial-metal band\n",
      " <wml>\n",
      "  <arg>\n",
      "   Ministry (band)\n",
      "  </arg>\n",
      "  <arg>\n",
      "   Ministry\n",
      "  </arg>\n",
      " </wml>\n",
      " . He made a second appearance on Ministry's 2021 album\n",
      " <wml>\n",
      "  <arg>\n",
      "   Moral Hygiene\n",
      "  </arg>\n",
      " </wml>\n",
      " .\n",
      " <h2>\n",
      "  <arg>\n",
      "   Other ventures\n",
      "  </arg>\n",
      " </h2>\n",
      " Aside from his music career, he worked in\n",
      " <wml>\n",
      "  <arg>\n",
      "   special effects\n",
      "  </arg>\n",
      " </wml>\n",
      " , 3D animation and\n",
      " <wml>\n",
      "  <arg>\n",
      "   video game\n",
      "  </arg>\n",
      " </wml>\n",
      " s.\n",
      " <h2>\n",
      "  <arg>\n",
      "   Discography\n",
      "  </arg>\n",
      " </h2>\n",
      " <h3>\n",
      "  <arg>\n",
      "   Solo\n",
      "  </arg>\n",
      " </h3>\n",
      " *Strange Life (Rapsur, 1984)\n",
      "* It Ain’t Tough (Rapsur, 1985)\n",
      "* Take You Home Girl / Innovator (Rapsur, 1985)\n",
      "* Situation Hot (Street Kut, 1986)\n",
      "* Freak City (Macola, 1986)\n",
      "* Professor X (Saga) (Techno Kut, 1989)\n",
      "*\n",
      " <wml>\n",
      "  <arg>\n",
      "   Brother Arab\n",
      "  </arg>\n",
      " </wml>\n",
      " (Orpheus, 1989)\n",
      "*\n",
      " <wml>\n",
      "  <arg>\n",
      "   Where's My Bytches\n",
      "  </arg>\n",
      " </wml>\n",
      " (Da Bozak, 1993)\n",
      "* Simple Planet / Beatdabeat (Stones Throw, 2008)\n",
      " <h3>\n",
      "  <arg>\n",
      "   Compilations\n",
      "  </arg>\n",
      " </h3>\n",
      " *\n",
      " <wml>\n",
      "  <arg>\n",
      "   Situation Hot\n",
      "  </arg>\n",
      " </wml>\n",
      " (Macola, 1990)\n",
      "*\n",
      " <wml>\n",
      "  <arg>\n",
      "   Innovative Life: The Anthology, 1984–1989\n",
      "  </arg>\n",
      " </wml>\n",
      " (\n",
      " <wml>\n",
      "  <arg>\n",
      "   Stones Throw Records\n",
      "  </arg>\n",
      "  <arg>\n",
      "   Stones Throw\n",
      "  </arg>\n",
      " </wml>\n",
      " , 2008)\n",
      " *Professor X (Clone, 2007/2008)\n",
      " <h3>\n",
      "  <arg>\n",
      "   With Bobby Jimmy and the Critters\n",
      "  </arg>\n",
      " </h3>\n",
      " * Ugly Knuckle Butt (1985)\n",
      "* Roaches: The Beginning (1986)\n",
      "* Back and Proud (1987)\n",
      " <h3>\n",
      "  <arg>\n",
      "   With N.W.A\n",
      "  </arg>\n",
      " </h3>\n",
      " *\"\n",
      " <wml>\n",
      "  <arg>\n",
      "   Panic Zone\n",
      "  </arg>\n",
      " </wml>\n",
      " \" (single) (1987)\n",
      "*\n",
      " <wml>\n",
      "  <arg>\n",
      "   N.W.A. and the Posse\n",
      "  </arg>\n",
      " </wml>\n",
      " (1987)\n",
      "*\n",
      " <wml>\n",
      "  <arg>\n",
      "   Straight Outta Compton\n",
      "  </arg>\n",
      " </wml>\n",
      " (1988)\n",
      " <h2>\n",
      "  <arg>\n",
      "   References\n",
      "  </arg>\n",
      " </h2>\n",
      " <wmt title=\"reflist\"/>\n",
      " <h2>\n",
      "  <arg>\n",
      "   External links\n",
      "  </arg>\n",
      " </h2>\n",
      " *[https://web.archive.org/web/20150808034808/http://westcoastpioneers.com/artists/arabian-prince.html Interview with Arabian Prince &amp; Biography on westcoastpioneers]\n",
      "*[http://larecord.com/issues/2008/08/19/arabian-prince-women-and-partying-and-freaks/ August 2008 Interview] with\n",
      " <wml>\n",
      "  <arg>\n",
      "   L.A. Record\n",
      "  </arg>\n",
      " </wml>\n",
      " *[https://web.archive.org/web/20120610182324/http://www.redbullmusicacademy.com/lectures/arabian-prince--brother-arab Arabian Prince RBMA lecture]\n",
      "*[http://blogs.phoenixnewtimes.com/uponsun/2010/03/_arabian_prince_also_known.php Arabian Prince: What Happened After N.W.A. and the Posse?]\n",
      " <wmt title=\"webarchive\">\n",
      "  <arg name=\"url\">\n",
      "   https://web.archive.org/web/20150405032751/http://blogs.phoenixnewtimes.com/uponsun/2010/03/_arabian_prince_also_known.php\n",
      "  </arg>\n",
      "  <arg name=\"date\">\n",
      "   2015-04-05\n",
      "  </arg>\n",
      " </wmt>\n",
      " at\n",
      " <wml>\n",
      "  <arg>\n",
      "   Phoenix New Times\n",
      "  </arg>\n",
      " </wml>\n",
      " *[https://www.namm.org/library/oral-history/dj-arabian-prince DJ Arabian Prince Interview] at\n",
      " <wml>\n",
      "  <arg>\n",
      "   NAMM Oral History Program\n",
      "  </arg>\n",
      "  <arg>\n",
      "   NAMM Oral History Library\n",
      "  </arg>\n",
      " </wml>\n",
      " (2020)\n",
      " <wmt title=\"arabian Prince\">\n",
      "  <arg name=\"state\">\n",
      "   expanded\n",
      "  </arg>\n",
      " </wmt>\n",
      " <wmt title=\"n.W.A\"/>\n",
      " <wmt title=\"authority control\"/>\n",
      " <wmt title=\"dEFAULTSORT:Prince, Arabian\"/>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:1964 births\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:Living people\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:African-American male rappers\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:American male rappers\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:Musicians from Compton, California\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:N.W.A members\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:Ruthless Records artists\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:20th-century American rappers\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:21st-century American rappers\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:American hip hop singers\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:African-American male singer-songwriters\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:American male singer-songwriters\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:20th-century African-American male singers\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:20th-century American male singers\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:20th-century American singers\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:21st-century African-American male singers\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:21st-century American male singers\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:Singer-songwriters from California\n",
      "  </arg>\n",
      " </wml>\n",
      " <wml>\n",
      "  <arg>\n",
      "   Category:Special effects people\n",
      "  </arg>\n",
      " </wml>\n",
      "</document>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(node_to_document(wdoc, lambda el: [] if el.name == \"ref\" else [el]).prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hatedge_arr\n",
    "\n",
    "Find links to highly related pages, displayed at the top using one of several Wikitext templates. This is used by RedditRank.ipynb, however it's not feasible to compute it there because opening the enwiki parquet takes too much memory (needed more by other computations in that notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linking the reader to other articles with similar titles or concepts\n",
    "# that they may have been seeking instead\n",
    "HATNOTE_ABOUT = 1\n",
    "# placed at the top of the article or section that is the primary topic\n",
    "# of a redirect, and links to other topics that are ambiguous with the\n",
    "# name of that redirect\n",
    "HATNOTE_REDIRECT = 2\n",
    "# concise about / other uses\n",
    "HATNOTE_FOR = 3\n",
    "# \"not to be confused with\"\n",
    "HATNOTE_DISTINGUISH = 4\n",
    "def parse_hatnote(line):\n",
    "    lst = split_template(line)\n",
    "    if len(lst) == 0:\n",
    "        return\n",
    "    tmpl_toks, *args = lst\n",
    "    tmpl = tokenlist_string(tmpl_toks)\n",
    "    if not tmpl:\n",
    "        return\n",
    "    tmpl = tmpl[0].upper() + tmpl[1:]\n",
    "    if tmpl == 'About':\n",
    "        # the Latin letter|the similar Greek letter|Alpha|the similar Cyrillic letter|A (Cyrillic)|other uses\n",
    "        for dest_ttl in map(tokenlist_gettext, args[2::2]):\n",
    "            yield HATNOTE_ABOUT, dest_ttl\n",
    "    elif tmpl == 'Redirect':\n",
    "        # Achilleus|the Roman usurper with this name|Aurelius Achilleus|other uses|Achilles (disambiguation)\n",
    "        for dest_ttl in map(tokenlist_gettext, args[2::2]):\n",
    "            yield HATNOTE_REDIRECT, dest_ttl\n",
    "    elif tmpl == 'For':\n",
    "        # the racehorse|Ambiorix (horse)\n",
    "        for dest_ttl in map(tokenlist_gettext, args[2::2]):\n",
    "            yield HATNOTE_FOR, dest_ttl\n",
    "    elif tmpl == 'Redirect-synonym':\n",
    "        # Wild cranberry|[[Arctostaphylos uva-ursi]]\n",
    "        for dest_ttl in map(tokenlist_gettext, args[2::2]):\n",
    "            yield HATNOTE_REDIRECT, dest_ttl\n",
    "    elif tmpl == 'About-distinguish':\n",
    "        if len(args) > 1:\n",
    "            yield HATNOTE_DISTINGUISH, tokenlist_gettext(args[1])\n",
    "    elif tmpl == 'Redirect2':\n",
    "        # Anarchist|Anarchists|other uses|Anarchist (disambiguation)\n",
    "        for dest_ttl in map(tokenlist_gettext, args[2::2]):\n",
    "            yield HATNOTE_REDIRECT, dest_ttl\n",
    "    elif tmpl == 'Redirect-multi':\n",
    "        # Redirect-multi|2|Oscars|The Oscar|other uses|Oscar (disambiguation)\n",
    "        try:\n",
    "            skip = int(tokenlist_string(args[1]))\n",
    "        except (TypeError, ValueError):\n",
    "            return\n",
    "        for dest_ttl in map(tokenlist_gettext, args[2::2]):\n",
    "            yield HATNOTE_REDIRECT, dest_ttl\n",
    "    elif tmpl == 'Distinguish' or tmpl == 'Redirect-distinguish-text':\n",
    "        # distinguish|text=[[Lucius Appuleius Saturninus]], a Roman demagogue, or others with the name Apuleius or [[Appuleia (gens)|Appuleius]]\n",
    "        for tl in args:\n",
    "            if (\n",
    "                tmpl == 'Redirect-distinguish-text'\n",
    "                or tokenlist_startswith(tl, 'text=')\n",
    "                or tokenlist_startswith(tl, 'Text=')\n",
    "            ):\n",
    "                for link_inner in tokenlist_links(tl):\n",
    "                    dest = link_inner.split('|', maxsplit=1)[0]\n",
    "                    yield HATNOTE_DISTINGUISH, dest\n",
    "            else:\n",
    "                yield HATNOTE_DISTINGUISH, tokenlist_gettext(tl)\n",
    "    elif tmpl == 'Redirect-distinguish':\n",
    "        # ethyne|ethane|ethene\n",
    "        for dest_ttl in map(tokenlist_gettext, args[1:]):\n",
    "            yield HATNOTE_DISTINGUISH, dest_ttl\n",
    "    elif tmpl == 'Redirect-distinguish-for':\n",
    "        # Phoebus|Phobos (mythology)|other uses|Phoebus (disambiguation)\n",
    "        if len(args) > 1:\n",
    "            yield HATNOTE_DISTINGUISH, tokenlist_gettext(args[1])\n",
    "            for dest_ttl in map(tokenlist_gettext, args[3::2]):\n",
    "                yield HATNOTE_FOR, dest_ttl\n",
    "    elif tmpl == 'About-distinguish':\n",
    "        if len(args) > 1:\n",
    "            yield HATNOTE_DISTINGUISH, tokenlist_gettext(args[1])\n",
    "    elif tmpl == 'About-distinguish-text':\n",
    "        # the sub-group of the Semitic languages native to Mesopotamia and the Levant|[[Amharic]], the Semitic language spoken in [[Ethiopia]]\n",
    "        for tl in args[1:]:\n",
    "            for link_inner in tokenlist_links(tl):\n",
    "                dest = link_inner.split('|', maxsplit=1)[0]\n",
    "                yield HATNOTE_DISTINGUISH, dest\n",
    "    elif tmpl == 'For-text':\n",
    "        for tl in args[1:]:\n",
    "            for link_inner in tokenlist_links(tl):\n",
    "                dest = link_inner.split('|', maxsplit=1)[0]\n",
    "                yield HATNOTE_DISTINGUISH, dest\n",
    "    elif tmpl in ('Other uses', 'Other people', 'About other people', 'Hatnote', 'Technical reasons'):\n",
    "        return\n",
    "    elif tmpl in ('Redirect-several',):\n",
    "        return  # too complicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PARTITION_SIZE = 16\n",
    "PARTITION_SIZE = 1 << LOG_PARTITION_SIZE\n",
    "NUM_PARTITIONS = math.ceil(N / PARTITION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hatedges():\n",
    "    iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "    iterator = map(\n",
    "        lambda b: zip(\n",
    "            b[\"id\"].to_numpy(),\n",
    "            b[\"ns\"].to_numpy(),\n",
    "            map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "            b[\"text\"].to_pylist()\n",
    "        ),\n",
    "        iterator\n",
    "    )\n",
    "    iterator = itertools.chain.from_iterable(iterator)\n",
    "    iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "    iterator = enumerate(map(operator.itemgetter(3), iterator))\n",
    "    filenames = files.hatedge_filenames(NUM_PARTITIONS)\n",
    "    hatcheck_arr = np.zeros(N, dtype=np.bool_)\n",
    "    for part_idx, subitr in enumerate(lazy_chunk(iterator, PARTITION_SIZE)):\n",
    "        edges = PairVec('int32')\n",
    "        for src_id, text in subitr:\n",
    "            for line in wikiplain.get_distinguish_hatnotes(text):\n",
    "                for tag, dest_ttl in parse_hatnote(line):\n",
    "                    dest_ttl = dest_ttl.split('{{!', maxsplit=1)[0]  # for {{!}}\n",
    "                    if dest_ttl == \"\" or '#' in dest_ttl:\n",
    "                        continue\n",
    "                    dest_ttl = dest_ttl[0].upper() + dest_ttl[1:]\n",
    "                    if (dest_id := id_map.get(dest_ttl) or id_map2.get(dest_ttl)) is not None:\n",
    "                        edges.append(src_id, (tag << 28) | dest_id)\n",
    "                        hatcheck_arr[src_id] = True\n",
    "        with open(filenames[part_idx], \"wb\") as fp:\n",
    "            np.save(fp, edges.array[:edges.length])\n",
    "    return hatcheck_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be908393bb6b4690bac9366455ec3486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hatedge_fnames = glob.glob(files.hatedge_filename_pattern)\n",
    "try:\n",
    "    assert set(hatedge_fnames) == set(files.hatedge_filenames(NUM_PARTITIONS))\n",
    "    with open(files.hatcheck_arr_filename, \"rb\") as fp:\n",
    "        hatcheck_arr = np.load(fp)\n",
    "except Exception as exc:\n",
    "    hatcheck_arr = get_hatedges()\n",
    "    with open(files.hatcheck_arr_filename, \"wb\") as fp:\n",
    "        np.save(fp, hatcheck_arr)\n",
    "    hatedge_fnames = glob.glob(files.hatedge_filename_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wiki_link(line):\n",
    "    dest_ttl = line.strip()\n",
    "    if len(dest_ttl) == 0:\n",
    "        return None\n",
    "    dest_ttl = dest_ttl[0].upper() + dest_ttl[1:]\n",
    "    dest_ttl = dest_ttl.split('|', maxsplit=1)[0]\n",
    "    dest_ttl = dest_ttl.split('#', maxsplit=1)[0]\n",
    "    return dest_ttl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_id_map = ChainMap(id_map, id_map2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6806227"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge format\n",
    "\n",
    "- `edges_{n}.npz` stores the outgoing links from `PARITION_SIZE*n ..< PARTITION_SIZE*(n+1)`\n",
    "- These are stored in a list where element `i` contains the links out to `PARITION_SIZE*i ..< PARTITION_SIZE*(i+1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(iterable, size):\n",
    "    \"\"\"Split an iterable into list chunks of size `n`.\n",
    "    \n",
    "    The last chunk can be fewer than `n` elements long, but it won't be empty.\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(iterator, size))\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "        else:\n",
    "            return\n",
    "\n",
    "def lazy_chunk(iterable, n):\n",
    "    \"\"\"Split an iterable into iterable chunks of size `n`.\n",
    "    \n",
    "    The last chunk can be fewer than `n` elements long, but it won't be empty.\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        try:\n",
    "            first = next(iterator)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        yield itertools.chain([first], itertools.islice(iterator, n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ORDER_TAG_BITS = 3\n",
    "NUM_ORDER_TAGS = 1 << ORDER_TAG_BITS\n",
    "DEST_ID_BITS = 31 - ORDER_TAG_BITS\n",
    "DEST_ID_MASK = (1 << DEST_ID_BITS) - 1\n",
    "def get_edges():\n",
    "    in_degree = np.zeros(N, dtype=np.int32)\n",
    "    out_degree = np.zeros(N, dtype=np.int32)\n",
    "    with tqdm(position=1) as progress:\n",
    "        iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "        iterator = map(\n",
    "            lambda b: zip(\n",
    "                b[\"id\"].to_numpy(),\n",
    "                b[\"ns\"].to_numpy(),\n",
    "                map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "                b[\"text\"].to_pylist()\n",
    "            ),\n",
    "            iterator\n",
    "        )\n",
    "        iterator = itertools.chain.from_iterable(iterator)\n",
    "        iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "        iterator = enumerate(map(operator.itemgetter(3), iterator))\n",
    "        filenames = files.edge_filenames(NUM_PARTITIONS)\n",
    "        order_tags = np.arange(NUM_ORDER_TAGS, dtype=np.int32)[::-1] << DEST_ID_BITS\n",
    "        for part_idx, subitr in enumerate(lazy_chunk(iterator, PARTITION_SIZE)):\n",
    "            edges = [PairVec('int32') for _ in range(0, N, PARTITION_SIZE)]\n",
    "            for src_id, text in subitr:\n",
    "                link_idx = 0\n",
    "                for link in wikiplain.get_links(text):\n",
    "                    dest_ttl = parse_wiki_link(link)\n",
    "                    if dest_ttl:\n",
    "                        dest_id = id_map.get(dest_ttl)\n",
    "                        dest_id = dest_id if (dest_id is not None) else id_map2.get(dest_ttl)\n",
    "                        if dest_id is not None:\n",
    "                            partition = dest_id >> LOG_PARTITION_SIZE\n",
    "                            edges[partition].append(src_id, order_tags[link_idx] | dest_id)\n",
    "                            in_degree[dest_id] += 1\n",
    "                            out_degree[src_id] += 1\n",
    "                            progress.update()\n",
    "                            link_idx = min(NUM_ORDER_TAGS - 1, link_idx + 1)\n",
    "            with open(filenames[part_idx], \"wb\") as fp:\n",
    "                np.savez(fp, *([vec.array[:vec.length] for vec in edges]))\n",
    "    return in_degree, out_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f0e90bcf1f4f679517bcd06975c4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31739fd5f45c4530bde88dedb7b09242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edge_fnames = glob.glob(files.edge_filename_pattern)\n",
    "try:\n",
    "    assert set(edge_fnames) == set(files.edge_filenames(NUM_PARTITIONS))\n",
    "    with open(files.in_degree_filename, \"rb\") as fp:\n",
    "        in_degree = np.load(fp)\n",
    "    with open(files.out_degree_filename, \"rb\") as fp:\n",
    "        out_degree = np.load(fp)\n",
    "    # for fname in edge_fnames:\n",
    "    #    with open(fname, \"rb\") as fp:\n",
    "    #         assert len(pickle.load(fp)) == NUM_PARTITIONS\n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "    in_degree, out_degree = get_edges()\n",
    "    edge_fnames = glob.glob(files.edge_filename_pattern)\n",
    "    with open(files.in_degree_filename, \"wb\") as fp:\n",
    "        np.save(fp, in_degree)\n",
    "    with open(files.out_degree_filename, \"wb\") as fp:\n",
    "        np.save(fp, out_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_edges time to complete\n",
    "\n",
    "| date | links/s | time |\n",
    "| :--- | :------ | :--- |\n",
    "| 2024-02-06 | 101000 | 46min 48s\n",
    "| 2024-05-14 | 62500 | 76min 39s\n",
    "| 2024-07-01 | 77000 | 62min 56s\n",
    "\n",
    "2024-05-14: `ORDER_BITS` and `link_idx` logic was added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_dab_array():\n",
    "#     result = np.zeros(N, dtype=np.bool8)\n",
    "#     dab_proc = subprocess.Popen(\n",
    "#         [\"wikiplain\", \"--fraction\", \"1\", \"-c\", \"only-dab\", \"--ns\", \"0\", files.enwiki_database_filename],\n",
    "#         stdout=subprocess.PIPE,\n",
    "#         stderr=subprocess.PIPE\n",
    "#     )\n",
    "#     iterator = make_links_iter(dab_proc.stdout)\n",
    "#     iterator = tqdm(iterator, position=0, total=len(id_map))\n",
    "#     iterator = map(lambda pair: (pair[0].decode(\"utf-8\"), pair[1]), iterator)\n",
    "#     for n, subitr in enumerate(lazy_chunk(iterator, PARTITION_SIZE)):\n",
    "#         for ttl, text in subitr:\n",
    "#             src_id = id_map[ttl]\n",
    "#             if len(text) > 0:\n",
    "#                 result[src_id] = True\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDF = pl.scan_parquet(files.enwiki_parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     with open(files.dab_array_filename, \"rb\") as fp:\n",
    "#         dab_array = pickle.load(fp)\n",
    "# except Exception as exc:\n",
    "#     print(exc)\n",
    "#     dab_array = get_dab_array()\n",
    "#     with open(files.dab_array_filename, \"wb\") as fp:\n",
    "#         pickle.dump(dab_array, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_adjacency_matrix_slice(partition, progress):\n",
    "    \"\"\"Computes the slice of the adjacency matrix A starting at row p*S and ending before row (p+1)*S\n",
    "    \n",
    "    p=partition, S=PARTITION_SIZE, and A is defined so that\n",
    "    A @ np.eye(N)[i] = v, a probability vector where\n",
    "        v[j] = out-degree(i) > 0 | count((i,j) in E) / out-degree(i)\n",
    "               otherwise         | 0\n",
    "    \"\"\"\n",
    "    origin_row = partition * PARTITION_SIZE\n",
    "    n_rows = min(PARTITION_SIZE, N - origin_row)\n",
    "    index_arrs = []\n",
    "    value_arrs = []\n",
    "    pkey = f'arr_{partition}'\n",
    "    for fname in glob.glob(files.edge_filename_pattern):\n",
    "        with np.load(fname) as npz:\n",
    "            vec = npz[pkey]\n",
    "        vec[:, 1] &= DEST_ID_MASK  # remove order_tag\n",
    "        # vec is\n",
    "        #  [[src_id_0, dest_id_0],\n",
    "        #   [src_id_1, dest_id_1],\n",
    "        #   ...\n",
    "        #  ]\n",
    "        # Sort by (src,dest), make unique and get counts\n",
    "        key_arr = (vec[:, 0].astype('int64') << 32) | vec[:, 1]\n",
    "        _, order, count = np.unique(key_arr, return_index=True, return_counts=True)\n",
    "        vec = vec[order]\n",
    "        # Normalize `count` based on (src,)\n",
    "        count = count.astype('float64') / out_degree[vec[:, 0]]\n",
    "        index_arrs.append(vec)\n",
    "        value_arrs.append(count)\n",
    "        progress.update()\n",
    "    index_arr = np.vstack(index_arrs)\n",
    "    matrix_slice = scipy.sparse.csr_array(\n",
    "        (np.hstack(value_arrs), (index_arr[:, 1] - origin_row, index_arr[:, 0])),\n",
    "        shape=(n_rows, N),\n",
    "        dtype=np.float64\n",
    "    )\n",
    "    return matrix_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66d4bd466af4234b67f7e5b9817cc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tqdm(total=NUM_PARTITIONS**2) as progress:\n",
    "    for partition in range(NUM_PARTITIONS):\n",
    "        adj_matrix = compute_adjacency_matrix_slice(partition, progress)\n",
    "        scipy.sparse.save_npz(files.adjacency_filename(partition), adj_matrix, compressed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     5,    19,    85,   412,  1198, 15252])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(out_degree, [0, 0.1, 0.5, 0.9, 0.99, 0.999, 1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_out_degree = np.log(out_degree + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_out_degree /= log_out_degree.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WMf5L5VEqZb"
   },
   "source": [
    "### Global PageRank\n",
    "\n",
    "The initial rank is a column vector $\\mathbf{r}$ = $\\frac{1}{N} \\left( \\mathbf{\\vec{1}} \\right)$\n",
    "\n",
    "The transition matrix $\\mathbf{M}$ is N x N; each column represents a source, and each row represents a destination.\n",
    "$\\mathbf{M}_{ij} = P(\\text{next}=i\\,|\\,\\text{current}=j)$. Each column **must** sum to 1 for the calculation to be stable, so if page $j$ contains no links, it is treated as if it had a link to every page.\n",
    "\n",
    "The power method iteratively computes better ranks: $\\mathbf{r'} = (1 - \\alpha) \\mathbf{M}\\mathbf{r} + \\frac{\\alpha}{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalized PageRank\n",
    "\n",
    "Personalized PageRank uses a preference vector $\\mathbf{p}$ in place of the uniform $\\frac{1}{N}$ for _teleportation_. Pages with no out-links still use a uniform distribution. The initial rank can be any vector, because of the converging property of the power method (explanation at https://mathworld.wolfram.com/Eigenvector.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WMf5L5VEqZb"
   },
   "source": [
    "### Ending iteration\n",
    "\n",
    "At each iteration, we calculate the [perplexity](https://en.wikipedia.org/wiki/Perplexity) of the PageRank distribution, where perplexity is defined as 2 raised to the [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) of the PageRank distribution, i.e., $2^{H(PR)}$. The initial guess is at maximum entropy, so the first iteration causes perplexity to decrease. Later iterations may change perplexity in either direction; we stop when the change is below a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "wsL0yQKvKqAC"
   },
   "outputs": [],
   "source": [
    "def perplexity(distribution):\n",
    "    return np.power(2, np.sum(-distribution * np.log2(distribution)))\n",
    "\n",
    "def personalized_page_rank(preference, threshold=1, random_jump_prob=0.15):\n",
    "    current_rank = np.ones(N, dtype=np.float64) / N\n",
    "    next_rank = np.zeros(N, dtype=np.float64)\n",
    "    # iteratively update current_rank\n",
    "    edge_follow_prob = 1 - random_jump_prob\n",
    "    prev_perplexity = float('inf')\n",
    "    current_perplexity = perplexity(current_rank)\n",
    "    current_iter = 0\n",
    "    iter_start = time.time()\n",
    "    print(\"Itr# | ΔPerplexity     | Seconds\")\n",
    "    while abs(prev_perplexity - current_perplexity) > threshold:\n",
    "        current_iter += 1\n",
    "        next_rank[:] = random_jump_prob * preference\n",
    "        # update destinations from non-sink nodes (N x N times N x 1 -> N x 1)\n",
    "        spread_probs = np.vstack([\n",
    "            adjacency_matrix_slice.dot(current_rank[:, np.newaxis])\n",
    "            for adjacency_matrix_slice in map(scipy.sparse.load_npz, files.adjacency_filenames(NUM_PARTITIONS))\n",
    "        ])\n",
    "        next_rank += edge_follow_prob * spread_probs[:, 0]  # make column vector 1-D\n",
    "        # update destinations from sink nodes\n",
    "        next_rank[:] += edge_follow_prob * current_rank[out_degree == 0].sum() / N\n",
    "        # copy `next_rank` values into `current_rank``\n",
    "        current_rank[:] = next_rank\n",
    "        # --\n",
    "        # compute perplexity and progress\n",
    "        prev_perplexity = current_perplexity\n",
    "        current_perplexity = perplexity(current_rank)\n",
    "        next_iter_start = time.time()\n",
    "        print(\"{:<3d}    {:<15.6f}   {:.3f}\".format(current_iter,\n",
    "                                                    current_perplexity - prev_perplexity,\n",
    "                                                    next_iter_start - iter_start))\n",
    "        iter_start = next_iter_start\n",
    "\n",
    "    title_df = pl.DataFrame({\n",
    "        \"title\": id_map.keys(),\n",
    "        \"node_id\": id_map.values(),\n",
    "    })\n",
    "    df = pl.DataFrame({\n",
    "        \"value\": next_rank,\n",
    "        \"in_deg\": in_degree,\n",
    "        \"out_deg\": out_degree,\n",
    "    })\n",
    "    df = (\n",
    "        df.with_row_count(name=\"node_id\")\n",
    "        .with_columns(pl.col(\"node_id\").cast(pl.Int64))\n",
    "        .join(title_df, on=\"node_id\", how=\"left\")\n",
    "    )\n",
    "    df = df.select(\"title\", \"value\", \"in_deg\", \"out_deg\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "wsL0yQKvKqAC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr# | ΔPerplexity     | Seconds\n",
      "1      -6073391.126198   5.526\n",
      "2      144200.081121     4.078\n",
      "3      -38439.087290     3.701\n",
      "4      -2527.114476      3.601\n",
      "5      -8461.559135      3.540\n",
      "6      -2811.155265      3.381\n",
      "7      -3236.445542      3.533\n",
      "8      -1700.729249      3.487\n",
      "9      -1473.381168      3.463\n",
      "10     -927.252160       3.456\n",
      "11     -725.435128       3.580\n",
      "12     -489.155403       3.431\n",
      "13     -372.249495       3.480\n",
      "14     -257.465879       3.446\n",
      "15     -194.636416       3.491\n",
      "16     -136.994864       3.436\n",
      "17     -103.115731       3.503\n",
      "18     -73.490105        3.509\n",
      "19     -55.399066        3.480\n",
      "20     -39.718448        3.482\n",
      "21     -30.075870        3.494\n",
      "22     -21.660356        3.504\n",
      "23     -16.469271        3.497\n",
      "24     -11.905127        3.467\n",
      "25     -9.095258         3.471\n",
      "26     -6.589565         3.596\n",
      "27     -5.060340         3.470\n",
      "28     -3.672664         3.433\n",
      "29     -2.834366         3.459\n",
      "30     -2.059805         3.455\n",
      "31     -1.597703         3.488\n",
      "32     -1.161960         3.514\n",
      "33     -0.905849         3.469\n"
     ]
    }
   ],
   "source": [
    "# Run until perplexity changes by less than 1\n",
    "PR = personalized_page_rank(log_out_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR.write_parquet(files.pagerank_parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_sorted = PR.sort('value', descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902ac85ce25f4e68b383c6b04fbb60f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='page', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function nbhelpers.polars.pager.<locals>.<lambda>(page)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pager(PR_sorted.slice(0, 2000), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538c683b40584c4cb33b7f758b8444bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='q'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function nbhelpers.polars.searcher.<locals>.searcher_run(q)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher(\n",
    "    PR_sorted.slice(0, 200000).with_columns(pl.Series(\"rank\", range(200000))).select([\"rank\", *PR_sorted.columns]),\n",
    "    ['title'],\n",
    "    20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNw97vMeu3UGKrk6j3TYQO8",
   "include_colab_link": true,
   "name": "PageRank.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
