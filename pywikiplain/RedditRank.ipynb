{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "## Counting mentions of Wikipedia articles in Reddit submission titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "1. Download a dump of Wikipedia's articles, named `enwiki-{date_string}-pages-articles-multistream.xml.bz2`\n",
    "2. Download the `enwiki-{date_string}-pages-articles-multistream-index.txt.bz2` file\n",
    "3. Move those files into the same folder, removing the `enwiki-{date_string}` prefix\n",
    "4. Process the `xml.bz2` file into a Parquet file using `wikiplain.load_bz2`\n",
    "5. Run `PageRank.ipynb`\n",
    "6. Download some of the `RS_{yyyy-mm}.zst` files from https://files.pushshift.io/reddit/submissions/\n",
    "    - I only use 2015-present, and I cut each download off at 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import glob\n",
    "import gzip\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import socket\n",
    "import struct\n",
    "import subprocess\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "from collections import ChainMap, defaultdict, deque\n",
    "from contextlib import asynccontextmanager, contextmanager\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from enum import Enum, auto\n",
    "from functools import lru_cache, partial\n",
    "from urllib.parse import urlencode, urlsplit, quote as urlquote, unquote as urlunquote\n",
    "from xml.sax.saxutils import unescape as xml_unescape\n",
    "from typing import Any, Awaitable, Callable, Literal, TypeVar\n",
    "\n",
    "import httpx\n",
    "import ijson\n",
    "import mmh3\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import polars as pl\n",
    "import sqlalchemy as sa\n",
    "import scipy.sparse\n",
    "import toolz\n",
    "from dotenv import load_dotenv\n",
    "from ipywidgets import interact\n",
    "from spacy.lang.en import English\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import column as sqlcolumn, select, text as sqltext\n",
    "from tqdm.auto import tqdm\n",
    "from zstandard import ZstdDecompressor, ZstdDecompressionReader\n",
    "from arsenal.datastructures.unionfind import UnionFind\n",
    "from arsenal.datastructures.heap import MinMaxHeap\n",
    "\n",
    "import wikiplain\n",
    "from special_cases import SECOND_LEVEL_DOMAINS\n",
    "from nbhelpers.polars import pager, searcher\n",
    "from umbc_web.process_possf2 import PENN_TAGS, PENN_TAGS_BY_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.Config.set_fmt_str_lengths(160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditRankFiles:\n",
    "    def __init__(self, date_string):\n",
    "        self.date_string = date_string\n",
    "        self.enwiki_dir = f\"{os.environ['ENWIKI_DIR']}/{date_string}\"\n",
    "        self.parquet_dir = os.environ.get('ENWIKI_PARQUET_DIR', self.enwiki_dir)\n",
    "        self.reddit_dir = f\"{os.environ['REDDIT_DIR']}\"\n",
    "        try:\n",
    "            os.mkdir(f\"{self.enwiki_dir}/pagerank\")\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "    \n",
    "    @property\n",
    "    def enwiki_parquet_filename(self):\n",
    "        return f\"{self.parquet_dir}/enwiki_{self.date_string}.parquet\"\n",
    "    \n",
    "    @property\n",
    "    def pagerank_parquet_filename(self):\n",
    "        return f\"{self.parquet_dir}/enwiki_{self.date_string}_pagerank.parquet\"\n",
    "    \n",
    "    @property\n",
    "    def enwiki_tokenized_database_uri(self):\n",
    "        return f\"sqlite:///{self.parquet_dir}/enwiki_tokenized_{self.date_string}.sqlite\"\n",
    "\n",
    "    @property\n",
    "    def nub_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/nub.pkl\"\n",
    "    \n",
    "    @property\n",
    "    def id_maps_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/id_maps.pkl\"\n",
    "    \n",
    "    @property\n",
    "    def dense_id_arr_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/dense_id_arr.pkl\"\n",
    "    \n",
    "    @property\n",
    "    def disambig_arr_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/disambig_arr.pkl\"\n",
    "    \n",
    "    @property\n",
    "    def top_cite_domains_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/top_cite_domains.pkl\"\n",
    "    \n",
    "    @property\n",
    "    def in_degree_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/in_degree.pkl\"\n",
    "    \n",
    "    @property\n",
    "    def out_degree_filename(self):\n",
    "        return f\"{self.enwiki_dir}/pagerank/out_degree.pkl\"\n",
    "\n",
    "    def edge_filenames(self, num_partitions):\n",
    "        return [\n",
    "            f\"{self.enwiki_dir}/pagerank/edges_{i}.pkl\"\n",
    "            for i in range(num_partitions)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = RedditRankFiles(\"20230301\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Re-use outputs computed by PageRank.ipynb\n",
    "\n",
    "1. Pages with the same title\n",
    "2. `id_map` from non-redirecting article titles to node number, and `id_map2` from redirecting article titles to node number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pqf = pq.ParquetFile(files.enwiki_parquet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(files.nub_filename, \"rb\") as fp:\n",
    "    overwritten, pqf_size = pickle.load(fp)\n",
    "with open(files.id_maps_filename, \"rb\") as fp:\n",
    "    id_map, id_map2 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4duRjzABB9n"
   },
   "source": [
    "### Build representation of articles/links as a graph\n",
    "\n",
    "1. Create `id_map` from non-redirecting article titles to node number, and `id_map2` from redirecting article titles to node number\n",
    "2. Use `wikiplain` to extract link titles, and use above maps to convert to (src_id, dest_id) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PARTITION_SIZE = 16\n",
    "PARTITION_SIZE = 1 << LOG_PARTITION_SIZE\n",
    "N = len(id_map)\n",
    "NUM_PARTITIONS = math.ceil(N / PARTITION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vec:\n",
    "    def __init__(self, dtype):\n",
    "        self.array = np.ndarray((1024,), dtype=dtype)\n",
    "        self.length = 0\n",
    "    \n",
    "    @property\n",
    "    def capacity(self):\n",
    "        return self.array.shape[0]\n",
    "\n",
    "    def append(self, v):\n",
    "        idx = self.length\n",
    "        if idx >= self.capacity:\n",
    "            addsz = max(2, self.capacity)\n",
    "            self.array = np.hstack((self.array, np.zeros((addsz,), dtype=self.array.dtype)))\n",
    "        self.array[idx] = v\n",
    "        self.length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disambig_arr():\n",
    "    iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "    iterator = map(\n",
    "        lambda b: zip(\n",
    "            b[\"id\"].to_numpy(),\n",
    "            b[\"ns\"].to_numpy(),\n",
    "            map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "            b[\"text\"].to_pylist()\n",
    "        ),\n",
    "        iterator\n",
    "    )\n",
    "    iterator = itertools.chain.from_iterable(iterator)\n",
    "    iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "    iterator = enumerate(map(operator.itemgetter(3), iterator))\n",
    "    disambig_arr = np.zeros(N, dtype=np.bool_)\n",
    "    for node_id, text in iterator:\n",
    "        disambig_arr[node_id] = wikiplain.is_disambiguation_page(text)\n",
    "    return disambig_arr\n",
    "\n",
    "try:\n",
    "    with open(files.disambig_arr_filename, \"rb\") as fp:\n",
    "        disambig_arr = pickle.load(fp)\n",
    "except Exception:\n",
    "    disambig_arr = get_disambig_arr()\n",
    "    with open(files.disambig_arr_filename, \"wb\") as fp:\n",
    "        pickle.dump(disambig_arr, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cython: language_level=3, boundscheck=False, infer_types=True, nonecheck=False\r\n",
      "# cython: overflowcheck=False, initializedcheck=False, wraparound=False, cdivision=True\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "Heap data structures with optional\r\n",
      " - Locators\r\n",
      " - Top-k (Bounded heap)\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "Vt = np.double\r\n",
      "cdef double NaN = np.nan\r\n",
      "\r\n",
      "# TODO: Use the C++ standard library's implementation of a vector of doubles.\r\n",
      "cdef class Vector:\r\n",
      "\r\n",
      "    cdef public:\r\n",
      "        int cap\r\n",
      "        int end\r\n",
      "        double[:] val\r\n",
      "\r\n",
      "    def __init__(self, cap):\r\n",
      "        self.cap = cap\r\n",
      "        self.val = np.zeros(self.cap, dtype=Vt)\r\n",
      "        self.end = 0\r\n",
      "\r\n",
      "    cpdef int push(self, double x):\r\n",
      "        i = self.end\r\n",
      "        self.ensure_size(i)\r\n",
      "        self.val[i] = x\r\n",
      "        self.end += 1\r\n",
      "        return i\r\n",
      "\r\n",
      "    cpdef object pop(self):\r\n",
      "        \"pop from the end\"\r\n",
      "        assert 0 < self.end\r\n",
      "        self.end -= 1\r\n",
      "        v = self.val[self.end]\r\n",
      "        self.val[self.end] = NaN\r\n",
      "        return v\r\n",
      "\r\n",
      "    cdef void grow(self):\r\n",
      "        self.cap *= 2\r\n",
      "        new = np.empty(self.cap, dtype=Vt)\r\n",
      "        new[:self.end] = self.val[:self.end]\r\n",
      "        self.val = new\r\n",
      "\r\n",
      "    cdef void ensure_size(self, int i):\r\n",
      "        \"grow in needed\"\r\n",
      "        if self.val.shape[0] < i + 1: self.grow()\r\n",
      "\r\n",
      "    def __getitem__(self, int i):\r\n",
      "        assert i < self.end\r\n",
      "        return self.get(i)\r\n",
      "\r\n",
      "    cdef double get(self, int i):\r\n",
      "        return self.val[i]\r\n",
      "\r\n",
      "    def __setitem__(self, int i, double v):\r\n",
      "        assert i < self.end\r\n",
      "        self.set(i, v)\r\n",
      "\r\n",
      "    cdef void set(self, int i, double v):\r\n",
      "        self.val[i] = v\r\n",
      "\r\n",
      "    def __len__(self):\r\n",
      "        return self.end\r\n",
      "\r\n",
      "    def __repr__(self):\r\n",
      "        return repr(self.val[:self.end])\r\n",
      "\r\n",
      "\r\n",
      "cdef class MaxHeap:\r\n",
      "\r\n",
      "    cdef public:\r\n",
      "        Vector val\r\n",
      "\r\n",
      "    def __init__(self, cap=2**8):\r\n",
      "        self.val = Vector(cap)\r\n",
      "        self.val.push(np.nan)\r\n",
      "\r\n",
      "    def __len__(self):\r\n",
      "        return len(self.val) - 1   # subtract one for dummy root element\r\n",
      "\r\n",
      "    def pop(self):\r\n",
      "        v = self.peek()\r\n",
      "        self._remove(1)\r\n",
      "        return v\r\n",
      "\r\n",
      "    def peek(self):\r\n",
      "        return self.val.val[1]\r\n",
      "\r\n",
      "    def push(self, v):\r\n",
      "        # put new element last and bubble up\r\n",
      "        return self.up(self.val.push(v))\r\n",
      "\r\n",
      "    cdef void swap(self, int i, int j):\r\n",
      "        assert i < self.val.end\r\n",
      "        assert j < self.val.end\r\n",
      "        self.val.val[i], self.val.val[j] = self.val.val[j], self.val.val[i]\r\n",
      "\r\n",
      "    cdef int up(self, int i):\r\n",
      "        while 1 < i:\r\n",
      "            p = i // 2\r\n",
      "            if self.val.val[p] < self.val.val[i]:\r\n",
      "                self.swap(i, p)\r\n",
      "                i = p\r\n",
      "            else:\r\n",
      "                break\r\n",
      "        return i\r\n",
      "\r\n",
      "    cdef int down(self, int i):\r\n",
      "        n = self.val.end\r\n",
      "        while 2*i < n:\r\n",
      "            a = 2 * i\r\n",
      "            b = 2 * i + 1\r\n",
      "            c = i\r\n",
      "            if self.val.val[c] < self.val.val[a]:\r\n",
      "                c = a\r\n",
      "            if b < n and self.val.val[c] < self.val.val[b]:\r\n",
      "                c = b\r\n",
      "            if c == i:\r\n",
      "                break\r\n",
      "            self.swap(i, c)\r\n",
      "            i = c\r\n",
      "        return i\r\n",
      "\r\n",
      "    def _update(self, int i, double old, double new):\r\n",
      "        assert i < self.val.end\r\n",
      "        if old == new: return i   # value unchanged\r\n",
      "        self.val.val[i] = new         # perform change\r\n",
      "        if old < new:             # increased\r\n",
      "            return self.up(i)\r\n",
      "        else:                     # decreased\r\n",
      "            return self.down(i)\r\n",
      "\r\n",
      "    def _remove(self, int i):\r\n",
      "        # update the locator stuff for last -> i\r\n",
      "        last = self.val.end - 1\r\n",
      "        self.swap(i, last)\r\n",
      "        old = self.val.pop()\r\n",
      "        # special handling for when the heap has size one.\r\n",
      "        if i == last: return\r\n",
      "        self._update(i, old, self.val.val[i])\r\n",
      "\r\n",
      "    def check(self):\r\n",
      "        # heap property\r\n",
      "        for i in range(2, self.val.end):\r\n",
      "            assert self.val[i] <= self.val[i // 2], (self.val[i // 2], self.val[i])   # child <= parent\r\n",
      "\r\n",
      "\r\n",
      "cdef class LocatorMaxHeap(MaxHeap):\r\n",
      "    \"\"\"\r\n",
      "    Dynamic heap. Maintains max of a map, via incrementally maintained partial\r\n",
      "    aggregation tree. Also known a priority queue with 'locators'.\r\n",
      "\r\n",
      "    This data structure efficiently maintains maximum of the priorities of a set\r\n",
      "    of keys. Priorites may increase or decrease. (Many max-heap implementations\r\n",
      "    only allow increasing priority.)\r\n",
      "\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    cdef public:\r\n",
      "        dict key\r\n",
      "        dict loc\r\n",
      "\r\n",
      "    def __init__(self, **kw):\r\n",
      "        super().__init__(**kw)\r\n",
      "        self.key = {}   # map from index `i` to `key`\r\n",
      "        self.loc = {}   # map from `key` to index in `val`\r\n",
      "\r\n",
      "    def __repr__(self):\r\n",
      "        return repr({k: self[k] for k in self.loc})\r\n",
      "\r\n",
      "    def pop(self):\r\n",
      "        k,v = self.peek()\r\n",
      "        super().pop()\r\n",
      "        return k,v\r\n",
      "\r\n",
      "    def popitem(self):\r\n",
      "        return self.pop()\r\n",
      "\r\n",
      "    def peek(self):\r\n",
      "        return self.key[1], super().peek()\r\n",
      "\r\n",
      "    def _remove(self, int i):\r\n",
      "        # update the locator stuff for last -> i\r\n",
      "        last = self.val.end - 1\r\n",
      "        self.swap(i, last)\r\n",
      "        old = self.val.pop()\r\n",
      "        # remove the key/loc/val associated with the deleted node.\r\n",
      "        self.loc.pop(self.key.pop(last))\r\n",
      "        # special handling for when the heap has size one.\r\n",
      "        if i == last: return\r\n",
      "        self._update(i, old, self.val.val[i])\r\n",
      "\r\n",
      "    def __delitem__(self, k):\r\n",
      "        self._remove(self.loc[k])\r\n",
      "\r\n",
      "    def __contains__(self, k):\r\n",
      "        return k in self.loc\r\n",
      "\r\n",
      "    def __getitem__(self, k):\r\n",
      "        return self.val.val[self.loc[k]]\r\n",
      "\r\n",
      "    def __setitem__(self, k, v):\r\n",
      "        \"upsert (update or insert) value associated with key.\"\r\n",
      "        cdef int i\r\n",
      "        if k in self:\r\n",
      "            # update\r\n",
      "            i = self.loc[k]\r\n",
      "            super()._update(i, self.val[i], v)\r\n",
      "        else:\r\n",
      "            # insert (put new element last and bubble up)\r\n",
      "            i = self.val.push(v)\r\n",
      "            # Annoyingly, we have to write key/loc here the super class's push\r\n",
      "            # method doesn't allow us to intervene before the up call.\r\n",
      "            self.val[i] = v\r\n",
      "            self.loc[k] = i\r\n",
      "            self.key[i] = k\r\n",
      "            # fix invariants\r\n",
      "            self.up(i)\r\n",
      "\r\n",
      "    cdef void swap(self, int i, int j):\r\n",
      "        assert i < self.val.end\r\n",
      "        assert j < self.val.end\r\n",
      "        self.val.val[i], self.val.val[j] = self.val.val[j], self.val.val[i]\r\n",
      "\r\n",
      "        self.key[i], self.key[j] = self.key[j], self.key[i]\r\n",
      "        self.loc[self.key[i]] = i\r\n",
      "        self.loc[self.key[j]] = j\r\n",
      "\r\n",
      "    def check(self):\r\n",
      "        super().check()\r\n",
      "        for key in self.loc:\r\n",
      "            assert self.key[self.loc[key]] == key\r\n",
      "        for i in range(1, self.val.end):\r\n",
      "            assert self.loc[self.key[i]] == i\r\n",
      "\r\n",
      "\r\n",
      "class MinMaxHeap:\r\n",
      "\r\n",
      "    def __init__(self, **kw):\r\n",
      "        self.max = LocatorMaxHeap(**kw)\r\n",
      "        self.min = LocatorMaxHeap(**kw)   # will pass negative values here\r\n",
      "\r\n",
      "    def __contains__(self, k):\r\n",
      "        return k in self.max\r\n",
      "\r\n",
      "    def __setitem__(self, k, v):\r\n",
      "        self.max[k] = v\r\n",
      "        self.min[k] = -v\r\n",
      "\r\n",
      "    def peekmin(self):\r\n",
      "        k, v = self.min.peek()\r\n",
      "        return k, -v\r\n",
      "\r\n",
      "    def peekmax(self):\r\n",
      "        return self.max.peek()\r\n",
      "\r\n",
      "    def popmax(self):\r\n",
      "        k, v = self.max.pop()\r\n",
      "        self.min._remove(self.min.loc[k])  # remove it from the min heap\r\n",
      "        return k, v\r\n",
      "\r\n",
      "    def popmin(self):\r\n",
      "        k, v = self.min.pop()\r\n",
      "        self.max._remove(self.max.loc[k])  # remove it from the min heap\r\n",
      "        return k, -v\r\n",
      "\r\n",
      "    def check(self):\r\n",
      "        self.min.check()\r\n",
      "        self.max.check()\r\n",
      "\r\n",
      "    def __len__(self):\r\n",
      "        return len(self.max)\r\n",
      "\r\n",
      "    def __repr__(self):\r\n",
      "        return repr(self.max)\r\n",
      "\r\n",
      "    def map(self):\r\n",
      "        return {k: self.max[k] for k in self.max.loc}\r\n",
      "\r\n",
      "\r\n",
      "class BoundedMaxHeap(MinMaxHeap):\r\n",
      "\r\n",
      "    def __init__(self, maxsize, **kw):\r\n",
      "        super().__init__(**kw)\r\n",
      "        self.maxsize = maxsize\r\n",
      "\r\n",
      "    def __setitem__(self, k, v):\r\n",
      "        super().__setitem__(k, v)\r\n",
      "        if len(self) > self.maxsize:\r\n",
      "            if v < self.peekmin()[1]:  # smaller than the smallest element.\r\n",
      "                return\r\n",
      "            else:\r\n",
      "                self.popmin()   # evict the smallest element\r\n",
      "\r\n",
      "    def pop(self):\r\n",
      "        return self.popmax()\r\n",
      "\r\n",
      "    def check(self):\r\n",
      "        super().check()\r\n",
      "        assert len(self.max) <= self.maxsize\r\n",
      "        assert len(self.min) <= self.maxsize\r\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/timvieira/arsenal/master/arsenal/datastructures/heap/heap.pyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_cite_domains():\n",
    "    iterator = tqdm(pqf.iter_batches(batch_size=100), total=math.ceil(pqf_size / 100))\n",
    "    iterator = map(\n",
    "        lambda b: zip(\n",
    "            b[\"id\"].to_numpy(),\n",
    "            b[\"ns\"].to_numpy(),\n",
    "            map(operator.attrgetter(\"is_valid\"), b[\"redirect\"]),\n",
    "            b[\"text\"].to_pylist()\n",
    "        ),\n",
    "        iterator\n",
    "    )\n",
    "    iterator = itertools.chain.from_iterable(iterator)\n",
    "    iterator = filter(lambda e: not e[2] and e[1] == 0 and e[0] not in overwritten, iterator)\n",
    "    iterator = enumerate(map(operator.itemgetter(3), iterator))\n",
    "    heap = MinMaxHeap()\n",
    "    heap_limit = 256 * 1024\n",
    "    for node_id, text in iterator:\n",
    "        if (node_id + 1) % 750000 == 0:\n",
    "            heap_limit /= 2\n",
    "            while len(heap) > heap_limit:\n",
    "                heap.popmin()\n",
    "        page = defaultdict(int)\n",
    "        for url in wikiplain.get_cite_urls(text):\n",
    "            full_domain = re.sub(r\"[:/].*\", \"\", url)\n",
    "            parts = full_domain.split('.')\n",
    "            if len(parts) >= 2:\n",
    "                site_domain = parts[-2] + '.' + parts[-1]\n",
    "                if site_domain in SECOND_LEVEL_DOMAINS:\n",
    "                    if len(parts) >= 3:\n",
    "                        site_domain = parts[-3] + '.' + site_domain\n",
    "                    else:\n",
    "                        continue\n",
    "                page[site_domain] += 1\n",
    "        for k, v in page.items():\n",
    "            if k in heap:\n",
    "                heap[k] = heap.max[k] + v\n",
    "            elif len(heap) < heap_limit:\n",
    "                heap[k] = v\n",
    "            elif v > heap.peekmin()[1]:\n",
    "                heap.popmin()\n",
    "                heap[k] = v\n",
    "    top_cite_domains = []\n",
    "    while len(heap) > 0:\n",
    "        top_cite_domains.append(heap.popmax())\n",
    "    return top_cite_domains\n",
    "\n",
    "try:\n",
    "    with open(files.top_cite_domains_filename, \"rb\") as fp:\n",
    "        top_cite_domains = pickle.load(fp)\n",
    "except Exception:\n",
    "    top_cite_domains = get_top_cite_domains()\n",
    "    with open(files.top_cite_domains_filename, \"wb\") as fp:\n",
    "        pickle.dump(top_cite_domains, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cf555595ff4377a3ace6eb759578b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='page', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function nbhelpers.polars.pager.<locals>.<lambda>(page)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pager(pl.DataFrame(top_cite_domains, schema=['domain', 'count']), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da4dd5a072a4e708e1323967aa884ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='q'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function nbhelpers.polars.searcher.<locals>.searcher_run(q)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher(pl.DataFrame(top_cite_domains, schema=['domain', 'count'])\n",
    "          .with_columns(pl.Series(\"rank\", range(len(top_cite_domains)))),\n",
    "         [\"domain\"],\n",
    "         16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_domains = {\"imgur.com\", \"twitter.com\", \"youtube.com\", \"soundcloud.com\",\n",
    "                    \"instagram.com\", \"amazon.com\", \"github.com\", \"vimeo.com\",\n",
    "                    \"google.com\"\n",
    "                   }\n",
    "top_cite_domain_set = {domain for domain, _ in top_cite_domains} - excluded_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (50, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>size</th><th>name</th></tr><tr><td>i64</td><td>i64</td><td>list[str]</td></tr></thead><tbody><tr><td>139903491947072</td><td>335544408</td><td>[&quot;id_map2&quot;]</td></tr><tr><td>139903491968640</td><td>335544408</td><td>[&quot;id_map&quot;]</td></tr><tr><td>139899293787584</td><td>2378992</td><td>[&quot;batch&quot;]</td></tr><tr><td>139899299262336</td><td>65752</td><td>[&quot;top_cite_domain_set&quot;]</td></tr><tr><td>139903498434176</td><td>65752</td><td>[&quot;SECOND_LEVEL_DOMAINS&quot;]</td></tr><tr><td>139907559281856</td><td>9304</td><td>[&quot;top_cite_domains&quot;]</td></tr><tr><td>94605535217072</td><td>2043</td><td>[&quot;_i13&quot;]</td></tr><tr><td>94602671107920</td><td>1856</td><td>[&quot;_i4&quot;]</td></tr><tr><td>94602571469328</td><td>1383</td><td>[&quot;_i1&quot;]</td></tr><tr><td>94602560147520</td><td>1072</td><td>[&quot;auto&quot;]</td></tr><tr><td>94602671077664</td><td>1072</td><td>[&quot;MinMaxHeap&quot;]</td></tr><tr><td>94602583240208</td><td>1072</td><td>[&quot;___&quot;, &quot;_3&quot;]</td></tr><tr><td>...</td><td>...</td><td>...</td></tr><tr><td>139903495148912</td><td>150</td><td>[&quot;_i11&quot;]</td></tr><tr><td>139907624684992</td><td>144</td><td>[&quot;urlquote&quot;]</td></tr><tr><td>139906137611216</td><td>144</td><td>[&quot;sqlcolumn&quot;]</td></tr><tr><td>139899293815792</td><td>144</td><td>[&quot;get_top_cite_domains&quot;]</td></tr><tr><td>139903491768384</td><td>144</td><td>[&quot;searcher&quot;]</td></tr><tr><td>139899293815504</td><td>144</td><td>[&quot;ex_batch&quot;]</td></tr><tr><td>139907624684416</td><td>144</td><td>[&quot;urlunquote&quot;]</td></tr><tr><td>139903491522368</td><td>144</td><td>[&quot;pager&quot;]</td></tr><tr><td>139906137613232</td><td>144</td><td>[&quot;sqltext&quot;]</td></tr><tr><td>139906137615248</td><td>144</td><td>[&quot;select&quot;]</td></tr><tr><td>139903491773424</td><td>144</td><td>[&quot;__&quot;, &quot;_14&quot;]</td></tr><tr><td>139906138214256</td><td>144</td><td>[&quot;create_engine&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (50, 3)\n",
       "┌─────────────────┬───────────┬─────────────────────────┐\n",
       "│ id              ┆ size      ┆ name                    │\n",
       "│ ---             ┆ ---       ┆ ---                     │\n",
       "│ i64             ┆ i64       ┆ list[str]               │\n",
       "╞═════════════════╪═══════════╪═════════════════════════╡\n",
       "│ 139903491947072 ┆ 335544408 ┆ [\"id_map2\"]             │\n",
       "│ 139903491968640 ┆ 335544408 ┆ [\"id_map\"]              │\n",
       "│ 139899293787584 ┆ 2378992   ┆ [\"batch\"]               │\n",
       "│ 139899299262336 ┆ 65752     ┆ [\"top_cite_domain_set\"] │\n",
       "│ ...             ┆ ...       ┆ ...                     │\n",
       "│ 139906137613232 ┆ 144       ┆ [\"sqltext\"]             │\n",
       "│ 139906137615248 ┆ 144       ┆ [\"select\"]              │\n",
       "│ 139903491773424 ┆ 144       ┆ [\"__\", \"_14\"]           │\n",
       "│ 139906138214256 ┆ 144       ┆ [\"create_engine\"]       │\n",
       "└─────────────────┴───────────┴─────────────────────────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localsizes = (pl.DataFrame([(id(value), name, sys.getsizeof(value)) for name, value in locals().items()],\n",
    "                          schema=['id', 'name', 'size'])\n",
    "              .groupby('id')\n",
    "              .agg(pl.max(\"size\"), pl.col(\"name\").apply(lambda ser: ser.to_list()))\n",
    "              .sort('size', descending=True)\n",
    "              .head(50)\n",
    "             )\n",
    "localsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pqf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1)\n",
    "def load_edges(partition):\n",
    "    with open(files.edge_filenames(NUM_PARTITIONS)[partition], \"rb\") as fp:\n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge format (copied from PageRank)\n",
    "\n",
    "- `edges_{n}.pkl` stores the outgoing links from `PARITION_SIZE*n ..< PARTITION_SIZE*(n+1)`\n",
    "- These are stored in a list where element `i` contains the links out to `PARITION_SIZE*i ..< PARTITION_SIZE*(i+1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR = pl.read_parquet(files.pagerank_parquet_filename)\n",
    "PR_value = PR[\"value\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(files.enwiki_tokenized_database_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef086cd488a44192b792939a10a38804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6625358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14639d4201e84d4fabc3f8b956aa20b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10408919 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_span_map = {}\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(sqltext(\"CREATE TABLE term_map (term TEXT NOT NULL, id INTEGER NOT NULL, weight FLOAT NOT NULL)\"))\n",
    "    stmt = sqltext(\"INSERT INTO term_map (term, id, weight) VALUES (:term, :id, :weight)\")\n",
    "    for title, node_id in tqdm(id_map.items(), total=len(id_map)):\n",
    "        parts = [token.norm_ for token in nlp.tokenizer(title) if not (token.is_left_punct or token.is_right_punct)]\n",
    "        if len(parts) == 0:\n",
    "            continue\n",
    "        if len(parts) > max_span_map.get(parts[0], -1):\n",
    "            max_span_map[parts[0]] = len(parts)\n",
    "        term = \" \".join(parts)\n",
    "        if disambig_arr[node_id]:\n",
    "            partition = node_id >> LOG_PARTITION_SIZE\n",
    "            destinations = []\n",
    "            for pair_tbl in load_edges(partition):\n",
    "                destinations.append(pair_tbl[pair_tbl[:, 0] == node_id][:, 1])\n",
    "            destination_arr = np.unique(np.hstack(destinations))\n",
    "            pr_value_total = PR_value[destination_arr].sum()\n",
    "            for dest_id in destination_arr.tolist():\n",
    "                conn.execute(stmt, {\"term\": term, \"id\": dest_id, \"weight\": PR_value[dest_id] / pr_value_total})\n",
    "        else:\n",
    "            term = \" \".join(token.norm_ for token in nlp.tokenizer(title))\n",
    "            conn.execute(stmt, {\"term\": term, \"id\": node_id, \"weight\": 2.0})\n",
    "    for title, node_id in tqdm(id_map2.items(), total=len(id_map2)):\n",
    "        if disambig_arr[node_id]:\n",
    "            continue\n",
    "        parts = [token.norm_ for token in nlp.tokenizer(title) if not (token.is_left_punct or token.is_right_punct)]\n",
    "        if len(parts) == 0:\n",
    "            continue\n",
    "        if len(parts) > max_span_map.get(parts[0], -1):\n",
    "            max_span_map[parts[0]] = len(parts)\n",
    "        term = \" \".join(parts)\n",
    "        conn.execute(stmt, {\"term\": term, \"id\": node_id, \"weight\": 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as conn:\n",
    "    conn.execute(sqltext(\"CREATE TABLE term_map_2 AS SELECT term, id, MAX(weight) AS weight FROM term_map GROUP BY term, id\"))\n",
    "    conn.execute(sqltext(\"DROP TABLE term_map\"))\n",
    "    conn.execute(sqltext(\"ALTER TABLE term_map_2 RENAME TO term_map\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as conn:\n",
    "    conn.execute(sqltext(\"CREATE INDEX ix_term_map_term ON term_map (term)\"))\n",
    "    conn.execute(sqltext(\"CREATE TABLE max_span_map (k TEXT NOT NULL, v INTEGER NOT NULL)\"))\n",
    "    stmt = sqltext(\"INSERT INTO max_span_map (k, v) VALUES (:k, :v)\")\n",
    "    for k, v in max_span_map.items():\n",
    "        conn.execute(stmt, {\"k\": k, \"v\": v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_map_sql = sa.Table(\n",
    "    \"term_map\",\n",
    "    sa.MetaData(),\n",
    "    sa.Column(\"term\", sa.String),\n",
    "    sa.Column(\"id\", sa.Integer),\n",
    "    sa.Column(\"weight\", sa.Float),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "renormalizations = {'|': '-'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def ensure_shutdown(sock):\n",
    "    try:\n",
    "        yield sock\n",
    "    finally:\n",
    "        sock.shutdown(socket.SHUT_RDWR)\n",
    "        sock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def submission_titles(filename):\n",
    "#     statinfo = os.stat(filename)\n",
    "#     with tqdm.wrapattr(open(filename, \"rb\"), \"read\", total=statinfo.st_size) as compressed:\n",
    "#         dctx = ZstdDecompressor()\n",
    "#         reader = dctx.stream_reader(compressed)\n",
    "#         dedup_q = deque()\n",
    "#         dedup_set = set()\n",
    "#         for submission in ijson.items(reader, \"\", multiple_values=True):\n",
    "#             if \"url\" not in submission or \"title\" not in submission:\n",
    "#                 continue\n",
    "#             url = urlsplit(submission[\"url\"])\n",
    "#             if url.netloc not in top_cite_domain_set:\n",
    "#                 continue\n",
    "#             dedup_key = (url.netloc, url.path)\n",
    "#             if dedup_key in dedup_set:\n",
    "#                 continue\n",
    "#             if len(dedup_q) > 1000:\n",
    "#                 dedup_set.discard(dedup_q.popleft())\n",
    "#             dedup_q.append(dedup_key)\n",
    "#             dedup_set.add(dedup_key)\n",
    "#             title = xml_unescape(submission[\"title\"])\n",
    "#             tokens = [\n",
    "#                 renormalizations.get(token.norm_, token.norm_)\n",
    "#                 for token in nlp.tokenizer(title)\n",
    "#                 if not (token.is_left_punct or token.is_right_punct)\n",
    "#             ]\n",
    "#             spans = []\n",
    "#             for i, w0 in enumerate(tokens):\n",
    "#                 max_size = min(len(tokens) - i, max_span_map.get(w0, 0))\n",
    "#                 for j in range(i + 1, i + max_size + 1):\n",
    "#                     spans.append((i, j))\n",
    "#             yield tokens, spans            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from types import TracebackType\n",
    "from typing import IO, Type, Iterator, Iterable, Optional, List\n",
    "\n",
    "class ReadableIterator(IO[bytes]):\n",
    "    __inner: Optional[Iterator[bytes]]\n",
    "    __buffered: bytes\n",
    "\n",
    "    def __init__(self, inner: Iterator[bytes]):\n",
    "        self.__inner = inner\n",
    "        self.__buffered = b\"\"\n",
    "\n",
    "    def __enter__(self) -> IO[bytes]:\n",
    "        return self\n",
    "\n",
    "    def __exit__(self,\n",
    "                 __t: Optional[Type[BaseException]],\n",
    "                 __value: Optional[BaseException],\n",
    "                 __traceback: Optional[TracebackType]) -> None:\n",
    "        self.close()\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"\n",
    "        Close the IO object.\n",
    "\n",
    "        Attempting any further operation after the object is closed will raise an OSError. This method has no\n",
    "        effect if the file is already closed.\n",
    "        \"\"\"\n",
    "        self.__inner = None\n",
    "\n",
    "    def fileno(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the underlying file descriptor (an integer).\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def readable(self) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if the IO object can be read.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def __require_inner(self) -> Iterator[bytes]:\n",
    "        if self.__inner is None:\n",
    "            raise OSError(\"Can't read a closed file\")\n",
    "        return self.__inner\n",
    "    \n",
    "    def read(self, size: int = -1) -> bytes:\n",
    "        \"\"\"\n",
    "        Read at most size bytes, returned as a bytes object.\n",
    "\n",
    "        If the size argument is negative, read until EOF is reached.\n",
    "        Return an empty bytes object at EOF.\n",
    "        \"\"\"\n",
    "        if size == 0:\n",
    "            return b\"\"\n",
    "        result = self.__buffered\n",
    "        while size < 0 or len(result) < size:\n",
    "            try:\n",
    "                result += next(self.__require_inner())\n",
    "            except StopIteration:\n",
    "                break\n",
    "        if size > 0:\n",
    "            self.__buffered = result[size:]\n",
    "            return result[:size]\n",
    "        self.__buffered = b\"\"\n",
    "        return result\n",
    "\n",
    "    def readinto(self, buffer: bytes) -> int:\n",
    "        \"\"\"\n",
    "        Read bytes into buffer.\n",
    "\n",
    "        Returns number of bytes read (0 for EOF), or None if the object\n",
    "        is set not to block and has no data to read.\n",
    "        \"\"\"\n",
    "        content = self.read(len(buffer))\n",
    "        buffer[:len(content)] = content\n",
    "        return len(content)\n",
    "\n",
    "    def readline(self, __limit: int = -1) -> bytes:\n",
    "        raise ValueError(\"Line-based methods are not available on ReadableIterator\")\n",
    "\n",
    "    def readlines(self, __hint: int = -1) -> List[bytes]:\n",
    "        raise ValueError(\"Line-based methods are not available on ReadableIterator\")\n",
    "\n",
    "    def seekable(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    def seek(self, __offset: int, __whence: int = io.SEEK_CUR) -> int:\n",
    "        raise ValueError(\"Cannot seek\")\n",
    "\n",
    "    def tell(self) -> int:\n",
    "        raise ValueError(\"Cannot tell\")\n",
    "\n",
    "    def writable(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    def flush(self) -> None:\n",
    "        raise ValueError(\"Cannot write\")\n",
    "\n",
    "    def truncate(self, __size: Optional[int] = None) -> int:\n",
    "        raise ValueError(\"Cannot write\")\n",
    "\n",
    "    def write(self, __s: bytes) -> int:\n",
    "        raise ValueError(\"Cannot write\")\n",
    "\n",
    "    def writelines(self, __lines: Iterable[bytes]) -> None:\n",
    "        raise ValueError(\"Cannot write\")\n",
    "\n",
    "    def isatty(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    def __next__(self) -> bytes:\n",
    "        raise ValueError(\"Iterable methods are not available on ReadableStreamWrapper\")\n",
    "\n",
    "    def __iter__(self) -> Iterator[bytes]:\n",
    "        raise ValueError(\"Iterable methods are not available on ReadableStreamWrapper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_titles(pushshift_url):\n",
    "    with httpx.stream(\"GET\", pushshift_url) as response:\n",
    "        fh = ReadableIterator(response.iter_bytes())\n",
    "        total_s = response.headers.get(\"content-length\")\n",
    "        total = None\n",
    "        if total_s is not None:\n",
    "            total = int(total_s)\n",
    "        with tqdm.wrapattr(fh, \"read\", total=total) as compressed:\n",
    "            dctx = ZstdDecompressor()\n",
    "            reader = dctx.stream_reader(compressed)\n",
    "            dedup_q = deque()\n",
    "            dedup_set = set()\n",
    "            for submission in ijson.items(reader, \"\", multiple_values=True):\n",
    "                if \"url\" not in submission or \"title\" not in submission:\n",
    "                    continue\n",
    "                url = urlsplit(submission[\"url\"])\n",
    "                if url.netloc not in top_cite_domain_set:\n",
    "                    continue\n",
    "                dedup_key = (url.netloc, url.path)\n",
    "                if dedup_key in dedup_set:\n",
    "                    continue\n",
    "                if len(dedup_q) > 1000:\n",
    "                    dedup_set.discard(dedup_q.popleft())\n",
    "                dedup_q.append(dedup_key)\n",
    "                dedup_set.add(dedup_key)\n",
    "                title = xml_unescape(submission[\"title\"])\n",
    "                tokens = [\n",
    "                    renormalizations.get(token.norm_, token.norm_)\n",
    "                    for token in nlp.tokenizer(title)\n",
    "                    if not (token.is_left_punct or token.is_right_punct)\n",
    "                ]\n",
    "                spans = []\n",
    "                for i, w0 in enumerate(tokens):\n",
    "                    max_size = min(len(tokens) - i, max_span_map.get(w0, 0))\n",
    "                    for j in range(i + 1, i + max_size + 1):\n",
    "                        spans.append((i, j))\n",
    "                yield tokens, spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('127.0.0.1', 59310)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1473d6a6b734bc29143d711c9bed94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12445460428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relevance = np.zeros(N, dtype=np.float64)\n",
    "with (engine.connect() as conn,\n",
    "      ensure_shutdown(socket.socket()) as sock,\n",
    "):\n",
    "    sock.connect((\"127.0.0.1\", 31323))\n",
    "    print(sock.getsockname())\n",
    "    rfile = sock.makefile(\"r\", encoding=\"utf-8\")\n",
    "    wfile = sock.makefile(\"wb\", buffering=0)\n",
    "    for group in toolz.partition_all(50, submission_titles(\"https://files.pushshift.io/reddit/submissions/RS_2023-01.zst\")):\n",
    "        sentences = [json.dumps(l).encode(\"utf-8\") + b\"\\n\" for l, _ in group]\n",
    "        req = b\"\".join(sentences)\n",
    "        wfile.write(struct.pack(\">2I\", len(req), 0) + req)\n",
    "        queries = []\n",
    "        query_rev = defaultdict(list)\n",
    "        for sentence, spans in group:\n",
    "            resp = json.loads(rfile.readline())\n",
    "            scores = np.array([e[\"scores\"] for e in resp])\n",
    "            observations = np.array([e[\"observations\"] for e in resp])\n",
    "            priors = np.array([int(w.isalpha()) for w in sentence])\n",
    "            scores -= scores.max(axis=1)[:, None]\n",
    "            scores *= 0.5\n",
    "            np.exp(scores, out=scores)\n",
    "            scores /= np.sum(scores, axis=1)[:, None]\n",
    "            score = np.max(scores[:, [PENN_TAGS[\"NNP\"], PENN_TAGS[\"NNPS\"]]], axis=1)\n",
    "            score = (score * observations + priors * 20) / (observations + 20)\n",
    "            span_dict = {}\n",
    "            for i, j in spans:\n",
    "                qid = len(queries)\n",
    "                span_score = score[i:j].mean()\n",
    "                if span_score >= 0.01:\n",
    "                    span_dict[i, j] = qid\n",
    "                    k = ' '.join(sentence[i:j])\n",
    "                    queries.append((span_score, []))\n",
    "                    query_rev[k].append(qid)\n",
    "            for (i, j), qid in span_dict.items():\n",
    "                if j - i > 1:\n",
    "                    children = []\n",
    "                    if (i, j - 1) in span_dict:\n",
    "                        children.append(span_dict[i, j - 1])\n",
    "                    if (i + 1, j) in span_dict:\n",
    "                        children.append(span_dict[i + 1, j])\n",
    "                    span_score, _ = queries[qid]\n",
    "                    queries[qid] = (span_score, children)\n",
    "        rs = conn.execute(\n",
    "            select(term_map_sql.c.term, term_map_sql.c.id, term_map_sql.c.weight)\n",
    "            .where(term_map_sql.c.weight >= 0.01)\n",
    "            .where(term_map_sql.c.term.in_(list(query_rev.keys())))\n",
    "        )\n",
    "        rs = list(rs)\n",
    "        inner_matches = set()\n",
    "        for term, _, _ in rs:\n",
    "            for qid in query_rev[term]:\n",
    "                _, children = queries[qid]\n",
    "                stack = deque(children)\n",
    "                while len(stack):\n",
    "                    curr = stack.pop()\n",
    "                    inner_matches.add(curr)\n",
    "                    _, grandchildren = queries[curr]\n",
    "                    for c in grandchildren:\n",
    "                        if c not in inner_matches:\n",
    "                            stack.append(c)\n",
    "        for term, node_id, weight in rs:\n",
    "            for qid in query_rev[term]:\n",
    "                if qid in inner_matches:\n",
    "                    continue\n",
    "                relevance[node_id] += weight * queries[qid][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d61e073aeb74a96a206052825f9e77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=165634, description='page', max=331268), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function nbhelpers.polars.pager.<locals>.<lambda>(page)>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pager(PR.with_columns(pl.Series(\"relevance\", relevance)).sort(\"relevance\", descending=True), 20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNw97vMeu3UGKrk6j3TYQO8",
   "include_colab_link": true,
   "name": "PageRank.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
